{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"../app/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_evaluation_template = \"\"\"You are a helpful and encouraging interviewer.\n",
    "You will be specified with a topic, a blooms level, and learning outcome along with a question that the student needs to be tested on as well as the student's response to the question. You need to provide an evaluation based on the student's response. The student's response will be delimited with #### characters.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "####\n",
    "{input}\n",
    "####\n",
    "\n",
    "To solve the problem, do the following\n",
    "- First, work out your own solution to the problem\n",
    "- Then, compare your solution to the student's solution. Don’t give any answers or hints. Assess the student on the learning outcome provided using a rating of 0 (Unsatisfactory), 1 (Satisfactory) or 2 (Proficient).\n",
    "- At the end, give some actionable feedback too.\n",
    "\n",
    "Use the following format:\n",
    "Actual solution:\n",
    "```\n",
    "concise steps to work out the solution and your solution here\n",
    "```\n",
    "{{\"Answer\": 0/1/2, \"feedback\": str }}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clarify_question_template = \"\"\"You are a helpful and encouraging interviewer.\n",
    "You will be specified with a topic, a blooms level, and learning outcome along with a question that the student needs to be tested on along with a series of interactions between a student and you.\n",
    "\n",
    "The student has asked a clarifying question that you need to answer. The student's response will be delimited with #### characters\n",
    "\n",
    "####\n",
    "{input}\n",
    "####\n",
    "\n",
    "Important:\n",
    "- Make sure to not give hints or answer the question.\n",
    "- If the student asks for the answer, refrain from answering.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"answer_evaluation\",\n",
    "        \"description\": \"Good for providing an evaluation for the answer given by the student to a question\",\n",
    "        \"prompt_template\": answer_evaluation_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"clarify_question_template\",\n",
    "        \"description\": \"Good for answering clarifying questions\",\n",
    "        \"prompt_template\": clarify_question_template,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForChainRun,\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "\n",
    "\n",
    "class ReturnStringChain(Chain):\n",
    "    \"\"\"\n",
    "    An example of a custom chain.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate\n",
    "    \"\"\"Prompt object to use.\"\"\"\n",
    "    output_key: str = \"text\"  #: :meta private:\n",
    "    output_string: str\n",
    "\n",
    "    # class Config:\n",
    "    #     \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "    #     extra = Extra.forbid\n",
    "    #     arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Will be whatever keys the prompt expects.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Will always return text key.\n",
    "\n",
    "        :meta private:\n",
    "        \"\"\"\n",
    "        return [self.output_key]\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, str]:\n",
    "        # If you want to log something about this run, you can do so by calling\n",
    "        # methods on the `run_manager`, as shown below. This will trigger any\n",
    "        # callbacks that are registered for that event.\n",
    "        if run_manager:\n",
    "            run_manager.on_text(f\"Just returning back: {self.output_string}\")\n",
    "\n",
    "        return {self.output_key: self.output_string}\n",
    "\n",
    "    @property\n",
    "    def _chain_type(self) -> str:\n",
    "        return \"return_string_chain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_chain_name = \"irrelevant_to_question\"\n",
    "random_chain_prompt = \"{input}\"\n",
    "random_chain_description = \"Good when the query is irrelevant to the question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ReturnStringChain(\n",
    "    output_key=\"answer\", output_string=\"Irrelevant\", prompt=random_chain_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains[random_chain_name] = chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_evaluation': LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='You are a helpful and encouraging interviewer.\\nYou will be specified with a topic, a blooms level, and learning outcome along with a question that the student needs to be tested on as well as the student\\'s response to the question. You need to provide an evaluation based on the student\\'s response. The student\\'s response will be delimited with #### characters.\\n\\n####\\n{input}\\n####\\n\\nTo solve the problem, do the following\\n- First, work out your own solution to the problem\\n- Then, compare your solution to the student\\'s solution. Don’t give any answers or hints. Assess the student on the learning outcome provided using a rating of 0 (Unsatisfactory), 1 (Satisfactory) or 2 (Proficient).\\n- At the end, give some actionable feedback too.\\n\\nUse the following format:\\nActual solution:\\n```\\nconcise steps to work out the solution and your solution here\\n```\\n{{\"Answer\": 0/1/2, \"feedback\": str }}\\n', template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all'), output_key='text'),\n",
       " 'clarify_question_template': LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template=\"You are a helpful and encouraging interviewer.\\nYou will be specified with a topic, a blooms level, and learning outcome along with a question that the student needs to be tested on along with a series of interactions between a student and you.\\n\\nThe student has asked a clarifying question that you need to answer. The student's response will be delimited with #### characters\\n\\n####\\n{input}\\n####\\n\\nImportant:\\n- Make sure to not give hints or answer the question.\\n- If the student asks for the answer, refrain from answering.\", template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all'), output_key='text'),\n",
       " 'irrelevant_to_question': ReturnStringChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, output_key='answer', output_string='Irrelevant')}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouterOutputParser(default_destination='DEFAULT', next_inputs_type=<class 'str'>, next_inputs_inner_key='input')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RouterOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
       "\n",
       "<< FORMATTING >>\n",
       "Return a markdown code snippet with a JSON object formatted to look like:\n",
       "```json\n",
       "{{{{\n",
       "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
       "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
       "}}}}\n",
       "```\n",
       "\n",
       "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
       "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
       "\n",
       "<< CANDIDATE PROMPTS >>\n",
       "{destinations}\n",
       "\n",
       "<< INPUT >>\n",
       "{{input}}\n",
       "\n",
       "<< OUTPUT >>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(MULTI_PROMPT_ROUTER_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"\n",
    "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >> Return a markdown code snippet with a JSON object formatted to look like:\n",
    "\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts. REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >> {destinations}\n",
    "\n",
    "<< INPUT >> {{input}}\n",
    "\n",
    "<< OUTPUT >>\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
