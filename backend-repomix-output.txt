This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)


================================================================
Directory Structure
================================================================
.cursor/
  rules/
    cursor-rules-location.mdc
    fastapi.mdc
    project-overview.mdc
.gitlab/
  merge_request_templates/
    default.md
docs/
  CONTRIBUTING.md
  DEPLOYMENT.md
  ENV.md
  INSTALL.md
src/
  api/
    db/
      __init__.py
      analytics.py
      chat.py
      code_draft.py
      cohort.py
      course.py
      migration.py
      milestone.py
      org.py
      task.py
      user.py
      utils.py
    routes/
      ai.py
      auth.py
      chat.py
      code.py
      cohort.py
      course.py
      file.py
      hva.py
      milestone.py
      org.py
      scorecard.py
      task.py
      user.py
    utils/
      __init__.py
      audio.py
      concurrency.py
      db.py
      logging.py
      phoenix.py
      s3.py
      url.py
    .env.aws.example
    .env.example
    config.py
    cron.py
    llm.py
    main.py
    models.py
    public.py
    scheduler.py
    settings.py
    slack.py
    todo
    websockets.py
  startup.py
.cursorignore
.dockerignore
.gitignore
.gitlab-ci.yml
.pre-commit-config.yaml
.repomixignore
codecov.yml
Dockerfile
LICENSE
pytest.ini
README.md
repomix.config.json
requirements-dev.txt
requirements.txt
run_tests.sh
setup.cfg

================================================================
Files
================================================================

================
File: .cursor/rules/cursor-rules-location.mdc
================
---
description: Cursor Rules Location
globs: *.mdc
alwaysApply: false
---

# Cursor Rules Location

Rules for placing and organizing Cursor rule files in the repository.

<rule>
name: cursor_rules_location
description: Standards for placing Cursor rule files in the correct directory
filters:
  # Match any .mdc files
  - type: file_extension
    pattern: "\\.mdc$"
  # Match files that look like Cursor rules
  - type: content
    pattern: "(?s)<rule>.*?</rule>"
  # Match file creation events
  - type: event
    pattern: "file_create"

actions:
  - type: reject
    conditions:
      - pattern: "^(?!\\.\\/\\.cursor\\/rules\\/.*\\.mdc$)"
        message: "Cursor rule files (.mdc) must be placed in the .cursor/rules directory"

  - type: suggest
    message: |
      When creating Cursor rules:

      1. Always place rule files in PROJECT_ROOT/.cursor/rules/:
         ```
         .cursor/rules/
         ├── your-rule-name.mdc
         ├── another-rule.mdc
         └── ...
         ```

      2. Follow the naming convention:
         - Use kebab-case for filenames
         - Always use .mdc extension
         - Make names descriptive of the rule's purpose

      3. Directory structure:
         ```
         PROJECT_ROOT/
         ├── .cursor/
         │   └── rules/
         │       ├── your-rule-name.mdc
         │       └── ...
         └── ...
         ```

      4. Never place rule files:
         - In the project root
         - In subdirectories outside .cursor/rules
         - In any other location

examples:
  - input: |
      # Bad: Rule file in wrong location
      rules/my-rule.mdc
      my-rule.mdc
      .rules/my-rule.mdc

      # Good: Rule file in correct location
      .cursor/rules/my-rule.mdc
    output: "Correctly placed Cursor rule file"

metadata:
  priority: high
  version: 1.0
</rule>

================
File: .cursor/rules/fastapi.mdc
================
---
description: Comprehensive guidelines for developing robust, scalable, and maintainable FastAPI applications. Covers code structure, performance, security, testing, and common pitfalls.
globs: **/*.py
alwaysApply: false
---
# FastAPI Best Practices: A Comprehensive Guide

This document provides a comprehensive set of best practices and coding standards for developing FastAPI applications. These guidelines cover various aspects of development, including project structure, common patterns, performance considerations, security, testing, and tooling.

## 1. Code Organization and Structure

A well-structured codebase is crucial for maintainability, scalability, and collaboration. Adopting a consistent and predictable project structure makes it easier for developers to navigate and understand the application.

### 1.1 Directory Structure Best Practices

Inspired by projects like Netflix's Dispatch, a feature-based directory structure is recommended, especially for larger applications:


fastapi-project/
├── alembic/               # Database migrations
├── src/                   # Source code
│   ├── auth/              # Authentication module
│   │   ├── router.py      # API endpoints for authentication
│   │   ├── schemas.py     # Pydantic models for request/response
│   │   ├── models.py      # Database models
│   │   ├── dependencies.py# Dependency injection definitions
│   │   ├── config.py      # Local configurations
│   │   ├── constants.py   # Constants and error codes
│   │   ├── exceptions.py  # Custom exceptions
│   │   ├── service.py     # Business logic
│   │   └── utils.py       # Utility functions
│   ├── aws/               # AWS integration module (example)
│   │   ├── ...
│   ├── posts/             # Posts module
│   │   ├── ...
│   ├── config.py          # Global configurations
│   ├── models.py          # Global models
│   ├── exceptions.py      # Global exceptions
│   ├── pagination.py      # Pagination logic
│   ├── database.py        # Database connection and ORM setup
│   └── main.py            # Main application entry point
├── tests/                 # Tests
│   ├── auth/
│   ├── aws/
│   └── posts/
├── templates/             # Jinja2 Templates
│   └── index.html
├── requirements/
│   ├── base.txt           # Base dependencies
│   ├── dev.txt            # Development dependencies
│   └── prod.txt           # Production dependencies
├── .env                   # Environment variables
├── .gitignore             # Git ignore file
├── logging.ini            # Logging configuration
└── alembic.ini          # Alembic configuration


Key aspects of this structure:

*   `src/`:  The root directory containing all application code.
*   Module-based organization:  Features are grouped into modules (e.g., `auth`, `posts`). Each module contains its own set of `router.py`, `schemas.py`, `models.py`, etc. This promotes loose coupling and high cohesion.
*   `main.py`: The entry point of the FastAPI application.
*   `config.py`: Stores global configurations.
*   `database.py`: Handles database connection and ORM setup (e.g., SQLAlchemy).
*   `requirements/`:  Separate dependency files for different environments.

### 1.2 File Naming Conventions

*   Python files: Use lowercase with underscores (e.g., `user_service.py`).
*   Pydantic schemas:  Use PascalCase with the suffix "Schema" or "Model" (e.g., `UserSchema`, `PostModel`).
*   Database models: Use PascalCase (e.g., `User`, `Post`).
*   Routers: Typically named `router.py` within each module.
*   Configuration files: `config.py`
*   Tests: `test_<module_name>.py` or `test_<feature>.py`

### 1.3 Module Organization

*   **Routers**: Contain API endpoint definitions.
*   **Schemas**: Define data structures using Pydantic models for request and response validation and serialization.
*   **Models**: Represent database entities (if using an ORM).
*   **Services**: Implement business logic, interacting with the database or other services.
*   **Dependencies**: Define dependency injection functions used in route handlers.
*   **Constants**: Store module-specific constants and error codes.
*   **Configuration**: Store module-specific environment variables and settings.
*   **Exceptions**: Define custom exceptions for specific modules.
*   **Utils**: Contains general-purpose utility functions.

### 1.4 Component Architecture

*   **Layered Architecture:** Separate the application into distinct layers (e.g., presentation, business logic, data access). This improves maintainability and testability.
*   **Loose Coupling:** Design components to be independent and minimize dependencies between them. This allows for easier modification and replacement of components.
*   **High Cohesion:** Ensure that each component has a single, well-defined responsibility.
*   **Dependency Injection:**  Use FastAPI's built-in dependency injection system to manage dependencies between components. This promotes testability and reusability. Favor interface-based dependency injection for added flexibility.

### 1.5 Code Splitting Strategies

*   **Feature-Based Splitting:** Divide the codebase into modules based on application features (e.g., user management, product catalog, order processing). This makes it easier to understand and maintain the code.
*   **Vertical Slicing:** Group related components (e.g., routers, schemas, models, services) into slices that represent specific use cases or functionalities.
*   **Horizontal Splitting:** Separate components based on technical layers (e.g., presentation, business logic, data access). This is useful for enforcing separation of concerns but can lead to more complex dependencies if not managed carefully.

## 2. Common Patterns and Anti-patterns

Employ established design patterns and avoid common anti-patterns to write clean, efficient, and maintainable FastAPI code.

### 2.1 Design Patterns Specific to FastAPI

*   **Repository Pattern:** Abstract data access logic behind a repository interface. This allows you to switch data sources easily (e.g., from a database to a mock for testing) and centralizes data access concerns.
*   **Service Layer Pattern:** Encapsulate business logic in service classes. Routers then call the service layer. Promotes testability and keeps routes thin and focused on request/response handling.
*   **Dependency Injection:**  Utilize FastAPI's dependency injection system extensively for request validation, authentication, authorization, and accessing shared resources like database connections.
*   **Asynchronous Operations:** Favor `async` functions for I/O-bound tasks to improve performance and concurrency.
*   **Pydantic Models for Validation:** Use Pydantic models for request and response data validation. Enforce data types, constraints, and custom validation logic.

### 2.2 Recommended Approaches for Common Tasks

*   **Configuration Management:** Use Pydantic's `BaseSettings` to manage environment variables and application settings.
*   **Database Interactions:** Use an ORM like SQLAlchemy for interacting with databases. Define database models and use them for data access.
*   **Authentication and Authorization:** Implement authentication and authorization using strategies like JWT (JSON Web Tokens) or OAuth 2.0. Use FastAPI's security utilities.
*   **Error Handling:** Use `HTTPException` for returning meaningful error responses to the client. Define custom exception classes for specific error conditions.
*   **Logging:** Configure logging using Python's `logging` module. Log important events and errors for debugging and monitoring.

### 2.3 Anti-patterns and Code Smells to Avoid

*   **Fat Route Handlers:** Avoid putting too much logic directly inside route handlers. Delegate complex tasks to service classes or utility functions.
*   **Tight Coupling:** Minimize dependencies between components to improve maintainability and testability.
*   **Ignoring Asynchronous Operations:** Blocking I/O in async routes can negate the benefits of concurrency. Ensure all I/O operations in async routes are non-blocking.
*   **Lack of Data Validation:** Failing to validate input data can lead to security vulnerabilities and unexpected behavior. Always use Pydantic models for data validation.
*   **Hardcoding Values:** Avoid hardcoding values in the code. Use configuration files or environment variables instead.
*   **Returning Pydantic objects directly from routes.** FastAPI makes an extra conversion. Return a dict.

### 2.4 State Management Best Practices

*   **Stateless Applications:** FastAPI applications are typically stateless, meaning they don't store any persistent data within the application itself. This makes them easier to scale and deploy.
*   **External Data Stores:** Store application state in external data stores like databases, caches, or message queues.
*   **Dependency Injection for State:** Use dependency injection to provide access to shared resources or stateful objects to route handlers.

### 2.5 Error Handling Patterns

*   **Centralized Exception Handling:** Implement a global exception handler to catch unhandled exceptions and return appropriate error responses.
*   **Custom Exception Classes:** Define custom exception classes for specific error conditions. This makes it easier to identify and handle different types of errors.
*   **Logging Errors:** Log all errors for debugging and monitoring.
*   **Meaningful Error Messages:** Return meaningful error messages to the client to help them understand what went wrong.

## 3. Performance Considerations

FastAPI is known for its performance, but optimizations are still crucial for high-load applications.

### 3.1 Optimization Techniques

*   **Asynchronous Operations:** Utilize `async` and `await` for I/O-bound operations to prevent blocking the event loop.
*   **Database Connection Pooling:** Use a database connection pool to reuse database connections and reduce connection overhead.
*   **Caching:** Implement caching for frequently accessed data to reduce database load and improve response times. Use tools like Redis or Memcached.
*   **Gzip Compression:** Enable gzip compression for API responses to reduce the size of the data transmitted over the network.
*   **Load Balancing:** Distribute traffic across multiple instances of the application to improve scalability and availability.
*   **Profiling:** Use profiling tools to identify performance bottlenecks in the code.

### 3.2 Memory Management

*   **Resource Management:** Properly manage resources like database connections, file handles, and network sockets. Close resources when they are no longer needed.
*   **Data Structures:** Use efficient data structures like sets and dictionaries for fast lookups.
*   **Generators:** Use generators for processing large datasets to avoid loading the entire dataset into memory at once.
*   **Object Reuse:** Reuse objects whenever possible to reduce memory allocation overhead. Consider using object pools for frequently used objects.

### 3.3 Rendering Optimization

*   **Template Caching:** Enable template caching for Jinja2 templates to reduce rendering overhead.
*   **Minimize Template Logic:** Keep template logic simple and avoid complex computations in templates.
*   **Content Delivery Network (CDN):** Use a CDN to serve static assets like images, CSS, and JavaScript files.

### 3.4 Bundle Size Optimization (for Frontend Integration)

*   **Code Splitting:** Split the frontend code into smaller bundles that can be loaded on demand.
*   **Tree Shaking:** Remove unused code from the frontend bundles using tree shaking techniques.
*   **Minification:** Minify the frontend code to reduce its size.
*   **Image Optimization:** Optimize images for the web by compressing them and using appropriate image formats.

### 3.5 Lazy Loading Strategies

*   **Lazy Loading of Modules:** Use lazy loading to load modules only when they are needed.
*   **Lazy Loading of Data:** Load data on demand instead of loading it all at once.
*   **Asynchronous Loading:** Use asynchronous loading to load data in the background without blocking the main thread.

## 4. Security Best Practices

Security is paramount. Protect your FastAPI application from common web vulnerabilities.

### 4.1 Common Vulnerabilities and How to Prevent Them

*   **SQL Injection:** Prevent SQL injection by using parameterized queries or an ORM with proper escaping.
*   **Cross-Site Scripting (XSS):** Prevent XSS by sanitizing user input and escaping output data.
*   **Cross-Site Request Forgery (CSRF):** Prevent CSRF by using CSRF tokens.
*   **Authentication and Authorization Flaws:** Implement robust authentication and authorization mechanisms to protect sensitive data and resources.
*   **Insecure Direct Object References (IDOR):** Prevent IDOR by verifying that users have access to the objects they are requesting.
*   **Denial of Service (DoS):** Prevent DoS attacks by implementing rate limiting and input validation.

### 4.2 Input Validation

*   **Pydantic Models:** Use Pydantic models to define data types, constraints, and validation rules for request bodies and query parameters.
*   **Custom Validation Logic:** Implement custom validation logic for complex validation scenarios.
*   **Sanitization:** Sanitize user input to remove potentially harmful characters or code.

### 4.3 Authentication and Authorization Patterns

*   **JWT (JSON Web Tokens):** Use JWT for stateless authentication. Generate a JWT when a user logs in and verify the JWT on subsequent requests.
*   **OAuth 2.0:** Use OAuth 2.0 for delegated authorization. Allow users to grant third-party applications access to their data without sharing their credentials.
*   **Role-Based Access Control (RBAC):** Implement RBAC to control access to resources based on user roles.
*   **Attribute-Based Access Control (ABAC):** Implement ABAC to control access to resources based on user attributes and resource attributes.
*  **CORS (Cross-Origin Resource Sharing):** Configure CORS middleware properly to allow requests only from trusted origins.

### 4.4 Data Protection Strategies

*   **Encryption:** Encrypt sensitive data at rest and in transit.
*   **Hashing:** Hash passwords and other sensitive data using a strong hashing algorithm like bcrypt or Argon2.
*   **Data Masking:** Mask sensitive data in logs and other output.
*   **Data Anonymization:** Anonymize data to protect user privacy.

### 4.5 Secure API Communication

*   **HTTPS:** Always use HTTPS to encrypt communication between the client and the server.
*   **TLS/SSL Certificates:** Use valid TLS/SSL certificates to establish secure connections.
*   **Strict Transport Security (HSTS):** Enable HSTS to force browsers to use HTTPS for all requests to the application.
*   **Content Security Policy (CSP):** Configure CSP to prevent XSS attacks by controlling the sources from which the browser is allowed to load resources.

## 5. Testing Approaches

Write comprehensive tests to ensure the quality and reliability of your FastAPI application.

### 5.1 Unit Testing Strategies

*   **Test Individual Components:** Write unit tests to test individual components like functions, classes, and modules in isolation.
*   **Mock Dependencies:** Use mocking frameworks like `unittest.mock` or `pytest-mock` to mock external dependencies and isolate the component being tested.
*   **Test Edge Cases:** Test edge cases and boundary conditions to ensure that the component handles unexpected input correctly.

### 5.2 Integration Testing

*   **Test Interactions Between Components:** Write integration tests to test the interactions between different components of the application.
*   **Use a Test Database:** Use a separate test database for integration tests to avoid affecting the production database.
*   **Test API Endpoints:** Write integration tests to test the API endpoints of the application.

### 5.3 End-to-End Testing

*   **Test the Entire Application Flow:** Write end-to-end tests to test the entire application flow, from the client to the database.
*   **Use a Testing Framework:** Use a testing framework like Selenium or Cypress to automate end-to-end tests.
*   **Test User Interface (UI):** Test the user interface of the application to ensure that it is working correctly.

### 5.4 Test Organization

*   **Organize Tests by Module:** Organize tests into separate directories or files based on the module or component being tested.
*   **Use Descriptive Test Names:** Use descriptive test names that clearly indicate what the test is verifying.
*   **Follow a Consistent Naming Convention:** Follow a consistent naming convention for test files and test functions.
*   **Keep Tests Concise:** Keep tests concise and focused on a single aspect of the component being tested.

### 5.5 Mocking and Stubbing

*   **Use Mocking Frameworks:** Use mocking frameworks like `unittest.mock` or `pytest-mock` to create mock objects and stub out external dependencies.
*   **Mock External APIs:** Mock external APIs to isolate the component being tested and avoid making actual API calls during testing.
*   **Stub Database Interactions:** Stub database interactions to avoid affecting the database during testing.
*   **Verify Interactions:** Verify that the component being tested interacts with the mock objects as expected.

## 6. Common Pitfalls and Gotchas

Be aware of common pitfalls and gotchas that can arise when developing FastAPI applications.

### 6.1 Frequent Mistakes Developers Make

*   **Incorrectly Using `Depends`:** Ensure `Depends` is used properly to inject dependencies into route handlers.
*   **Blocking I/O in Async Routes:** Avoid blocking I/O operations in async routes.
*   **Not Handling Exceptions:** Implement proper exception handling to prevent unhandled exceptions from crashing the application.
*   **Ignoring Security Best Practices:** Follow security best practices to protect the application from vulnerabilities.
*   **Not Writing Tests:** Write comprehensive tests to ensure the quality and reliability of the application.

### 6.2 Edge Cases to Be Aware Of

*   **Unicode Handling:** Be aware of unicode handling issues when processing user input.
*   **Time Zones:** Handle time zones correctly when working with dates and times.
*   **Large File Uploads:** Handle large file uploads efficiently to prevent memory exhaustion.
*   **Concurrency Issues:** Be aware of concurrency issues when working with shared resources in a multi-threaded or multi-process environment.

### 6.3 Version-Specific Issues

*   **Check Changelogs:** Review the changelogs for FastAPI and its dependencies to be aware of any breaking changes or new features.
*   **Test Compatibility:** Test the application with different versions of FastAPI and its dependencies to ensure compatibility.

### 6.4 Compatibility Concerns

*   **Python Version:** Ensure that the application is compatible with the target Python version.
*   **Operating System:** Test the application on different operating systems to ensure compatibility.
*   **Database Compatibility:** Ensure that the application is compatible with the target database.

### 6.5 Debugging Strategies

*   **Use a Debugger:** Use a debugger like `pdb` or `ipdb` to step through the code and inspect variables.
*   **Logging:** Use logging to track the execution flow and identify errors.
*   **Profiling:** Use profiling tools to identify performance bottlenecks.
*   **Remote Debugging:** Use remote debugging to debug applications running on remote servers.

## 7. Tooling and Environment

Utilize the right tools and environment for efficient FastAPI development.

### 7.1 Recommended Development Tools

*   **IDE:** VS Code, PyCharm, or other IDE with Python support.
*   **Virtual Environment Manager:** `venv`, `conda`, or `poetry` for managing project dependencies.
*   **Package Manager:** `pip` or `poetry` for installing and managing Python packages.
*   **Debugger:** `pdb` or `ipdb` for debugging Python code.
*   **Profiler:** `cProfile` or `py-spy` for profiling Python code.

### 7.2 Build Configuration

*   **`requirements.txt`:** Use `requirements.txt` to specify project dependencies. Generate it using `pip freeze > requirements.txt`.
*   **`pyproject.toml`:**  Consider using `pyproject.toml` (with Poetry or similar tools) for more advanced dependency management and build configuration.

### 7.3 Linting and Formatting

*   **Linters:** Use linters like `flake8`, `pylint`, or `ruff` to enforce code style and identify potential errors.
*   **Formatters:** Use code formatters like `black` or `autopep8` to automatically format the code according to PEP 8 standards.
*   **Pre-commit Hooks:** Use pre-commit hooks to automatically run linters and formatters before committing code.

### 7.4 Deployment Best Practices

*   **Containerization:** Use Docker to containerize the application for easy deployment and scaling.
*   **Reverse Proxy:** Use a reverse proxy like Nginx or Apache to handle incoming requests and forward them to the application.
*   **Process Manager:** Use a process manager like Supervisor or systemd to manage the application process.
*   **Load Balancing:** Use a load balancer to distribute traffic across multiple instances of the application.
*   **Monitoring:** Monitor the application using tools like Prometheus or Grafana.

### 7.5 CI/CD Integration

*   **Continuous Integration (CI):** Set up a CI pipeline to automatically build, test, and lint the code on every commit.
*   **Continuous Delivery (CD):** Set up a CD pipeline to automatically deploy the application to the production environment after the CI pipeline has passed.
*   **Version Control:** Use a version control system like Git to manage the code and track changes.
*   **Automated Testing:** Integrate automated tests into the CI/CD pipeline to ensure that the application is working correctly before deployment.
*   **Automated Rollbacks:** Implement automated rollbacks to revert to a previous version of the application if a deployment fails.

## Conclusion

By adhering to these best practices, you can develop robust, scalable, and maintainable FastAPI applications that are secure, performant, and easy to test. This guide provides a foundation for building high-quality APIs with FastAPI.

================
File: .cursor/rules/project-overview.mdc
================


================
File: .gitlab/merge_request_templates/default.md
================
## Summary
(a concise summary of the changes made in the MR)



## Tasks Done
(list of tasks completed - if the changes are reasonably big)



## Steps to test



## Screenshots
(if your changes has any UI updates, include screenshots or demo videos of the changes)



## MR Checklist
- [ ] I have tested all my changes for all possible edge cases
- [ ] I have created a PR in the [Documentation repo](https://github.com/dalmia/sensai-docs) to reflect the changes made here.
- [ ] I committed my changes with `pre-commit` being set up on my local machine (refer to `docs/INSTALL.md`) to handle issues like code formatting, linting, etc.
- [ ] I have made sure that all of the APIs are working fine.
- [ ] I have added appropriate comments to the code to explain the changes made.
- [ ] I have updated `requirements.txt` if any new packages have been used.
- [ ] I have updated `.env.example` and `docs/ENV.md` with any new environment variables and added them to the CD pipeline.
- [ ] I have updated `INSTALL.md` with any additional steps required to run the app.

================
File: docs/CONTRIBUTING.md
================
You can contribute to SensAI in many ways:

## Contributing Code
1. You can contribute by adding new features or fixing bugs. Fork the repository, set it up locally on your system following the instructions in `docs/INSTALL.md`, check out our [public roadmap](https://hyperverge.notion.site/fa1dd0cef7194fa9bf95c28820dca57f?v=ec52c6a716e94df180dcc8ced3d87610), pick up tasks from the `Not Started` or `Backlog` sections and make a `Merge Request` (MR).
2. When you create an MR, please fill in the MR template that will be provided to you and adhere to the checklist to make sure the review process is efficient for everyone.
3. On the roadmap, there are specific features/tasks that have been tagged as ⁠`Research`. These are tasks which might need a lot more experimentation before they can be ready for prime time. For those of you interested to work on research problems, you can pick up any research problem that excites you.
4. You can also contribute your own ideas for interesting research problems we could potentially work on!

## Other ways to contribute
- `Add/improve documentation`: This is no less important than improving the library itself. If you find a typo in the documentation, or if you feel that the readability of the code/documentation can be improved, do not hesitate to submit a pull request to the [Documentation repo](https://github.com/dalmia/sensai-docs).
- `Report issues`: Report issues you are facing either by [creating an issue](https://gitlab.com/hvacademy/sensai-ai/-/issues) or sharing it in our [community](https://chat.whatsapp.com/LmiulDbWpcXIgqNK6fZyxe), and give a "thumbs up" on issues reported by others that are relevant to you. 
- `Spread the word`: Reference the project from your blog and articles, link to it from your website, or simply star it.
- `Provide feedback`: If you are a teacher or a non-profit, you can test SensAI with your learners and provide valuable feedback.
- `Learning engineering`: SensAI generates a lot of data on how learners are learning and growing in their journey. Analysing this data and uncovering insights will help us improve SensAI to better support our learners and teachers. If you are interested in this, do reach out to us on the community and we'll be happy to discuss this with you.
- `Engage with the community`: Join our [community](https://chat.whatsapp.com/LmiulDbWpcXIgqNK6fZyxe) of builders in AI + Education and ask any questions that you might have or engage in the discussions, helping other fellow contributors and sharing your ideas/thoughts. 

## FAQs

**I have created a new branch and made changes to it but why am I not able to push my changes to the remote branch?**

Since you are not added as a collaborator to the repository, you will not be able to push your changes to the remote branch. Please follow the instructions above and create a fork of the repository instead. You can push changes to your fork and once you are ready, you can create a Merge Request to the main repository.


## Code of Conduct
We abide by the principles of openness, respect, and consideration of others of the Python Software Foundation: https://www.python.org/psf/codeofconduct/.

## Reference
These guidelines have been adapted from the Contributing guidelines of [scikit-learn](https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md)!

================
File: docs/DEPLOYMENT.md
================
## Setting up nginx

sudo apt update && sudo apt upgrade -y
sudo apt install nginx -y
sudo systemctl start nginx
sudo systemctl enable nginx

# Create a new configuration file for sensai
sudo vim /etc/nginx/sites-available/sensai
Add the following config:
```
server {
    listen 80;

    # Production site
    server_name sensai.hyperverge.org;

    location / {
        proxy_pass http://localhost:8501;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}

server {
    listen 80;

    # Development site
    server_name sensai.dev.hyperverge.org;

    location / {
        proxy_pass http://localhost:8502;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}
```

# Create symlink to sites-enabled
sudo ln -s /etc/nginx/sites-available/sensai /etc/nginx/sites-enabled/

# Remove the default site (optional)
sudo rm /etc/nginx/sites-enabled/default

# Test the configuration
sudo nginx -t

# Reload nginx to apply changes
sudo systemctl reload nginx

================
File: src/api/utils/__init__.py
================
from datetime import datetime, timezone, timedelta
import random
import colorsys


def generate_random_color() -> str:
    # Generate a random hue
    hue = random.random()

    # Create two colors with the same hue but different lightness
    saturation = random.uniform(0.3, 0.9)
    value = random.uniform(0.4, 1.0)
    color = colorsys.hsv_to_rgb(hue, saturation, value)

    # Convert RGB values to hex
    return "#{:02x}{:02x}{:02x}".format(
        int(color[0] * 255), int(color[1] * 255), int(color[2] * 255)
    )


def get_date_from_str(date_str: str, source_timezone: str) -> datetime.date:
    """source_timezone: which timezone the date_str is in. Can be IST or UTC"""
    if source_timezone == "IST":
        # return the date as is
        return datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S").date()

    return (
        datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S")
        .replace(tzinfo=timezone.utc)  # first convert from utc to ist
        .astimezone(timezone(timedelta(hours=5, minutes=30)))
        .date()  # then get the date
    )


def convert_utc_to_ist(utc_dt: datetime) -> datetime:
    # First ensure the datetime is UTC aware if it isn't already
    if utc_dt.tzinfo is None:
        utc_dt = utc_dt.replace(tzinfo=timezone.utc)

    # Create IST timezone
    ist = timezone(timedelta(hours=5, minutes=30))

    # Convert to IST
    ist_dt = utc_dt.astimezone(ist)

    return ist_dt

================
File: src/api/utils/audio.py
================
import base64


def prepare_audio_input_for_ai(audio_data: bytes):
    return base64.b64encode(audio_data).decode("utf-8")

================
File: src/api/utils/db.py
================
import sqlite3
from typing import List, Tuple
from api.config import sqlite_db_path
from api.utils.logging import logger
import aiosqlite
from contextlib import asynccontextmanager


def trace_callback(sql):
    # Record the start time and SQL
    logger.info(f"Executing operation: {sql}")


@asynccontextmanager
async def get_new_db_connection():
    conn = None
    try:
        conn = await aiosqlite.connect(sqlite_db_path)
        await conn.execute("PRAGMA synchronous=NORMAL;")
        await conn.set_trace_callback(trace_callback)
        yield conn
    except Exception as e:
        if conn:
            await conn.rollback()  # Rollback on any exception
        raise  # Re-raise the exception to propagate the error
    finally:
        if conn:
            await conn.close()


def set_db_defaults():
    conn = sqlite3.connect(sqlite_db_path)

    current_mode = conn.execute("PRAGMA journal_mode;").fetchone()[0]

    if current_mode.lower() != "wal":
        settings = "PRAGMA journal_mode = WAL;"

        conn.executescript(settings)
        print("Defaults set.")
    else:
        print("Defaults already set.")


async def execute_db_operation(
    operation,
    params=None,
    fetch_one=False,
    fetch_all=False,
    get_last_row_id=False,
):
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        if params:
            await cursor.execute(operation, params)
        else:
            await cursor.execute(operation)

        if fetch_one:
            result = await cursor.fetchone()
        elif fetch_all:
            result = await cursor.fetchall()
        else:
            result = None

        await conn.commit()

        if get_last_row_id:
            return cursor.lastrowid

        return result


async def execute_many_db_operation(operation, params_list):
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.executemany(operation, params_list)
        await conn.commit()


async def execute_multiple_db_operations(commands_and_params: List[Tuple[str, Tuple]]):
    """
    Execute multiple SQL commands under the same connection.
    Each command is a tuple of (sql_command, params).
    All commands are executed in a single transaction.
    """
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        for command, params in commands_and_params:
            await cursor.execute(command, params)

        await conn.commit()


async def check_table_exists(table_name: str, cursor):
    await cursor.execute(
        f"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'"
    )
    table_exists = await cursor.fetchone()

    return table_exists is not None


def serialise_list_to_str(list_to_serialise: List[str]):
    if list_to_serialise:
        return ",".join(list_to_serialise)

    return None


def deserialise_list_from_str(str_to_deserialise: str):
    if str_to_deserialise:
        return str_to_deserialise.split(",")

    return []

================
File: src/api/utils/logging.py
================
import logging
from api.config import log_file_path


def setup_logging(log_file_path: str):
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    # Add a StreamHandler to output logs to the console
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    # Add a FileHandler to write logs to app.log
    file_handler = logging.FileHandler(log_file_path)
    file_handler.setLevel(logging.INFO)

    # Create a formatter and add it to the handlers
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)

    # Add the handlers to the logger
    # logger.addHandler(console_handler)
    logger.addHandler(file_handler)

    return logger


logger = setup_logging(log_file_path)

================
File: src/api/utils/url.py
================
import streamlit as st
import re
import os
from typing import Dict
from unidecode import unidecode


def slugify(text: str) -> str:
    """
    Convert a string to a URL-friendly slug.
    Example: "Hello World! How are you?" -> "hello-world-how-are-you"

    Args:
        text: The string to convert

    Returns:
        A URL-friendly version of the string
    """
    # Convert to lowercase and normalize unicode characters
    text = str(text).lower()
    text = unidecode(text)

    # Replace any non-alphanumeric character with a hyphen
    text = re.sub(r"[^\w\s-]", "", text)

    # Replace all spaces or repeated hyphens with a single hyphen
    text = re.sub(r"[-\s]+", "-", text.strip())

    return text


def get_home_url(params: Dict[str, str] = None) -> str:
    home_page_url = os.environ.get("APP_URL")

    if params:
        query_params = "&".join([f"{key}={value}" for key, value in params.items()])
        home_page_url += f"?{query_params}"

    return home_page_url

================
File: src/api/.env.aws.example
================
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_SESSION_TOKEN=

================
File: src/api/todo
================
todo

migrations:
- tasks table 
  - reading material -> learning material
  - questions go to separate questions table and linked to quiz tasks and exam tasks
  - many attributes go to questions table out of tasks table

- on delete cascade for all tables

================
File: .cursorignore
================
.env
*.env
.env.*
secrets.toml
**/secrets.toml
**/secrets.*.toml
secrets.*.toml

================
File: .pre-commit-config.yaml
================
repos:
-   repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v3.2.0
    hooks:
    -   id: trailing-whitespace
    -   id: end-of-file-fixer
    -   id: check-json
    -   id: check-yaml
    -   id: check-merge-conflict
    -   id: check-added-large-files
-   repo: https://github.com/psf/black
    rev: 22.3.0
    hooks:
    -   id: black
repos:
-   repo: https://github.com/hhatto/autopep8
    rev: v2.3.1
    hooks:
    -   id: autopep8
        args: [--ignore=E402, --diff]  # add any other args you need

================
File: .repomixignore
================
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/

experimental/
tests/
venv/

================
File: LICENSE
================
GNU AFFERO GENERAL PUBLIC LICENSE
                       Version 3, 19 November 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU Affero General Public License is a free, copyleft license for
software and other kinds of works, specifically designed to ensure
cooperation with the community in the case of network server software.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
our General Public Licenses are intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  Developers that use our General Public Licenses protect your rights
with two steps: (1) assert copyright on the software, and (2) offer
you this License which gives you legal permission to copy, distribute
and/or modify the software.

  A secondary benefit of defending all users' freedom is that
improvements made in alternate versions of the program, if they
receive widespread use, become available for other developers to
incorporate.  Many developers of free software are heartened and
encouraged by the resulting cooperation.  However, in the case of
software used on network servers, this result may fail to come about.
The GNU General Public License permits making a modified version and
letting the public access it on a server without ever releasing its
source code to the public.

  The GNU Affero General Public License is designed specifically to
ensure that, in such cases, the modified source code becomes available
to the community.  It requires the operator of a network server to
provide the source code of the modified version running there to the
users of that server.  Therefore, public use of a modified version, on
a publicly accessible server, gives the public access to the source
code of the modified version.

  An older license, called the Affero General Public License and
published by Affero, was designed to accomplish similar goals.  This is
a different license, not a version of the Affero GPL, but Affero has
released a new version of the Affero GPL which permits relicensing under
this license.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU Affero General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Remote Network Interaction; Use with the GNU General Public License.

  Notwithstanding any other provision of this License, if you modify the
Program, your modified version must prominently offer all users
interacting with it remotely through a computer network (if your version
supports such interaction) an opportunity to receive the Corresponding
Source of your version by providing access to the Corresponding Source
from a network server at no charge, through some standard or customary
means of facilitating copying of software.  This Corresponding Source
shall include the Corresponding Source for any work covered by version 3
of the GNU General Public License that is incorporated pursuant to the
following paragraph.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the work with which it is combined will remain governed by version
3 of the GNU General Public License.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU Affero General Public License from time to time.  Such new versions
will be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU Affero General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU Affero General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU Affero General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If your software can interact with users remotely through a computer
network, you should also make sure that it provides a way for users to
get its source.  For example, if your program is a web application, its
interface could display a "Source" link that leads users to an archive
of the code.  There are many ways you could offer source, and different
solutions will be better for different programs; see section 13 for the
specific requirements.

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU AGPL, see
<https://www.gnu.org/licenses/>.

================
File: repomix.config.json
================
{
  "$schema": "https://repomix.com/schemas/latest/schema.json",
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "backend-repomix-output.txt",
    "style": "plain",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "files": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "copyToClipboard": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100,
      "includeDiffs": false
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}

================
File: setup.cfg
================
[pycodestyle]
max_line_length = 120

================
File: src/api/db/analytics.py
================
from typing import Dict, List, Optional
from collections import defaultdict
from api.utils.db import execute_db_operation
from api.config import (
    chat_history_table_name,
    questions_table_name,
    tasks_table_name,
    organizations_table_name,
    task_completions_table_name,
    course_tasks_table_name,
    course_cohorts_table_name,
    users_table_name,
    user_cohorts_table_name,
)
from api.models import LeaderboardViewType, TaskType, TaskStatus
from api.db.user import get_user_streak_from_usage_dates


async def get_usage_summary_by_organization(
    filter_period: Optional[str] = None,
) -> List[Dict]:
    """Get usage summary by organization from chat history."""

    if filter_period and filter_period not in [
        "last_day",
        "current_month",
        "current_year",
    ]:
        raise ValueError("Invalid filter period")

    # Build the date filter condition based on the filter_period
    date_filter = ""
    if filter_period == "last_day":
        date_filter = "AND ch.created_at >= datetime('now', '-1 day')"
    elif filter_period == "current_month":
        date_filter = "AND ch.created_at >= datetime('now', 'start of month')"
    elif filter_period == "current_year":
        date_filter = "AND ch.created_at >= datetime('now', 'start of year')"

    rows = await execute_db_operation(
        f"""
        SELECT 
            o.id as org_id,
            o.name as org_name,
            COUNT(ch.id) as user_message_count
        FROM {chat_history_table_name} ch
        JOIN {questions_table_name} q ON ch.question_id = q.id
        JOIN {tasks_table_name} t ON q.task_id = t.id
        JOIN {organizations_table_name} o ON t.org_id = o.id
        WHERE ch.role = 'user' {date_filter}
        GROUP BY o.id, o.name
        ORDER BY user_message_count DESC
        """,
        fetch_all=True,
    )

    return [
        {
            "org_id": row[0],
            "org_name": row[1],
            "user_message_count": row[2],
        }
        for row in rows
    ]


async def get_cohort_completion(
    cohort_id: int, user_ids: List[int], course_id: int = None
):
    """
    Retrieves completion data for a user in a specific cohort.

    Args:
        cohort_id: The ID of the cohort
        user_ids: The IDs of the users
        course_id: The ID of the course (optional, if not provided, all courses in the cohort will be considered)

    Returns:
        A dictionary mapping task IDs to their completion status:
        {
            task_id: {
                "is_complete": bool,
                "questions": [{"question_id": int, "is_complete": bool}]
            }
        }
    """
    results = defaultdict(dict)

    # user_in_cohort = await is_user_in_cohort(user_id, cohort_id)
    # if not user_in_cohort:
    #     results[user_id] = {}
    #     continue

    # Get completed tasks for the users from task_completions_table
    completed_tasks = await execute_db_operation(
        f"""
        SELECT user_id, task_id 
        FROM {task_completions_table_name}
        WHERE user_id in ({','.join(map(str, user_ids))}) AND task_id IS NOT NULL
        """,
        fetch_all=True,
    )
    completed_task_ids_for_user = defaultdict(set)
    for user_id, task_id in completed_tasks:
        completed_task_ids_for_user[user_id].add(task_id)

    # Get completed questions for the users from task_completions_table
    completed_questions = await execute_db_operation(
        f"""
        SELECT user_id, question_id 
        FROM {task_completions_table_name}
        WHERE user_id in ({','.join(map(str, user_ids))}) AND question_id IS NOT NULL
        """,
        fetch_all=True,
    )
    completed_question_ids_for_user = defaultdict(set)
    for user_id, question_id in completed_questions:
        completed_question_ids_for_user[user_id].add(question_id)

    # Get all tasks for the cohort
    # Get learning material tasks
    query = f"""
        SELECT DISTINCT t.id
        FROM {tasks_table_name} t
        JOIN {course_tasks_table_name} ct ON t.id = ct.task_id
        JOIN {course_cohorts_table_name} cc ON ct.course_id = cc.course_id
        WHERE cc.cohort_id = ? AND t.deleted_at IS NULL AND t.type = '{TaskType.LEARNING_MATERIAL}' AND t.status = '{TaskStatus.PUBLISHED}' AND t.scheduled_publish_at IS NULL
        """
    params = (cohort_id,)

    if course_id is not None:
        query += " AND ct.course_id = ?"
        params += (course_id,)

    learning_material_tasks = await execute_db_operation(
        query,
        params,
        fetch_all=True,
    )

    for user_id in user_ids:
        for task in learning_material_tasks:
            # For learning material, check if it's in the completed tasks list
            results[user_id][task[0]] = {
                "is_complete": task[0] in completed_task_ids_for_user[user_id]
            }

    # Get quiz and exam task questions
    query = f"""
        SELECT DISTINCT t.id as task_id, q.id as question_id
        FROM {tasks_table_name} t
        JOIN {course_tasks_table_name} ct ON t.id = ct.task_id
        JOIN {course_cohorts_table_name} cc ON ct.course_id = cc.course_id
        LEFT JOIN {questions_table_name} q ON t.id = q.task_id AND q.deleted_at IS NULL
        WHERE cc.cohort_id = ? AND t.deleted_at IS NULL AND t.type = '{TaskType.QUIZ}' AND t.status = '{TaskStatus.PUBLISHED}' AND t.scheduled_publish_at IS NULL{
            " AND ct.course_id = ?" if course_id else ""
        } 
        ORDER BY t.id, q.position ASC
        """
    params = (cohort_id,)

    if course_id is not None:
        params += (course_id,)

    quiz_exam_questions = await execute_db_operation(
        query,
        params,
        fetch_all=True,
    )

    # Group questions by task_id
    quiz_exam_tasks = defaultdict(list)
    for row in quiz_exam_questions:
        task_id = row[0]
        question_id = row[1]

        quiz_exam_tasks[task_id].append(question_id)

    for user_id in user_ids:
        for task_id in quiz_exam_tasks:
            is_task_complete = True
            question_completions = []

            for question_id in quiz_exam_tasks[task_id]:
                is_question_complete = (
                    question_id in completed_question_ids_for_user[user_id]
                )

                question_completions.append(
                    {
                        "question_id": question_id,
                        "is_complete": is_question_complete,
                    }
                )

                if not is_question_complete:
                    is_task_complete = False

            results[user_id][task_id] = {
                "is_complete": is_task_complete,
                "questions": question_completions,
            }

    return results


async def get_cohort_course_attempt_data(cohort_learner_ids: List[int], course_id: int):
    """
    Retrieves attempt data for users in a specific cohort, focusing on whether each user
    has attempted any task from each course assigned to the cohort.

    An attempt is defined as either:
    1. Having at least one entry in task_completions_table for a learning material task in the course
    2. Having at least one message in chat_history_table for a question in a quiz/exam task in the course

    Args:
        cohort_learner_ids: The IDs of the learners in the cohort
        course_id: The ID of the course to check

    Returns:
        A dictionary with the following structure:
        {
            user_id: {
                course_id: {
                    "course_name": str,
                    "has_attempted": bool,
                    "last_attempt_date": str or None,
                    "attempt_count": int
                }
            }
        }
    """
    result = defaultdict(dict)

    # Initialize result structure with all courses for all users
    for user_id in cohort_learner_ids:
        result[user_id][course_id] = {
            "has_attempted": False,
        }

    cohort_learner_ids_str = ",".join(map(str, cohort_learner_ids))

    # Get all learning material tasks attempted for this course
    task_completions = await execute_db_operation(
        f"""
        SELECT DISTINCT tc.user_id
        FROM {task_completions_table_name} tc
        JOIN {course_tasks_table_name} ct ON tc.task_id = ct.task_id
        WHERE tc.user_id IN ({cohort_learner_ids_str}) AND ct.course_id = ?
        ORDER BY tc.created_at ASC
        """,
        (course_id,),
        fetch_all=True,
    )

    # Process task completion data
    for completion in task_completions:
        user_id = completion[0]
        result[user_id][course_id]["has_attempted"] = True

    chat_messages = await execute_db_operation(
        f"""
        SELECT DISTINCT ch.user_id
        FROM {chat_history_table_name} ch
        JOIN {questions_table_name} q ON ch.question_id = q.id
        JOIN {tasks_table_name} t ON q.task_id = t.id
        JOIN {course_tasks_table_name} ct ON t.id = ct.task_id
        WHERE ch.user_id IN ({cohort_learner_ids_str}) AND ct.course_id = ?
        GROUP BY ch.user_id
        """,
        (course_id,),
        fetch_all=True,
    )

    # Process chat message data
    for message_data in chat_messages:
        user_id = message_data[0]
        result[user_id][course_id]["has_attempted"] = True

    # Convert defaultdict to regular dict for cleaner response
    return {user_id: dict(courses) for user_id, courses in result.items()}


async def get_cohort_streaks(
    view: LeaderboardViewType = LeaderboardViewType.ALL_TIME, cohort_id: int = None
):
    # Build date filter based on duration
    date_filter = ""
    if view == LeaderboardViewType.WEEKLY:
        date_filter = "AND DATE(datetime(timestamp, '+5 hours', '+30 minutes')) > DATE('now', 'weekday 0', '-7 days')"
    elif view == LeaderboardViewType.MONTHLY:
        date_filter = "AND strftime('%Y-%m', datetime(timestamp, '+5 hours', '+30 minutes')) = strftime('%Y-%m', 'now')"

    # Get all user interactions, ordered by user and timestamp
    usage_per_user = await execute_db_operation(
        f"""
    SELECT 
        u.id,
        u.email,
        u.first_name,
        u.middle_name,
        u.last_name,
        GROUP_CONCAT(t.created_at) as created_ats
    FROM {users_table_name} u
    LEFT JOIN (
        -- Chat history interactions
        SELECT user_id, MAX(datetime(created_at, '+5 hours', '+30 minutes')) as created_at
        FROM {chat_history_table_name}
        WHERE 1=1 {date_filter} AND question_id IN (SELECT id FROM {questions_table_name} WHERE task_id IN (SELECT task_id FROM {course_tasks_table_name} WHERE course_id IN (SELECT course_id FROM {course_cohorts_table_name} WHERE cohort_id = ?)))
        GROUP BY user_id, DATE(datetime(created_at, '+5 hours', '+30 minutes'))
        
        UNION
        
        -- Task completions
        SELECT user_id, MAX(datetime(created_at, '+5 hours', '+30 minutes')) as created_at
        FROM {task_completions_table_name}
        WHERE 1=1 {date_filter} AND task_id IN (
            SELECT task_id FROM {course_tasks_table_name} 
            WHERE course_id IN (SELECT course_id FROM {course_cohorts_table_name} WHERE cohort_id = ?)
        )
        GROUP BY user_id, DATE(datetime(created_at, '+5 hours', '+30 minutes'))
        
        ORDER BY created_at DESC, user_id
    ) t ON u.id = t.user_id
    WHERE u.id IN (
        -- Users who are in the cohort as learners
        SELECT user_id FROM {user_cohorts_table_name} WHERE cohort_id = ? and role = 'learner'
       
    )
    GROUP BY u.id, u.email, u.first_name, u.middle_name, u.last_name
    """,
        (cohort_id, cohort_id, cohort_id),
        fetch_all=True,
    )

    streaks = []

    for (
        user_id,
        user_email,
        user_first_name,
        user_middle_name,
        user_last_name,
        user_usage_dates_str,
    ) in usage_per_user:

        if user_usage_dates_str:
            user_usage_dates = user_usage_dates_str.split(",")
            user_usage_dates = sorted(user_usage_dates, reverse=True)
            streak_count = len(get_user_streak_from_usage_dates(user_usage_dates))
        else:
            streak_count = 0

        streaks.append(
            {
                "user": {
                    "id": user_id,
                    "email": user_email,
                    "first_name": user_first_name,
                    "middle_name": user_middle_name,
                    "last_name": user_last_name,
                },
                "streak_count": streak_count,
            }
        )

    return streaks

================
File: src/api/db/chat.py
================
from typing import List, Tuple
from datetime import datetime
from api.utils.db import get_new_db_connection, execute_db_operation
from api.config import (
    chat_history_table_name,
    questions_table_name,
    tasks_table_name,
    users_table_name,
    task_completions_table_name,
)
from api.models import StoreMessageRequest, ChatMessage, TaskType
from api.db.task import get_basic_task_details


async def store_messages(
    messages: List[StoreMessageRequest],
    user_id: int,
    question_id: int,
    is_complete: bool,
):
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        new_row_ids = []

        for message in messages:
            # Insert the new message
            await cursor.execute(
                f"""
            INSERT INTO {chat_history_table_name} (user_id, question_id, role, content, response_type, created_at)
            VALUES (?, ?, ?, ?, ?, ?)
            """,
                (
                    user_id,
                    question_id,
                    message.role,
                    message.content,
                    message.response_type,
                    message.created_at,
                ),
            )

            new_row_id = cursor.lastrowid
            new_row_ids.append(new_row_id)

        if is_complete:
            await cursor.execute(
                f"""
                INSERT INTO {task_completions_table_name} (user_id, question_id)
                VALUES (?, ?) ON CONFLICT(user_id, question_id) DO NOTHING
                """,
                (user_id, question_id),
            )

        await conn.commit()

    # Fetch the newly inserted row
    new_rows = await execute_db_operation(
        f"""SELECT id, created_at, user_id, question_id, role, content, response_type
    FROM {chat_history_table_name}
    WHERE id IN ({','.join(map(str, new_row_ids))})
    """,
        fetch_all=True,
    )

    # Return the newly inserted row as a dictionary
    return [
        {
            "id": new_row[0],
            "created_at": new_row[1],
            "user_id": new_row[2],
            "question_id": new_row[3],
            "role": new_row[4],
            "content": new_row[5],
            "response_type": new_row[6],
        }
        for new_row in new_rows
    ]


async def get_all_chat_history(org_id: int):
    chat_history = await execute_db_operation(
        f"""
        SELECT message.id, message.created_at, user.id AS user_id, user.email AS user_email, message.question_id, task.id AS task_id, message.role, message.content, message.response_type
        FROM {chat_history_table_name} message
        INNER JOIN {questions_table_name} question ON message.question_id = question.id
        INNER JOIN {tasks_table_name} task ON question.task_id = task.id
        INNER JOIN {users_table_name} user ON message.user_id = user.id 
        WHERE task.deleted_at IS NULL AND task.org_id = ?
        ORDER BY message.created_at ASC
        """,
        (org_id,),
        fetch_all=True,
    )

    return [
        {
            "id": row[0],
            "created_at": row[1],
            "user_id": row[2],
            "user_email": row[3],
            "question_id": row[4],
            "task_id": row[5],
            "role": row[6],
            "content": row[7],
            "response_type": row[8],
        }
        for row in chat_history
    ]


def convert_chat_message_to_dict(message: Tuple) -> ChatMessage:
    return {
        "id": message[0],
        "created_at": message[1],
        "user_id": message[2],
        "question_id": message[3],
        "role": message[4],
        "content": message[5],
        "response_type": message[6],
    }


async def get_question_chat_history_for_user(
    question_id: int, user_id: int
) -> List[ChatMessage]:
    chat_history = await execute_db_operation(
        f"""
    SELECT id, created_at, user_id, question_id, role, content, response_type FROM {chat_history_table_name} WHERE question_id = ? AND user_id = ?
    """,
        (question_id, user_id),
        fetch_all=True,
    )

    return [convert_chat_message_to_dict(row) for row in chat_history]


async def get_task_chat_history_for_user(
    task_id: int, user_id: int
) -> List[ChatMessage]:
    task = await get_basic_task_details(task_id)

    if not task:
        raise ValueError("Task does not exist")

    if task["type"] == TaskType.LEARNING_MATERIAL:
        raise ValueError("Task is not a quiz")

    query = f"""
        SELECT ch.id, ch.created_at, ch.user_id, ch.question_id, ch.role, ch.content, ch.response_type
        FROM {chat_history_table_name} ch
        JOIN {questions_table_name} q ON ch.question_id = q.id
        WHERE q.task_id = ? 
        AND ch.user_id = ?
        ORDER BY ch.created_at ASC
    """

    chat_history = await execute_db_operation(
        query,
        (task_id, user_id),
        fetch_all=True,
    )

    return [convert_chat_message_to_dict(row) for row in chat_history]


async def delete_message(message_id: int):
    await execute_db_operation(
        f"DELETE FROM {chat_history_table_name} WHERE id = ?", (message_id,)
    )


async def update_message_timestamp(message_id: int, new_timestamp: datetime):
    await execute_db_operation(
        f"UPDATE {chat_history_table_name} SET timestamp = ? WHERE id = ?",
        (new_timestamp, message_id),
    )


async def delete_user_chat_history_for_task(question_id: int, user_id: int):
    await execute_db_operation(
        f"DELETE FROM {chat_history_table_name} WHERE question_id = ? AND user_id = ?",
        (question_id, user_id),
    )


async def delete_all_chat_history():
    await execute_db_operation(f"DELETE FROM {chat_history_table_name}")

================
File: src/api/db/code_draft.py
================
from typing import List, Dict
import json

from api.config import code_drafts_table_name
from api.utils.db import execute_db_operation


async def upsert_user_code_draft(user_id: int, question_id: int, code: List[Dict]):
    """Insert or update a code draft for a (user_id, question_id) pair."""

    await execute_db_operation(
        f"""
        INSERT INTO {code_drafts_table_name} (user_id, question_id, code)
        VALUES (?, ?, ?)
        ON CONFLICT(user_id, question_id) DO UPDATE SET
            code = excluded.code,
            updated_at = CURRENT_TIMESTAMP
        """,
        (user_id, question_id, json.dumps(code)),
    )


async def get_user_code_draft(user_id: int, question_id: int):
    """Retrieve the latest code draft for the given user & question pair."""

    row = await execute_db_operation(
        f"""SELECT id, code, updated_at FROM {code_drafts_table_name}
            WHERE user_id = ? AND question_id = ?""",
        (user_id, question_id),
        fetch_one=True,
    )

    if not row:
        return None

    return {
        "id": row[0],
        "code": json.loads(row[1]),
        "updated_at": row[2],
    }


async def delete_user_code_draft(user_id: int, question_id: int):
    await execute_db_operation(
        f"DELETE FROM {code_drafts_table_name} WHERE user_id = ? AND question_id = ?",
        (user_id, question_id),
    )

================
File: src/api/db/cohort.py
================
from datetime import datetime, timezone, timedelta
from typing import List, Optional, Dict, Tuple
from dateutil.relativedelta import relativedelta
from collections import defaultdict
from api.models import TaskType, TaskStatus
from api.config import (
    cohorts_table_name,
    course_cohorts_table_name,
    courses_table_name,
    tasks_table_name,
    chat_history_table_name,
    user_cohorts_table_name,
    organizations_table_name,
    user_organizations_table_name,
    users_table_name,
)
from api.utils.db import (
    execute_db_operation,
    execute_many_db_operation,
    execute_multiple_db_operations,
    get_new_db_connection,
)
from api.db.user import insert_or_return_user
from api.db.course import get_course
from api.slack import send_slack_notification_for_learner_added_to_cohort


async def add_courses_to_cohort(
    cohort_id: int,
    course_ids: List[int],
    is_drip_enabled: Optional[bool] = False,
    frequency_value: Optional[int] = None,
    frequency_unit: Optional[str] = None,
    publish_at: Optional[datetime] = None,
):
    values = []
    for course_id in course_ids:
        values.append(
            (
                course_id,
                cohort_id,
                is_drip_enabled,
                frequency_value,
                frequency_unit,
                publish_at,
            )
        )

    await execute_many_db_operation(
        f"""INSERT INTO {course_cohorts_table_name} 
            (course_id, cohort_id, is_drip_enabled, frequency_value, frequency_unit, publish_at) 
            VALUES (?, ?, ?, ?, ?, ?)""",
        values,
    )


async def add_course_to_cohorts(
    course_id: int,
    cohort_ids: List[int],
    is_drip_enabled: Optional[bool] = False,
    frequency_value: Optional[int] = None,
    frequency_unit: Optional[str] = None,
    publish_at: Optional[datetime] = None,
):
    values = []
    for cohort_id in cohort_ids:
        values.append(
            (
                course_id,
                cohort_id,
                is_drip_enabled,
                frequency_value,
                frequency_unit,
                publish_at,
            )
        )

    await execute_many_db_operation(
        f"""INSERT INTO {course_cohorts_table_name} 
            (course_id, cohort_id, is_drip_enabled, frequency_value, frequency_unit, publish_at) 
            VALUES (?, ?, ?, ?, ?, ?)""",
        values,
    )


async def remove_course_from_cohorts(course_id: int, cohort_ids: List[int]):
    await execute_many_db_operation(
        f"DELETE FROM {course_cohorts_table_name} WHERE course_id = ? AND cohort_id = ?",
        [(course_id, cohort_id) for cohort_id in cohort_ids],
    )


async def remove_courses_from_cohort(cohort_id: int, course_ids: List[int]):
    await execute_many_db_operation(
        f"DELETE FROM {course_cohorts_table_name} WHERE cohort_id = ? AND course_id = ?",
        [(cohort_id, course_id) for course_id in course_ids],
    )


async def update_cohort_name(cohort_id: int, name: str):
    await execute_db_operation(
        f"UPDATE {cohorts_table_name} SET name = ? WHERE id = ?",
        (name, cohort_id),
    )


def drop_user_cohorts_table():
    execute_db_operation(f"DROP TABLE IF EXISTS {user_cohorts_table_name}")


def delete_all_cohort_info():
    execute_db_operation(f"DELETE FROM {cohorts_table_name}")


async def delete_cohort(cohort_id: int):
    await execute_multiple_db_operations(
        [
            (
                f"DELETE FROM {user_cohorts_table_name} WHERE cohort_id = ?",
                (cohort_id,),
            ),
            (
                f"DELETE FROM {course_cohorts_table_name} WHERE cohort_id = ?",
                (cohort_id,),
            ),
            (
                f"DELETE FROM {cohorts_table_name} WHERE id = ?",
                (cohort_id,),
            ),
        ]
    )


def drop_cohorts_table():
    execute_db_operation(f"DROP TABLE IF EXISTS {cohorts_table_name}")


async def create_cohort(name: str, org_id: int) -> int:
    return await execute_db_operation(
        f"""
        INSERT INTO {cohorts_table_name} (name, org_id)
        VALUES (?, ?)
        """,
        params=(name, org_id),
        get_last_row_id=True,
    )


async def add_members_to_cohort(
    cohort_id: int, org_slug: str, org_id: int, emails: List[str], roles: List[str]
):
    if org_slug is None and org_id is None:
        raise Exception("Either org_slug or org_id must be provided")

    if org_slug is not None:
        org_id = await execute_db_operation(
            f"SELECT id FROM {organizations_table_name} WHERE slug = ?",
            (org_slug,),
            fetch_one=True,
        )

        if org_id is None:
            raise Exception("Organization not found")

        org_id = org_id[0]
    else:
        org = await execute_db_operation(
            f"SELECT slug FROM {organizations_table_name} WHERE id = ?",
            (org_id,),
            fetch_one=True,
        )

        if org is None:
            raise Exception("Organization not found")

        org_slug = org[0]

    # Check if cohort belongs to the organization
    cohort = await execute_db_operation(
        f"""
        SELECT name FROM {cohorts_table_name} WHERE id = ? AND org_id = ?
        """,
        (cohort_id, org_id),
        fetch_one=True,
    )

    if not cohort:
        raise Exception("Cohort does not belong to this organization")

    # Check if any of the emails is an admin for the org
    admin_emails = await execute_db_operation(
        f"""
        SELECT email FROM {users_table_name} u
        JOIN {user_organizations_table_name} uo ON u.id = uo.user_id
        WHERE uo.org_id = ?
        AND (uo.role = 'admin' OR uo.role = 'owner')
        AND u.email IN ({','.join(['?' for _ in emails])})
        """,
        (org_id, *emails),
        fetch_all=True,
    )

    if admin_emails:
        raise Exception(f"Cannot add an admin to the cohort.")

    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        users_to_add = []

        for email in emails:
            # Get or create user
            user = await insert_or_return_user(
                cursor,
                email,
            )
            users_to_add.append(user)

        await cursor.execute(
            f"""
            SELECT 1 FROM {user_cohorts_table_name} WHERE user_id IN ({','.join(['?' for _ in [user["id"] for user in users_to_add]])}) AND cohort_id = ?
            """,
            (*[user["id"] for user in users_to_add], cohort_id),
        )

        user_exists = await cursor.fetchone()

        if user_exists:
            raise Exception("User already exists in cohort")

        for user in users_to_add:
            await send_slack_notification_for_learner_added_to_cohort(
                user, org_slug, org_id, cohort[0], cohort_id
            )

        # Add users to cohort
        await cursor.executemany(
            f"""
            INSERT INTO {user_cohorts_table_name} (user_id, cohort_id, role)
            VALUES (?, ?, ?)
            """,
            [(user["id"], cohort_id, role) for user, role in zip(users_to_add, roles)],
        )

        await conn.commit()


async def remove_members_from_cohort(cohort_id: int, member_ids: List[int]):
    members_in_cohort = await execute_db_operation(
        f"""
        SELECT user_id FROM {user_cohorts_table_name}
        WHERE cohort_id = ? AND user_id IN ({','.join(['?' for _ in member_ids])})
        """,
        (cohort_id, *member_ids),
        fetch_all=True,
    )

    if len(members_in_cohort) != len(member_ids):
        raise Exception("One or more members are not in the cohort")

    await execute_multiple_db_operations(
        [
            (
                f"""
            DELETE FROM {user_cohorts_table_name}
            WHERE user_id IN ({','.join(['?' for _ in member_ids])})
            AND cohort_id = ?
            """,
                (*member_ids, cohort_id),
            ),
        ]
    )


async def get_cohorts_for_org(org_id: int) -> List[Dict]:
    """Get all cohorts that belong to an organization"""
    results = await execute_db_operation(
        f"""
        SELECT c.id, c.name, o.id, o.name
        FROM {cohorts_table_name} c
        JOIN {organizations_table_name} o ON o.id = c.org_id
        WHERE o.id = ?
        """,
        (org_id,),
        fetch_all=True,
    )

    # Convert results into nested dict structure
    return [
        {"id": cohort_id, "name": cohort_name, "org_id": org_id, "org_name": org_name}
        for cohort_id, cohort_name, org_id, org_name in results
    ]


async def get_all_cohorts_for_org(org_id: int):
    cohorts = await execute_db_operation(
        f"""
        SELECT c.id, c.name
        FROM {cohorts_table_name} c
        WHERE c.org_id = ?
        ORDER BY c.id DESC
        """,
        (org_id,),
        fetch_all=True,
    )

    return [{"id": row[0], "name": row[1]} for row in cohorts]


async def get_cohort_by_id(cohort_id: int):
    # Fetch cohort details
    cohort = await execute_db_operation(
        f"""SELECT * FROM {cohorts_table_name} WHERE id = ?""",
        (cohort_id,),
        fetch_one=True,
    )

    if not cohort:
        return None

    # Get all users and their roles in the cohort
    members = await execute_db_operation(
        f"""
        SELECT DISTINCT u.id, u.email, uc.role
        FROM {users_table_name} u
        JOIN {user_cohorts_table_name} uc ON u.id = uc.user_id 
        WHERE uc.cohort_id = ?
        ORDER BY uc.role
        """,
        (cohort_id,),
        fetch_all=True,
    )

    cohort_data = {
        "id": cohort[0],
        "org_id": cohort[2],
        "name": cohort[1],
        "members": [
            {"id": member[0], "email": member[1], "role": member[2]}
            for member in members
        ],
    }

    return cohort_data


async def is_user_in_cohort(user_id: int, cohort_id: int):
    output = await execute_db_operation(
        f"""
        SELECT COUNT(*) > 0 FROM (
            SELECT 1
            FROM {user_cohorts_table_name} uc
            WHERE uc.user_id = ? AND uc.cohort_id = ?
            UNION
            SELECT 1 
            FROM {cohorts_table_name} c
            JOIN {organizations_table_name} o ON o.id = c.org_id
            JOIN {user_organizations_table_name} ou ON ou.org_id = o.id
            WHERE c.id = ? AND ou.user_id = ? AND ou.role IN ('admin', 'owner')
        )
        """,
        (user_id, cohort_id, cohort_id, user_id),
        fetch_one=True,
    )

    return output[0]


def format_user_cohort_group(group: Tuple):
    learners = []
    for id, email in zip(group[2].split(","), group[3].split(",")):
        learners.append({"id": int(id), "email": email})

    return {
        "id": group[0],
        "name": group[1],
        "learners": learners,
    }


async def get_cohort_analytics_metrics_for_tasks(cohort_id: int, task_ids: List[int]):
    results = await execute_db_operation(
        f"""
        WITH cohort_learners AS (
            SELECT u.id, u.email
            FROM {users_table_name} u
            JOIN {user_cohorts_table_name} uc ON u.id = uc.user_id
            WHERE uc.cohort_id = ? AND uc.role = 'learner'
        ),
        task_completion AS (
            SELECT
                cl.id as user_id,
                cl.email,
                ch.task_id,
                MAX(COALESCE(ch.is_solved, 0)) as is_solved
            FROM cohort_learners cl
            INNER JOIN {chat_history_table_name} ch
                ON cl.id = ch.user_id
                AND ch.task_id IN ({','.join('?' * len(task_ids))})
            INNER JOIN {tasks_table_name} t
                ON ch.task_id = t.id
            GROUP BY cl.id, cl.email, ch.task_id, t.name
        )
        SELECT
            user_id,
            email,
            GROUP_CONCAT(task_id) as task_ids,
            GROUP_CONCAT(is_solved) as task_completion
        FROM task_completion
        GROUP BY user_id, email
        """,
        (cohort_id, *task_ids),
        fetch_all=True,
    )

    user_metrics = []
    task_metrics = defaultdict(list)
    for row in results:
        user_task_completions = [
            int(x) if x else 0 for x in (row[3].split(",") if row[3] else [])
        ]
        user_task_ids = list(map(int, row[2].split(","))) if row[2] else []

        for task_id, task_completion in zip(user_task_ids, user_task_completions):
            task_metrics[task_id].append(task_completion)

        for task_id in task_ids:
            if task_id in user_task_ids:
                continue

            # this user did not attempt this task - add default
            task_metrics[task_id].append(0)

        num_completed = sum(user_task_completions)

        user_metrics.append(
            {
                "user_id": row[0],
                "email": row[1],
                "num_completed": num_completed,
            }
        )

    task_metrics = {task_id: task_metrics[task_id] for task_id in task_ids}

    for index, row in enumerate(user_metrics):
        for task_id in task_ids:
            row[f"task_{task_id}"] = task_metrics[task_id][index]

    return user_metrics


async def get_cohort_attempt_data_for_tasks(cohort_id: int, task_ids: List[int]):
    results = await execute_db_operation(
        f"""
        WITH cohort_learners AS (
            SELECT u.id, u.email
            FROM {users_table_name} u
            JOIN {user_cohorts_table_name} uc ON u.id = uc.user_id 
            WHERE uc.cohort_id = ? AND uc.role = 'learner'
        ),
        task_attempts AS (
            SELECT 
                cl.id as user_id,
                cl.email,
                ch.task_id,
                CASE WHEN COUNT(ch.id) > 0 THEN 1 ELSE 0 END as has_attempted
            FROM cohort_learners cl
            INNER JOIN {chat_history_table_name} ch 
                ON cl.id = ch.user_id 
                AND ch.task_id IN ({','.join('?' * len(task_ids))})
            INNER JOIN {tasks_table_name} t
                ON ch.task_id = t.id
            GROUP BY cl.id, cl.email, ch.task_id, t.name
        )
        SELECT 
            user_id,
            email,
            GROUP_CONCAT(task_id) as task_ids,
            GROUP_CONCAT(has_attempted) as task_attempts
        FROM task_attempts
        GROUP BY user_id, email
        """,
        (cohort_id, *task_ids),
        fetch_all=True,
    )

    user_metrics = []
    task_attempts = defaultdict(list)

    for row in results:
        user_task_attempts_data = [
            int(x) if x else 0 for x in (row[3].split(",") if row[3] else [])
        ]
        user_task_ids = list(map(int, row[2].split(","))) if row[2] else []

        for task_id, task_attempt in zip(user_task_ids, user_task_attempts_data):
            task_attempts[task_id].append(task_attempt)

        for task_id in task_ids:
            if task_id in user_task_ids:
                continue

            task_attempts[task_id].append(0)

        num_attempted = sum(user_task_attempts_data)

        user_metrics.append(
            {
                "user_id": row[0],
                "email": row[1],
                "num_attempted": num_attempted,
            }
        )

    task_attempts = {task_id: task_attempts[task_id] for task_id in task_ids}

    for index, row in enumerate(user_metrics):
        for task_id in task_ids:
            row[f"task_{task_id}"] = task_attempts[task_id][index]

    return user_metrics


def transfer_chat_history_to_user(prev_user_id: int, new_user_id: int):
    execute_db_operation(
        f"UPDATE {chat_history_table_name} SET user_id = ? WHERE user_id = ?",
        (new_user_id, prev_user_id),
    )

================
File: src/api/db/course.py
================
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timezone, timedelta
from dateutil.relativedelta import relativedelta
from uuid import uuid4
import json
import itertools
from collections import defaultdict
from api.config import (
    courses_table_name,
    course_generation_jobs_table_name,
    course_milestones_table_name,
    milestones_table_name,
    course_tasks_table_name,
    questions_table_name,
    tasks_table_name,
    scorecards_table_name,
    question_scorecards_table_name,
    cohorts_table_name,
    course_cohorts_table_name,
    uncategorized_milestone_name,
    task_generation_jobs_table_name,
    organizations_table_name,
    group_role_learner,
)
from api.db.task import (
    get_task,
    create_draft_task_for_course,
    update_learning_material_task,
    update_draft_quiz,
    get_scorecard,
    create_scorecard,
)
from api.db.utils import EnumEncoder, get_org_id_for_course
from api.utils.db import (
    execute_db_operation,
    get_new_db_connection,
    execute_multiple_db_operations,
    execute_many_db_operation,
    deserialise_list_from_str,
)
from api.db.user import get_user_cohorts, get_user_organizations
from api.db.org import get_org_by_id
from api.slack import send_slack_notification_for_new_course
from api.models import (
    GenerateCourseJobStatus,
    TaskType,
    TaskStatus,
    ScorecardStatus,
    GenerateTaskJobStatus,
)


async def calculate_milestone_unlock_dates(
    course_details: Dict, drip_config: Dict, joined_at: datetime | None = None
):
    if not drip_config or not drip_config.get("is_drip_enabled"):
        # All milestones unlocked
        for milestone in course_details["milestones"]:
            milestone["unlock_at"] = None
        return course_details

    start_date = None
    if drip_config.get("publish_at"):
        publish_at = drip_config["publish_at"]
        start_date = datetime.fromisoformat(publish_at)
    else:
        start_date = joined_at

    # Ensure start_date is always timezone-aware (UTC)
    if start_date and start_date.tzinfo is None:
        start_date = start_date.replace(tzinfo=timezone.utc)

    today = datetime.now(timezone.utc)
    freq_value = drip_config["frequency_value"]
    freq_unit = drip_config["frequency_unit"]
    non_empty_module_count = 0

    for milestone in course_details["milestones"]:
        milestone_tasks = milestone.get("tasks", [])

        # Skip empty milestones
        if not milestone_tasks or not start_date:
            milestone["unlock_at"] = None
            continue

        # Calculate unlock date based on non_empty_module_count
        unlock_date = start_date
        if non_empty_module_count > 0:
            if freq_unit == "minute":
                unlock_date = unlock_date + timedelta(
                    minutes=freq_value * non_empty_module_count
                )
            elif freq_unit == "hour":
                unlock_date = unlock_date + timedelta(
                    hours=freq_value * non_empty_module_count
                )
            elif freq_unit == "day":
                unlock_date = unlock_date + timedelta(
                    days=freq_value * non_empty_module_count
                )
            elif freq_unit == "week":
                unlock_date = unlock_date + timedelta(
                    weeks=freq_value * non_empty_module_count
                )
            elif freq_unit == "month":
                unlock_date = unlock_date + relativedelta(
                    months=freq_value * non_empty_module_count
                )
            elif freq_unit == "year":
                unlock_date = unlock_date + relativedelta(
                    years=freq_value * non_empty_module_count
                )
            else:
                raise ValueError(f"Invalid frequency unit: {freq_unit}")

        # First milestone with tasks is always unlocked
        if non_empty_module_count == 0:
            milestone["unlock_at"] = None
        else:
            # Check if this milestone should be locked
            is_locked = today < unlock_date
            unlock_at = unlock_date.isoformat() if is_locked else None
            milestone["unlock_at"] = unlock_at

        non_empty_module_count += 1

    return course_details


async def get_courses_for_cohort(
    cohort_id: int, include_tree: bool = False, joined_at: datetime | None = None
):
    courses = await execute_db_operation(
        f"""
        SELECT c.id, c.name, cc.is_drip_enabled, cc.frequency_value, cc.frequency_unit, cc.publish_at
        FROM {courses_table_name} c
        JOIN {course_cohorts_table_name} cc ON c.id = cc.course_id
        WHERE cc.cohort_id = ?
        """,
        (cohort_id,),
        fetch_all=True,
    )
    courses = [
        {
            "id": course[0],
            "name": course[1],
            "drip_config": {
                "is_drip_enabled": course[2],
                "frequency_value": course[3],
                "frequency_unit": course[4],
                "publish_at": course[5],
            },
        }
        for course in courses
    ]

    if not include_tree:
        return courses

    for index, course in enumerate(courses):
        course_details = await get_course(course["id"])
        course_details = await calculate_milestone_unlock_dates(
            course_details, course["drip_config"], joined_at
        )
        courses[index] = course_details

    return courses


async def store_course_generation_request(course_id: int, job_details: Dict) -> str:
    job_uuid = str(uuid4())

    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"INSERT INTO {course_generation_jobs_table_name} (uuid, course_id, status, job_details) VALUES (?, ?, ?, ?)",
            (
                job_uuid,
                course_id,
                str(GenerateCourseJobStatus.STARTED),
                json.dumps(job_details),
            ),
        )

        await conn.commit()

    return job_uuid


async def get_course_generation_job_details(job_uuid: str) -> Dict:
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"SELECT job_details FROM {course_generation_jobs_table_name} WHERE uuid = ?",
            (job_uuid,),
        )

        job = await cursor.fetchone()

        if job is None:
            raise ValueError("Job not found")

        return json.loads(job[0])


async def duplicate_course_to_org(course_id: int, org_id: int):
    course = await get_course(course_id)

    new_course_id = await create_course(course["name"], org_id)

    for milestone in course["milestones"]:
        new_milestone_id, _ = await add_milestone_to_course(
            new_course_id, milestone["name"], milestone["color"]
        )

        for task in milestone["tasks"]:
            task_details = await get_task(task["id"])

            new_task_id, _ = await create_draft_task_for_course(
                task_details["title"],
                task_details["type"],
                new_course_id,
                new_milestone_id,
            )

            if task_details["type"] == TaskType.LEARNING_MATERIAL:
                await update_learning_material_task(
                    new_task_id,
                    task_details["title"],
                    task_details["blocks"],
                    None,
                    TaskStatus.PUBLISHED,
                )
            else:
                # Handle quiz tasks with scorecard duplication
                scorecard_mapping = {}  # Map original scorecard_id to new scorecard_id

                for question in task_details["questions"]:
                    if question.get("scorecard_id") is not None:
                        original_scorecard_id = question["scorecard_id"]

                        # Check if we've already duplicated this scorecard
                        if original_scorecard_id not in scorecard_mapping:
                            # Get the original scorecard
                            original_scorecard = await get_scorecard(
                                original_scorecard_id
                            )

                            # Create new scorecard for the new org
                            new_scorecard = await create_scorecard(
                                {
                                    "title": original_scorecard["title"],
                                    "criteria": original_scorecard["criteria"],
                                    "org_id": org_id,
                                },
                                ScorecardStatus.PUBLISHED,
                            )

                            scorecard_mapping[original_scorecard_id] = new_scorecard[
                                "id"
                            ]

                        # Update question to use the new scorecard
                        question["scorecard_id"] = scorecard_mapping[
                            original_scorecard_id
                        ]

                await update_draft_quiz(
                    new_task_id,
                    task_details["title"],
                    task_details["questions"],
                    None,
                    TaskStatus.PUBLISHED,
                )


async def update_course_generation_job_status_and_details(
    job_uuid: str, status: GenerateCourseJobStatus, details: Dict
):
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"UPDATE {course_generation_jobs_table_name} SET status = ?, job_details = ? WHERE uuid = ?",
            (str(status), json.dumps(details, cls=EnumEncoder), job_uuid),
        )

        await conn.commit()


async def update_course_generation_job_status(
    job_uuid: str, status: GenerateCourseJobStatus
):
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"UPDATE {course_generation_jobs_table_name} SET status = ? WHERE uuid = ?",
            (str(status), job_uuid),
        )

        await conn.commit()


async def get_all_pending_course_structure_generation_jobs() -> List[Dict]:
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"SELECT uuid, course_id, job_details FROM {course_generation_jobs_table_name} WHERE status = ?",
            (str(GenerateCourseJobStatus.STARTED),),
        )

        return [
            {
                "uuid": row[0],
                "course_id": row[1],
                "job_details": json.loads(row[2]),
            }
            for row in await cursor.fetchall()
        ]


async def add_course_modules(course_id: int, modules: List[Dict]):
    import random

    module_ids = []
    for module in modules:
        color = random.choice(
            [
                "#2d3748",  # Slate blue
                "#433c4c",  # Deep purple
                "#4a5568",  # Cool gray
                "#312e51",  # Indigo
                "#364135",  # Forest green
                "#4c393a",  # Burgundy
                "#334155",  # Navy blue
                "#553c2d",  # Rust brown
                "#37303f",  # Plum
                "#3c4b64",  # Steel blue
                "#463c46",  # Mauve
                "#3c322d",  # Coffee
            ]
        )
        module_id, _ = await add_milestone_to_course(course_id, module["name"], color)
        module_ids.append(module_id)

    return module_ids


async def transfer_course_to_org(course_id: int, org_id: int):
    await execute_db_operation(
        f"UPDATE {courses_table_name} SET org_id = ? WHERE id = ?",
        (org_id, course_id),
    )

    milestones = await execute_db_operation(
        f"SELECT cm.milestone_id FROM {course_milestones_table_name} cm INNER JOIN {courses_table_name} c ON cm.course_id = c.id WHERE c.id = ?",
        (course_id,),
        fetch_all=True,
    )

    for milestone in milestones:
        await execute_db_operation(
            f"UPDATE {milestones_table_name} SET org_id = ? WHERE id = ?",
            (org_id, milestone[0]),
        )

    tasks = await execute_db_operation(
        f"SELECT ct.task_id FROM {course_tasks_table_name} ct INNER JOIN {courses_table_name} c ON ct.course_id = c.id WHERE c.id = ?",
        (course_id,),
        fetch_all=True,
    )

    task_ids = [task[0] for task in tasks]

    questions = await execute_db_operation(
        f"SELECT q.id FROM {questions_table_name} q INNER JOIN {tasks_table_name} t ON q.task_id = t.id WHERE t.id IN ({', '.join(map(str, task_ids))})",
        fetch_all=True,
    )

    question_ids = [question[0] for question in questions]

    scorecards = await execute_db_operation(
        f"SELECT qs.scorecard_id FROM {question_scorecards_table_name} qs INNER JOIN {questions_table_name} q ON qs.question_id = q.id WHERE q.id IN ({', '.join(map(str, question_ids))})",
        fetch_all=True,
    )

    scorecard_ids = [scorecard[0] for scorecard in scorecards]

    await execute_db_operation(
        f"UPDATE {scorecards_table_name} SET org_id = ? WHERE id IN ({', '.join(map(str, scorecard_ids))})",
        (org_id,),
    )

    await execute_db_operation(
        f"UPDATE {tasks_table_name} SET org_id = ? WHERE id IN ({', '.join(map(str, task_ids))})",
        (org_id,),
    )


async def get_cohorts_for_course(course_id: int):
    cohorts = await execute_db_operation(
        f"""
        SELECT ch.id, ch.name, cc.is_drip_enabled, cc.frequency_value, cc.frequency_unit, cc.publish_at
        FROM {cohorts_table_name} ch
        JOIN {course_cohorts_table_name} cc ON ch.id = cc.cohort_id
        WHERE cc.course_id = ?
        """,
        (course_id,),
        fetch_all=True,
    )

    return [
        {
            "id": cohort[0],
            "name": cohort[1],
            "drip_config": {
                "is_drip_enabled": cohort[2],
                "frequency_value": cohort[3],
                "frequency_unit": cohort[4],
                "publish_at": cohort[5],
            },
        }
        for cohort in cohorts
    ]


def drop_course_cohorts_table():
    execute_multiple_db_operations(
        [
            (f"DELETE FROM {course_cohorts_table_name}", ()),
            (f"DROP TABLE IF EXISTS {course_cohorts_table_name}", ()),
        ]
    )


def drop_courses_table():
    drop_course_cohorts_table()

    execute_multiple_db_operations(
        [
            (f"DELETE FROM {courses_table_name}", ()),
            (f"DROP TABLE IF EXISTS {courses_table_name}", ()),
        ]
    )


async def get_tasks_for_course(course_id: int, milestone_id: int = None):
    query = f"""SELECT t.id, t.name, COALESCE(m.name, '{uncategorized_milestone_name}') as milestone_name, t.verified, t.input_type, t.response_type, t.coding_language, ct.ordering, ct.id as course_task_id, ct.milestone_id, t.type
        FROM {tasks_table_name} t
        JOIN {course_tasks_table_name} ct ON ct.task_id = t.id 
        LEFT JOIN {milestones_table_name} m ON ct.milestone_id = m.id
        WHERE t.deleted_at IS NULL
        """

    params = []

    if milestone_id is not None:
        query += f" AND ct.course_id = ? AND ct.milestone_id = ?"
        params.extend([course_id, milestone_id])
    else:
        query += " AND ct.course_id = ?"
        params.append(course_id)

    query += " ORDER BY ct.ordering"

    tasks = await execute_db_operation(query, tuple(params), fetch_all=True)

    return [
        {
            "id": task[0],
            "name": task[1],
            "milestone": task[2],
            "verified": task[3],
            "input_type": task[4],
            "response_type": task[5],
            "coding_language": json.loads(task[6]) if task[6] else [],
            "ordering": task[7],
            "course_task_id": task[8],
            "milestone_id": task[9],
            "type": task[10],
        }
        for task in tasks
    ]


async def get_milestones_for_course(course_id: int):
    milestones = await execute_db_operation(
        f"SELECT cm.id, cm.milestone_id, m.name, cm.ordering FROM {course_milestones_table_name} cm JOIN {milestones_table_name} m ON cm.milestone_id = m.id WHERE cm.course_id = ? ORDER BY cm.ordering",
        (course_id,),
        fetch_all=True,
    )
    return [
        {
            "course_milestone_id": milestone[0],
            "id": milestone[1],
            "name": milestone[2],
            "ordering": milestone[3],
        }
        for milestone in milestones
    ]


async def get_all_courses_for_org(org_id: int):
    courses = await execute_db_operation(
        f"SELECT id, name FROM {courses_table_name} WHERE org_id = ? ORDER BY id DESC",
        (org_id,),
        fetch_all=True,
    )

    return [convert_course_db_to_dict(course) for course in courses]


async def delete_course(course_id: int):
    await execute_multiple_db_operations(
        [
            (
                f"DELETE FROM {course_cohorts_table_name} WHERE course_id = ?",
                (course_id,),
            ),
            (
                f"DELETE FROM {course_tasks_table_name} WHERE course_id = ?",
                (course_id,),
            ),
            (
                f"DELETE FROM {course_milestones_table_name} WHERE course_id = ?",
                (course_id,),
            ),
            (
                f"DELETE FROM {course_generation_jobs_table_name} WHERE course_id = ?",
                (course_id,),
            ),
            (
                f"DELETE FROM {task_generation_jobs_table_name} WHERE course_id = ?",
                (course_id,),
            ),
            (f"DELETE FROM {courses_table_name} WHERE id = ?", (course_id,)),
        ]
    )


def delete_all_courses_for_org(org_id: int):
    execute_multiple_db_operations(
        [
            (
                f"DELETE FROM {course_cohorts_table_name} WHERE course_id IN (SELECT id FROM {courses_table_name} WHERE org_id = ?)",
                (org_id,),
            ),
            (f"DELETE FROM {courses_table_name} WHERE org_id = ?", (org_id,)),
        ]
    )


async def swap_milestone_ordering_for_course(
    course_id: int, milestone_1_id: int, milestone_2_id: int
):
    # First, check if both milestones exist for the course
    milestone_entries = await execute_db_operation(
        f"SELECT milestone_id, ordering FROM {course_milestones_table_name} WHERE course_id = ? AND milestone_id IN (?, ?)",
        (course_id, milestone_1_id, milestone_2_id),
        fetch_all=True,
    )

    if len(milestone_entries) != 2:
        raise ValueError("One or both milestones do not exist for this course")

    # Get the IDs and orderings for the course_milestones entries
    milestone_1_id, milestone_1_ordering = milestone_entries[0]
    milestone_2_id, milestone_2_ordering = milestone_entries[1]

    update_params = [
        (milestone_2_ordering, milestone_1_id),
        (milestone_1_ordering, milestone_2_id),
    ]

    await execute_many_db_operation(
        f"UPDATE {course_milestones_table_name} SET ordering = ? WHERE id = ?",
        params_list=update_params,
    )


async def swap_task_ordering_for_course(course_id: int, task_1_id: int, task_2_id: int):
    # First, check if both tasks exist for the course
    task_entries = await execute_db_operation(
        f"SELECT task_id, milestone_id, ordering FROM {course_tasks_table_name} WHERE course_id = ? AND task_id IN (?, ?)",
        (course_id, task_1_id, task_2_id),
        fetch_all=True,
    )

    if len(task_entries) != 2:
        raise ValueError("One or both tasks do not exist for this course")

    # Get the IDs and orderings for the course_tasks entries
    task_1_id, task_1_milestone_id, task_1_ordering = task_entries[0]
    task_2_id, task_2_milestone_id, task_2_ordering = task_entries[1]

    if task_1_milestone_id != task_2_milestone_id:
        raise ValueError("Tasks are not in the same milestone")

    update_params = [
        (task_2_ordering, task_1_id),
        (task_1_ordering, task_2_id),
    ]

    await execute_many_db_operation(
        f"UPDATE {course_tasks_table_name} SET ordering = ? WHERE id = ?",
        params_list=update_params,
    )


async def create_course(name: str, org_id: int) -> int:
    org = await get_org_by_id(org_id)

    if not org:
        raise Exception(f"Organization with id '{org_id}' not found")

    course_id = await execute_db_operation(
        f"""
        INSERT INTO {courses_table_name} (name, org_id)
        VALUES (?, ?)
        """,
        (name, org_id),
        get_last_row_id=True,
    )

    await send_slack_notification_for_new_course(name, course_id, org["slug"], org_id)

    return course_id


def convert_course_db_to_dict(course: Tuple) -> Dict:
    result = {
        "id": course[0],
        "name": course[1],
    }

    if len(course) > 2:
        result["org"] = {
            "id": course[2],
            "name": course[3],
            "slug": course[4],
        }

    return result


async def get_course_org_id(course_id: int) -> int:
    course = await execute_db_operation(
        f"SELECT org_id FROM {courses_table_name} WHERE id = ?",
        (course_id,),
        fetch_one=True,
    )

    if not course:
        raise ValueError("Course not found")

    return course[0]


async def get_course(course_id: int, only_published: bool = True) -> Dict:
    course = await execute_db_operation(
        f"SELECT c.id, c.name, cgj.status as course_generation_status FROM {courses_table_name} c LEFT JOIN {course_generation_jobs_table_name} cgj ON c.id = cgj.course_id WHERE c.id = ?",
        (course_id,),
        fetch_one=True,
    )

    if not course:
        return None

    # Fix the milestones query to match the actual schema
    milestones = await execute_db_operation(
        f"""SELECT m.id, m.name, m.color, cm.ordering 
            FROM {course_milestones_table_name} cm
            JOIN milestones m ON cm.milestone_id = m.id
            WHERE cm.course_id = ? ORDER BY cm.ordering""",
        (course_id,),
        fetch_all=True,
    )

    # Fetch all tasks for this course
    tasks = await execute_db_operation(
        f"""SELECT t.id, t.title, t.type, t.status, t.scheduled_publish_at, ct.milestone_id, ct.ordering,
            (CASE WHEN t.type = '{TaskType.QUIZ}' THEN 
                (SELECT COUNT(*) FROM {questions_table_name} q 
                 WHERE q.task_id = t.id)
             ELSE NULL END) as num_questions,
            tgj.status as task_generation_status
            FROM {course_tasks_table_name} ct
            JOIN {tasks_table_name} t ON ct.task_id = t.id
            LEFT JOIN {task_generation_jobs_table_name} tgj ON t.id = tgj.task_id
            WHERE ct.course_id = ? AND t.deleted_at IS NULL
            {
                f"AND t.status = '{TaskStatus.PUBLISHED}' AND t.scheduled_publish_at IS NULL"
                if only_published
                else ""
            }
            ORDER BY ct.milestone_id, ct.ordering""",
        (course_id,),
        fetch_all=True,
    )

    # Group tasks by milestone_id
    tasks_by_milestone = defaultdict(list)
    for task in tasks:
        milestone_id = task[5]

        tasks_by_milestone[milestone_id].append(
            {
                "id": task[0],
                "title": task[1],
                "type": task[2],
                "status": task[3],
                "scheduled_publish_at": task[4],
                "ordering": task[6],
                "num_questions": task[7],
                "is_generating": task[8] is not None
                and task[8] == GenerateTaskJobStatus.STARTED,
            }
        )

    course_dict = {
        "id": course[0],
        "name": course[1],
        "course_generation_status": course[2],
    }
    course_dict["milestones"] = []

    for milestone in milestones:
        milestone_id = milestone[0]
        milestone_dict = {
            "id": milestone_id,
            "name": milestone[1],
            "color": milestone[2],
            "ordering": milestone[3],
            "tasks": tasks_by_milestone.get(milestone_id, []),
        }
        course_dict["milestones"].append(milestone_dict)

    return course_dict


async def update_course_name(course_id: int, name: str):
    await execute_db_operation(
        f"UPDATE {courses_table_name} SET name = ? WHERE id = ?",
        (name, course_id),
    )


async def check_and_insert_missing_course_milestones(
    course_tasks_to_add: List[Tuple[int, int, int]],
):
    # Find unique course, milestone pairs to validate they exist
    unique_course_milestone_pairs = {
        (course_id, milestone_id)
        for _, course_id, milestone_id in course_tasks_to_add
        if milestone_id is not None
    }

    if unique_course_milestone_pairs:
        # Verify all milestone IDs exist for their respective courses
        milestone_check = await execute_db_operation(
            f"""
            SELECT course_id, milestone_id FROM {course_milestones_table_name}
            WHERE (course_id, milestone_id) IN ({','.join(['(?,?)'] * len(unique_course_milestone_pairs))})
            """,
            tuple(itertools.chain(*unique_course_milestone_pairs)),
            fetch_all=True,
        )

        found_pairs = {(row[0], row[1]) for row in milestone_check}
        pairs_not_found = unique_course_milestone_pairs - found_pairs

        if pairs_not_found:
            # For each missing pair, get the max ordering for that course and increment
            for course_id, milestone_id in pairs_not_found:
                # Get current max ordering for this course
                max_ordering = (
                    await execute_db_operation(
                        f"SELECT COALESCE(MAX(ordering), -1) FROM {course_milestones_table_name} WHERE course_id = ?",
                        (course_id,),
                        fetch_one=True,
                    )
                )[0]

                # Insert with incremented ordering
                await execute_db_operation(
                    f"INSERT INTO {course_milestones_table_name} (course_id, milestone_id, ordering) VALUES (?, ?, ?)",
                    (course_id, milestone_id, max_ordering + 1),
                )


async def add_tasks_to_courses(course_tasks_to_add: List[Tuple[int, int, int]]):
    await check_and_insert_missing_course_milestones(course_tasks_to_add)

    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        # Group tasks by course_id
        course_to_tasks = defaultdict(list)
        for task_id, course_id, milestone_id in course_tasks_to_add:
            course_to_tasks[course_id].append((task_id, milestone_id))

        # For each course, get max ordering and insert tasks with incremented order
        for course_id, task_details in course_to_tasks.items():
            await cursor.execute(
                f"SELECT COALESCE(MAX(ordering), -1) FROM {course_tasks_table_name} WHERE course_id = ?",
                (course_id,),
            )
            max_ordering = (await cursor.fetchone())[0]

            # Insert tasks with incremented ordering
            values_to_insert = []
            for i, (task_id, milestone_id) in enumerate(task_details, start=1):
                values_to_insert.append(
                    (task_id, course_id, max_ordering + i, milestone_id)
                )

            await cursor.executemany(
                f"INSERT OR IGNORE INTO {course_tasks_table_name} (task_id, course_id, ordering, milestone_id) VALUES (?, ?, ?, ?)",
                values_to_insert,
            )

        await conn.commit()


async def remove_tasks_from_courses(course_tasks_to_remove: List[Tuple[int, int]]):
    await execute_many_db_operation(
        f"DELETE FROM {course_tasks_table_name} WHERE task_id = ? AND course_id = ?",
        params_list=course_tasks_to_remove,
    )


async def update_task_orders(task_orders: List[Tuple[int, int]]):
    await execute_many_db_operation(
        f"UPDATE {course_tasks_table_name} SET ordering = ? WHERE id = ?",
        params_list=task_orders,
    )


async def add_milestone_to_course(
    course_id: int, milestone_name: str, milestone_color: str
) -> Tuple[int, int]:
    org_id = await get_org_id_for_course(course_id)

    # Wrap the entire operation in a transaction
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        # Get the max ordering value for this course
        await cursor.execute(
            f"INSERT INTO {milestones_table_name} (name, color, org_id) VALUES (?, ?, ?)",
            (milestone_name, milestone_color, org_id),
        )

        milestone_id = cursor.lastrowid

        await cursor.execute(
            f"SELECT COALESCE(MAX(ordering), -1) FROM {course_milestones_table_name} WHERE course_id = ?",
            (course_id,),
        )
        max_ordering = await cursor.fetchone()

        # Set the new milestone's order to be the next value
        next_order = max_ordering[0] + 1 if max_ordering else 0

        await cursor.execute(
            f"INSERT INTO {course_milestones_table_name} (course_id, milestone_id, ordering) VALUES (?, ?, ?)",
            (course_id, milestone_id, next_order),
        )

        await conn.commit()

        return milestone_id, next_order


async def update_milestone_orders(milestone_orders: List[Tuple[int, int]]):
    await execute_many_db_operation(
        f"UPDATE {course_milestones_table_name} SET ordering = ? WHERE id = ?",
        params_list=milestone_orders,
    )


async def get_user_courses(user_id: int) -> List[Dict]:
    """
    Get all courses for a user based on different roles:
    1. Courses where the user is a learner or mentor through cohorts
    2. All courses from organizations where the user is an admin or owner

    Args:
        user_id: The ID of the user

    Returns:
        List of course dictionaries with their details and user's role
    """
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        # Get all courses where the user is a learner or mentor through cohorts
        user_cohorts = await get_user_cohorts(user_id)

        # Dictionary to track user's role in each course
        course_roles = {}
        course_to_cohort = {}

        # Add courses from user's cohorts with their roles
        for cohort in user_cohorts:
            cohort_id = cohort["id"]
            user_role_in_cohort = cohort.get("role")  # Get user's role in this cohort

            cohort_courses = await get_courses_for_cohort(cohort_id)
            for course in cohort_courses:
                course_id = course["id"]
                course_to_cohort[course_id] = cohort_id

                # Only update role if not already an admin/owner
                if course_id not in course_roles or course_roles[course_id] not in [
                    "admin",
                    "owner",
                ]:
                    course_roles[course_id] = user_role_in_cohort

        # Get organizations where the user is an admin or owner
        user_orgs = await get_user_organizations(user_id)
        admin_owner_org_ids = [
            org["id"] for org in user_orgs if org["role"] in ["admin", "owner"]
        ]

        # Add all courses from organizations where user is admin or owner
        for org_id in admin_owner_org_ids:
            org_courses = await get_all_courses_for_org(org_id)
            for course in org_courses:
                course_id = course["id"]
                # Admin/owner role takes precedence
                course_roles[course_id] = "admin"

        # If no courses found, return empty list
        if not course_roles:
            return []

        # Fetch detailed information for all course IDs
        courses = []
        for course_id, role in course_roles.items():
            # Fetch course from DB including org_id
            await cursor.execute(
                f"SELECT c.id, c.name, o.id, o.name, o.slug FROM {courses_table_name} c JOIN {organizations_table_name} o ON c.org_id = o.id WHERE c.id = ?",
                (course_id,),
            )
            course_row = await cursor.fetchone()
            if course_row:
                course_dict = convert_course_db_to_dict(course_row)
                course_dict["role"] = role  # Add user's role to the course dictionary

                if role == group_role_learner:
                    course_dict["cohort_id"] = course_to_cohort[course_id]

                courses.append(course_dict)

        return courses

================
File: src/api/db/milestone.py
================
from typing import Dict, Tuple
from api.utils.db import execute_db_operation, execute_multiple_db_operations
from api.config import (
    milestones_table_name,
    course_tasks_table_name,
    course_milestones_table_name,
    chat_history_table_name,
    tasks_table_name,
    uncategorized_milestone_name,
    uncategorized_milestone_color,
)


def convert_milestone_db_to_dict(milestone: Tuple) -> Dict:
    return {"id": milestone[0], "name": milestone[1], "color": milestone[2]}


async def get_all_milestones():
    milestones = await execute_db_operation(
        f"SELECT id, name, color FROM {milestones_table_name}", fetch_all=True
    )

    return [convert_milestone_db_to_dict(milestone) for milestone in milestones]


execute_db_operation


async def get_all_milestones_for_org(org_id: int):
    milestones = await execute_db_operation(
        f"SELECT id, name, color FROM {milestones_table_name} WHERE org_id = ?",
        (org_id,),
        fetch_all=True,
    )

    return [convert_milestone_db_to_dict(milestone) for milestone in milestones]


async def update_milestone(milestone_id: int, name: str):
    await execute_db_operation(
        f"UPDATE {milestones_table_name} SET name = ? WHERE id = ?",
        (name, milestone_id),
    )


async def delete_milestone(milestone_id: int):
    await execute_multiple_db_operations(
        [
            (f"DELETE FROM {milestones_table_name} WHERE id = ?", (milestone_id,)),
            (
                f"UPDATE {course_tasks_table_name} SET milestone_id = NULL WHERE milestone_id = ?",
                (milestone_id,),
            ),
            (
                f"DELETE FROM {course_milestones_table_name} WHERE milestone_id = ?",
                (milestone_id,),
            ),
        ]
    )


async def get_user_metrics_for_all_milestones(user_id: int, course_id: int):
    # Get milestones with tasks
    base_results = await execute_db_operation(
        f"""
        SELECT 
            m.id AS milestone_id,
            m.name AS milestone_name,
            m.color AS milestone_color,
            COUNT(DISTINCT t.id) AS total_tasks,
            (
                SELECT COUNT(DISTINCT ch.task_id)
                FROM {chat_history_table_name} ch
                WHERE ch.user_id = ?
                AND ch.is_solved = 1
                AND ch.task_id IN (
                    SELECT t2.id 
                    FROM {tasks_table_name} t2 
                    JOIN {course_tasks_table_name} ct2 ON t2.id = ct2.task_id
                    WHERE ct2.milestone_id = m.id 
                    AND ct2.course_id = ?
                    AND t2.deleted_at IS NULL
                )
            ) AS completed_tasks
        FROM 
            {milestones_table_name} m
        LEFT JOIN 
            {course_tasks_table_name} ct ON m.id = ct.milestone_id
        LEFT JOIN
            {tasks_table_name} t ON ct.task_id = t.id
        LEFT JOIN
            {course_milestones_table_name} cm ON m.id = cm.milestone_id AND ct.course_id = cm.course_id
        WHERE 
            t.verified = 1 AND ct.course_id = ? AND t.deleted_at IS NULL
        GROUP BY 
            m.id, m.name, m.color
        HAVING 
            COUNT(DISTINCT t.id) > 0
        ORDER BY 
            cm.ordering
        """,
        params=(user_id, course_id, course_id),
        fetch_all=True,
    )

    # Get tasks with null milestone_id
    null_milestone_results = await execute_db_operation(
        f"""
        SELECT 
            NULL AS milestone_id,
            '{uncategorized_milestone_name}' AS milestone_name,
            '{uncategorized_milestone_color}' AS milestone_color,
            COUNT(DISTINCT t.id) AS total_tasks,
            (
                SELECT COUNT(DISTINCT ch.task_id)
                FROM {chat_history_table_name} ch
                WHERE ch.user_id = ?
                AND ch.is_solved = 1
                AND ch.task_id IN (
                    SELECT t2.id 
                    FROM {tasks_table_name} t2 
                    JOIN {course_tasks_table_name} ct2 ON t2.id = ct2.task_id
                    WHERE ct2.milestone_id IS NULL 
                    AND ct2.course_id = ?
                    AND t2.deleted_at IS NULL
                )
            ) AS completed_tasks
        FROM 
            {tasks_table_name} t
        LEFT JOIN
            {course_tasks_table_name} ct ON t.id = ct.task_id
        WHERE 
            ct.milestone_id IS NULL 
            AND t.verified = 1 
            AND t.deleted_at IS NULL
            AND ct.course_id = ?
        HAVING
            COUNT(DISTINCT t.id) > 0
        ORDER BY 
            ct.ordering
        """,
        params=(user_id, course_id, course_id),
        fetch_all=True,
    )

    results = base_results + null_milestone_results

    return [
        {
            "milestone_id": row[0],
            "milestone_name": row[1],
            "milestone_color": row[2],
            "total_tasks": row[3],
            "completed_tasks": row[4],
        }
        for row in results
    ]

================
File: src/api/db/org.py
================
from typing import Literal, List, Dict, Tuple
import secrets
import hashlib

from api.utils.db import (
    get_new_db_connection,
    execute_db_operation,
    execute_multiple_db_operations,
)
from api.config import (
    organizations_table_name,
    user_organizations_table_name,
    org_api_keys_table_name,
)
from api.db.user import get_user_by_id, insert_or_return_user
from api.slack import (
    send_slack_notification_for_new_org,
    send_slack_notification_for_member_added_to_org,
)


async def get_all_orgs() -> List[Dict]:
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(f"SELECT id, name, slug FROM {organizations_table_name}")

        return [
            {
                "id": row[0],
                "name": row[1],
                "slug": row[2],
            }
            for row in await cursor.fetchall()
        ]


def generate_api_key(org_id: int):
    """Generate a new API key"""
    # Create a random API key
    identifier = secrets.token_urlsafe(32)

    api_key = f"org__{org_id}__{identifier}"

    # Hash it for storage
    hashed_key = hashlib.sha256(api_key.encode()).hexdigest()
    return api_key, hashed_key  # Return both - give api_key to user, store hashed_key


async def create_org_api_key(org_id: int) -> str:
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        api_key, hashed_key = generate_api_key(org_id)

        await cursor.execute(
            f"INSERT INTO {org_api_keys_table_name} (org_id, hashed_key) VALUES (?, ?)",
            (org_id, hashed_key),
        )

        await conn.commit()

        return api_key


async def get_org_id_from_api_key(api_key: str) -> int:
    api_key_parts = api_key.split("__")

    if len(api_key_parts) != 3:
        raise ValueError("Invalid API key")

    try:
        org_id = int(api_key_parts[1])
    except ValueError:
        raise ValueError("Invalid API key")

    rows = await execute_db_operation(
        f"SELECT hashed_key FROM {org_api_keys_table_name} WHERE org_id = ?",
        (org_id,),
        fetch_all=True,
    )

    if not rows:
        raise ValueError("Invalid API key")

    hashed_key = hashlib.sha256(api_key.encode()).hexdigest()

    for row in rows:
        if hashed_key == row[0]:
            return org_id

    raise ValueError("Invalid API key")


async def create_organization_with_user(org_name: str, slug: str, user_id: int):
    user = await get_user_by_id(user_id)

    if not user:
        raise Exception(f"User with id '{user_id}' not found")

    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"SELECT id FROM {organizations_table_name} WHERE slug = ?",
            (slug,),
        )
        existing_org = await cursor.fetchone()

        if existing_org:
            raise Exception(f"Organization with slug '{slug}' already exists")

        await cursor.execute(
            f"""INSERT INTO {organizations_table_name} 
                (slug, name)
                VALUES (?, ?)""",
            (slug, org_name),
        )

        org_id = cursor.lastrowid
        await add_user_to_org_by_user_id(cursor, user_id, org_id, "owner")
        await conn.commit()

    await send_slack_notification_for_new_org(org_name, org_id, user)

    return org_id


def convert_org_db_to_dict(org: Tuple):
    if not org:
        return None

    return {
        "id": org[0],
        "slug": org[1],
        "name": org[2],
        "logo_color": org[3],
        "openai_api_key": org[5],
        "openai_free_trial": org[6],
    }


async def get_org_by_id(org_id: int):
    org_details = await execute_db_operation(
        f"SELECT * FROM {organizations_table_name} WHERE id = ?",
        (org_id,),
        fetch_one=True,
    )

    return convert_org_db_to_dict(org_details)


async def get_org_by_slug(slug: str):
    org_details = await execute_db_operation(
        f"SELECT * FROM {organizations_table_name} WHERE slug = ?",
        (slug,),
        fetch_one=True,
    )
    return convert_org_db_to_dict(org_details)


async def get_hva_org_id():
    hva_org_id = await execute_db_operation(
        "SELECT id FROM organizations WHERE name = ?",
        ("HyperVerge Academy",),
        fetch_one=True,
    )

    if hva_org_id is None:
        return None

    hva_org_id = hva_org_id[0]
    return hva_org_id


async def get_hva_cohort_ids() -> List[int]:
    hva_org_id = await get_hva_org_id()

    if hva_org_id is None:
        return []

    cohorts = await execute_db_operation(
        "SELECT id FROM cohorts WHERE org_id = ?",
        (hva_org_id,),
        fetch_all=True,
    )
    return [cohort[0] for cohort in cohorts]


async def is_user_hva_learner(user_id: int) -> bool:
    hva_cohort_ids = await get_hva_cohort_ids()

    if not hva_cohort_ids:
        return False

    num_hva_users_matching_user_id = (
        await execute_db_operation(
            f"SELECT COUNT(*) FROM user_cohorts WHERE user_id = ? AND cohort_id IN ({', '.join(map(str, hva_cohort_ids))}) AND role = 'learner'",
            (user_id,),
            fetch_one=True,
        )
    )[0]

    return num_hva_users_matching_user_id > 0


async def get_hva_openai_api_key() -> str:
    org_details = await get_org_by_id(await get_hva_org_id())
    return org_details["openai_api_key"]


async def add_users_to_org_by_email(
    org_id: int,
    emails: List[str],
):
    org = await get_org_by_id(org_id)

    if not org:
        raise Exception("Organization not found")

    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        user_ids = []
        for email in emails:
            user = await insert_or_return_user(cursor, email)
            user_ids.append(user["id"])

            await send_slack_notification_for_member_added_to_org(
                user, org["slug"], org_id
            )

        # Check if any of the users are already in the organization
        placeholders = ", ".join(["?" for _ in user_ids])

        await cursor.execute(
            f"""SELECT user_id FROM {user_organizations_table_name} 
            WHERE org_id = ? AND user_id IN ({placeholders})
            """,
            (org_id, *user_ids),
        )

        existing_user_ids = await cursor.fetchall()

        if existing_user_ids:
            raise Exception(f"Some users already exist in organization")

        await cursor.executemany(
            f"""INSERT INTO {user_organizations_table_name}
                (user_id, org_id, role)
                VALUES (?, ?, ?)""",
            [(user_id, org_id, "admin") for user_id in user_ids],
        )
        await conn.commit()


async def remove_members_from_org(org_id: int, user_ids: List[int]):
    query = f"DELETE FROM {user_organizations_table_name} WHERE org_id = ? AND user_id IN ({', '.join(map(str, user_ids))})"
    await execute_db_operation(query, (org_id,))


def convert_user_organization_db_to_dict(user_organization: Tuple):
    return {
        "id": user_organization[0],
        "user_id": user_organization[1],
        "org_id": user_organization[2],
        "role": user_organization[3],
    }


async def get_org_members(org_id: int):
    org_users = await execute_db_operation(
        f"""SELECT uo.user_id, u.email, uo.role 
        FROM {user_organizations_table_name} uo
        JOIN users u ON uo.user_id = u.id 
        WHERE uo.org_id = ?""",
        (org_id,),
        fetch_all=True,
    )

    return [
        {
            "id": org_user[0],
            "email": org_user[1],
            "role": org_user[2],
        }
        for org_user in org_users
    ]


def drop_user_organizations_table():
    execute_multiple_db_operations(
        [
            (f"DELETE FROM {user_organizations_table_name}", ()),
            (f"DROP TABLE IF EXISTS {user_organizations_table_name}", ()),
        ]
    )


def drop_organizations_table():
    drop_user_organizations_table()

    execute_multiple_db_operations(
        [
            (f"DELETE FROM {organizations_table_name}", ()),
            (f"DROP TABLE IF EXISTS {organizations_table_name}", ()),
        ]
    )


async def update_org(org_id: int, org_name: str):
    await execute_db_operation(
        f"UPDATE {organizations_table_name} SET name = ? WHERE id = ?",
        (org_name, org_id),
    )


async def update_org_openai_api_key(
    org_id: int, encrypted_openai_api_key: str, is_free_trial: bool
):
    await execute_db_operation(
        f"UPDATE {organizations_table_name} SET openai_api_key = ?, openai_free_trial = ? WHERE id = ?",
        (encrypted_openai_api_key, is_free_trial, org_id),
    )


async def clear_org_openai_api_key(org_id: int):
    await execute_db_operation(
        f"UPDATE {organizations_table_name} SET openai_api_key = NULL WHERE id = ?",
        (org_id,),
    )


async def add_user_to_org_by_user_id(
    cursor,
    user_id: int,
    org_id: int,
    role: Literal["owner", "admin"],
):
    await cursor.execute(
        f"""INSERT INTO {user_organizations_table_name}
            (user_id, org_id, role)
            VALUES (?, ?, ?)""",
        (user_id, org_id, role),
    )

    return cursor.lastrowid

================
File: src/api/db/user.py
================
from typing import Dict, List, Tuple
from datetime import datetime, timezone, timedelta
from api.config import (
    users_table_name,
    cohorts_table_name,
    user_cohorts_table_name,
    organizations_table_name,
    group_role_learner,
    courses_table_name,
    chat_history_table_name,
    questions_table_name,
    course_tasks_table_name,
    course_cohorts_table_name,
    task_completions_table_name,
    user_organizations_table_name,
)
from api.slack import send_slack_notification_for_new_user
from api.models import UserCohort
from api.utils import generate_random_color, get_date_from_str
from api.utils.db import execute_db_operation, get_new_db_connection


async def update_user_email(email_1: str, email_2: str) -> None:
    await execute_db_operation(
        f"UPDATE {users_table_name} SET email = ? WHERE email = ?",
        (email_2, email_1),
    )


async def get_user_organizations(user_id: int):
    user_organizations = await execute_db_operation(
        f"""SELECT uo.org_id, o.name, uo.role, o.openai_api_key, o.openai_free_trial
        FROM {user_organizations_table_name} uo
        JOIN organizations o ON uo.org_id = o.id 
        WHERE uo.user_id = ? ORDER BY uo.id DESC""",
        (user_id,),
        fetch_all=True,
    )

    return [
        {
            "id": user_organization[0],
            "name": user_organization[1],
            "role": user_organization[2],
            "openai_api_key": user_organization[3],
            "openai_free_trial": user_organization[4],
        }
        for user_organization in user_organizations
    ]


async def get_user_org_cohorts(user_id: int, org_id: int) -> List[UserCohort]:
    """
    Get all the cohorts in the organization that the user is a member in
    """
    cohorts = await execute_db_operation(
        f"""SELECT c.id, c.name, uc.role, uc.joined_at
            FROM {cohorts_table_name} c
            JOIN {user_cohorts_table_name} uc ON c.id = uc.cohort_id
            WHERE uc.user_id = ? AND c.org_id = ?""",
        (user_id, org_id),
        fetch_all=True,
    )

    if not cohorts:
        return []

    return [
        {
            "id": cohort[0],
            "name": cohort[1],
            "role": cohort[2],
            "joined_at": cohort[3],
        }
        for cohort in cohorts
    ]


def drop_users_table():
    execute_db_operation(f"DELETE FROM {users_table_name}")
    execute_db_operation(f"DROP TABLE IF EXISTS {users_table_name}")


def convert_user_db_to_dict(user: Tuple) -> Dict:
    if not user:
        return

    return {
        "id": user[0],
        "email": user[1],
        "first_name": user[2],
        "middle_name": user[3],
        "last_name": user[4],
        "default_dp_color": user[5],
        "created_at": user[6],
    }


async def insert_or_return_user(
    cursor,
    email: str,
    given_name: str = None,
    family_name: str = None,
):
    """
    Inserts a new user or returns an existing user.

    Args:
        email: The user's email address.
        given_name: The user's given name (first and middle names).
        family_name: The user's family name (last name).
        cursor: An existing database cursor

    Returns:
        A dictionary representing the user.

    Raises:
        Any exception raised by the database operations.
    """

    if given_name is None:
        first_name = None
        middle_name = None
    else:
        given_name_parts = given_name.split(" ")
        first_name = given_name_parts[0]
        middle_name = " ".join(given_name_parts[1:])
        if not middle_name:
            middle_name = None

    # if user exists, no need to do anything, just return the user
    await cursor.execute(
        f"""SELECT * FROM {users_table_name} WHERE email = ?""",
        (email,),
    )

    user = await cursor.fetchone()

    if user:
        user = convert_user_db_to_dict(user)
        if user["first_name"] is None and first_name:
            user = await update_user(
                cursor,
                user["id"],
                first_name,
                middle_name,
                family_name,
                user["default_dp_color"],
            )

        return user

    # create a new user
    color = generate_random_color()
    await cursor.execute(
        f"""
        INSERT INTO {users_table_name} (email, default_dp_color, first_name, middle_name, last_name)
        VALUES (?, ?, ?, ?, ?)
    """,
        (email, color, first_name, middle_name, family_name),
    )

    await cursor.execute(
        f"""SELECT * FROM {users_table_name} WHERE email = ?""",
        (email,),
    )

    user = convert_user_db_to_dict(await cursor.fetchone())

    # Send Slack notification for new user
    await send_slack_notification_for_new_user(user)

    return user


async def update_user(
    cursor,
    user_id: str,
    first_name: str,
    middle_name: str,
    last_name: str,
    default_dp_color: str,
):
    await cursor.execute(
        f"UPDATE {users_table_name} SET first_name = ?, middle_name = ?, last_name = ?, default_dp_color = ? WHERE id = ?",
        (first_name, middle_name, last_name, default_dp_color, user_id),
    )

    user = await get_user_by_id(user_id)
    return user


async def get_all_users():
    users = await execute_db_operation(
        f"SELECT * FROM {users_table_name}",
        fetch_all=True,
    )

    return [convert_user_db_to_dict(user) for user in users]


async def get_user_by_email(email: str) -> Dict:
    user = await execute_db_operation(
        f"SELECT * FROM {users_table_name} WHERE email = ?", (email,), fetch_one=True
    )

    return convert_user_db_to_dict(user)


async def get_user_by_id(user_id: str) -> Dict:
    user = await execute_db_operation(
        f"SELECT * FROM {users_table_name} WHERE id = ?", (user_id,), fetch_one=True
    )

    return convert_user_db_to_dict(user)


async def get_user_cohorts(user_id: int) -> List[Dict]:
    """Get all cohorts (and the groups in each cohort) that the user is a part of along with their role in each group"""
    results = await execute_db_operation(
        f"""
        SELECT c.id, c.name, uc.role, o.id, o.name
        FROM {cohorts_table_name} c
        JOIN {user_cohorts_table_name} uc ON uc.cohort_id = c.id
        JOIN {organizations_table_name} o ON o.id = c.org_id
        WHERE uc.user_id = ?
        """,
        (user_id,),
        fetch_all=True,
    )

    # Convert results into nested dict structure
    return [
        {
            "id": cohort_id,
            "name": cohort_name,
            "org_id": org_id,
            "org_name": org_name,
            "role": role,
        }
        for cohort_id, cohort_name, role, org_id, org_name in results
    ]


async def get_user_active_in_last_n_days(user_id: int, n: int, cohort_id: int):
    activity_per_day = await execute_db_operation(
        f"""
    WITH chat_activity AS (
        SELECT DATE(datetime(created_at, '+5 hours', '+30 minutes')) as activity_date, COUNT(*) as count
        FROM {chat_history_table_name}
        WHERE user_id = ? 
        AND DATE(datetime(created_at, '+5 hours', '+30 minutes')) >= DATE(datetime('now', '+5 hours', '+30 minutes'), '-{n} days') 
        AND question_id IN (
            SELECT question_id 
            FROM {questions_table_name} 
            WHERE task_id IN (
                SELECT task_id 
                FROM {course_tasks_table_name} 
                WHERE course_id IN (
                    SELECT course_id 
                    FROM {course_cohorts_table_name} 
                    WHERE cohort_id = ?
                )
            )
        )
        GROUP BY activity_date
    ),
    task_activity AS (
        SELECT DATE(datetime(created_at, '+5 hours', '+30 minutes')) as activity_date, COUNT(*) as count
        FROM {task_completions_table_name}
        WHERE user_id = ? 
        AND DATE(datetime(created_at, '+5 hours', '+30 minutes')) >= DATE(datetime('now', '+5 hours', '+30 minutes'), '-{n} days')
        AND task_id IN (
            SELECT task_id 
            FROM {course_tasks_table_name} 
            WHERE course_id IN (
                SELECT course_id 
                FROM {course_cohorts_table_name} 
                WHERE cohort_id = ?
            )
        )
        GROUP BY activity_date
    )
    SELECT activity_date, count FROM chat_activity
    UNION
    SELECT activity_date, count FROM task_activity
    ORDER BY activity_date
    """,
        (user_id, cohort_id, user_id, cohort_id),
        fetch_all=True,
    )

    active_days = set()

    for date, count in activity_per_day:
        if count > 0:
            active_days.add(date)

    return list(active_days)


async def get_user_activity_for_year(user_id: int, year: int):
    # Get all chat messages for the user in the given year, grouped by day
    activity_per_day = await execute_db_operation(
        f"""
        SELECT 
            strftime('%j', datetime(timestamp, '+5 hours', '+30 minutes')) as day_of_year,
            COUNT(*) as message_count
        FROM {chat_history_table_name}
        WHERE user_id = ? 
        AND strftime('%Y', datetime(timestamp, '+5 hours', '+30 minutes')) = ?
        AND role = 'user'
        GROUP BY day_of_year
        ORDER BY day_of_year
        """,
        (user_id, str(year)),
        fetch_all=True,
    )

    # Convert to dictionary mapping day of year to message count
    activity_map = {int(day) - 1: count for day, count in activity_per_day}

    num_days = 366 if not year % 4 else 365

    data = [activity_map.get(index, 0) for index in range(num_days)]

    return data


def get_user_streak_from_usage_dates(user_usage_dates: List[str]) -> int:
    if not user_usage_dates:
        return []

    today = datetime.now(timezone(timedelta(hours=5, minutes=30))).date()
    current_streak = []

    user_usage_dates = sorted(
        list(
            set([get_date_from_str(date_str, "IST") for date_str in user_usage_dates])
        ),
        reverse=True,
    )

    for i, date in enumerate(user_usage_dates):
        if i == 0 and (today - date).days > 1:
            # the user has not used the app yesterday or today, so the streak is broken
            break
        if i == 0 or (user_usage_dates[i - 1] - date).days == 1:
            current_streak.append(date)
        else:
            break

    if not current_streak:
        return current_streak

    for index, date in enumerate(current_streak):
        current_streak[index] = datetime.strftime(date, "%Y-%m-%d")

    return current_streak


async def get_user_streak(user_id: int, cohort_id: int):
    user_usage_dates = await execute_db_operation(
        f"""
    SELECT MAX(datetime(created_at, '+5 hours', '+30 minutes')) as created_at
    FROM {chat_history_table_name}
    WHERE user_id = ? AND question_id IN (SELECT id FROM {questions_table_name} WHERE task_id IN (SELECT task_id FROM {course_tasks_table_name} WHERE course_id IN (SELECT course_id FROM {course_cohorts_table_name} WHERE cohort_id = ?)))
    GROUP BY DATE(datetime(created_at, '+5 hours', '+30 minutes'))
    
    UNION
    
    SELECT MAX(datetime(created_at, '+5 hours', '+30 minutes')) as created_at
    FROM {task_completions_table_name}
    WHERE user_id = ? AND task_id IN (
        SELECT task_id FROM {course_tasks_table_name} 
        WHERE course_id IN (SELECT course_id FROM {course_cohorts_table_name} WHERE cohort_id = ?)
    )
    GROUP BY DATE(datetime(created_at, '+5 hours', '+30 minutes'))
    
    ORDER BY created_at DESC
    """,
        (user_id, cohort_id, user_id, cohort_id),
        fetch_all=True,
    )

    return get_user_streak_from_usage_dates(
        [date_str for date_str, in user_usage_dates]
    )

================
File: src/api/db/utils.py
================
from typing import List, Dict
import json
from enum import Enum
from api.config import courses_table_name
from api.utils.db import execute_db_operation


class EnumEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, Enum):
            return obj.value
        return super().default(obj)


async def get_org_id_for_course(course_id: int):
    course = await execute_db_operation(
        f"SELECT org_id FROM {courses_table_name} WHERE id = ?",
        (course_id,),
        fetch_one=True,
    )

    if not course:
        raise ValueError("Course not found")

    return course[0]


def convert_blocks_to_right_format(blocks: List[Dict]) -> List[Dict]:
    for block in blocks:
        for content in block["content"]:
            content["type"] = "text"
            if "styles" not in content:
                content["styles"] = {}

    return blocks


def construct_description_from_blocks(
    blocks: List[Dict], nesting_level: int = 0
) -> str:
    """
    Constructs a textual description from a tree of block data.

    Args:
        blocks: A list of block dictionaries, potentially with nested children
        nesting_level: The current nesting level (used for proper indentation)

    Returns:
        A formatted string representing the content of the blocks
    """
    if not blocks:
        return ""

    description = ""
    indent = "    " * nesting_level  # 4 spaces per nesting level

    for block in blocks:
        block_type = block.get("type", "")
        content = block.get("content", [])
        children = block.get("children", [])

        # Process based on block type
        if block_type == "paragraph":
            # Content is a list of text objects
            if isinstance(content, list):
                paragraph_text = ""
                for text_obj in content:
                    if isinstance(text_obj, dict) and "text" in text_obj:
                        paragraph_text += text_obj["text"]
                if paragraph_text:
                    description += f"{indent}{paragraph_text}\n"

        elif block_type == "heading":
            level = block.get("props", {}).get("level", 1)
            if isinstance(content, list):
                heading_text = ""
                for text_obj in content:
                    if isinstance(text_obj, dict) and "text" in text_obj:
                        heading_text += text_obj["text"]
                if heading_text:
                    # Headings are typically not indented, but we'll respect nesting for consistency
                    description += f"{indent}{'#' * level} {heading_text}\n"

        elif block_type == "codeBlock":
            language = block.get("props", {}).get("language", "")
            if isinstance(content, list):
                code_text = ""
                for text_obj in content:
                    if isinstance(text_obj, dict) and "text" in text_obj:
                        code_text += text_obj["text"]
                if code_text:
                    description += (
                        f"{indent}```{language}\n{indent}{code_text}\n{indent}```\n"
                    )

        elif block_type in ["numberedListItem", "checkListItem", "bulletListItem"]:
            if isinstance(content, list):
                item_text = ""
                for text_obj in content:
                    if isinstance(text_obj, dict) and "text" in text_obj:
                        item_text += text_obj["text"]
                if item_text:
                    # Use proper list marker based on parent list type
                    if block_type == "numberedListItem":
                        marker = "1. "
                    elif block_type == "checkListItem":
                        marker = "- [ ] "
                    elif block_type == "bulletListItem":
                        marker = "- "

                    description += f"{indent}{marker}{item_text}\n"

        if children:
            child_description = construct_description_from_blocks(
                children, nesting_level + 1
            )
            description += child_description

    return description

================
File: src/api/routes/chat.py
================
from fastapi import APIRouter, HTTPException, Query
from typing import List, Dict
from api.db.chat import (
    store_messages as store_messages_in_db,
    get_all_chat_history as get_all_chat_history_from_db,
    get_task_chat_history_for_user as get_task_chat_history_for_user_from_db,
    delete_all_chat_history as delete_all_chat_history_from_db,
)
from api.models import (
    ChatMessage,
    StoreMessagesRequest,
)

router = APIRouter()


@router.post("/", response_model=List[ChatMessage])
async def store_messages(request: StoreMessagesRequest) -> List[ChatMessage]:
    return await store_messages_in_db(
        messages=request.messages,
        user_id=request.user_id,
        question_id=request.question_id,
        is_complete=request.is_complete,
    )


@router.get("/", response_model=List[ChatMessage])
async def get_all_chat_history(org_id: int) -> List[ChatMessage]:
    return await get_all_chat_history_from_db(org_id)


@router.get("/user/{user_id}/task/{task_id}", response_model=List[ChatMessage])
async def get_user_chat_history_for_task(
    user_id: int, task_id: int
) -> List[ChatMessage]:
    return await get_task_chat_history_for_user_from_db(
        user_id=user_id, task_id=task_id
    )


@router.delete("/")
async def delete_all_chat_history():
    await delete_all_chat_history_from_db()
    return {"message": "All chat history deleted"}

================
File: src/api/routes/hva.py
================
from fastapi import APIRouter
from api.db.org import (
    get_hva_openai_api_key as get_hva_openai_api_key_from_db,
    is_user_hva_learner as is_user_hva_learner_from_db,
    get_hva_org_id as get_hva_org_id_from_db,
)


router = APIRouter()


@router.get("/openai_key")
async def get_hva_openai_api_key():
    return await get_hva_openai_api_key_from_db()


@router.get("/is_user_hva_learner")
async def is_user_hva_learner(user_id: int):
    return await is_user_hva_learner_from_db(user_id)


@router.get("/org_id")
async def get_hva_org_id():
    return await get_hva_org_id_from_db()

================
File: src/api/routes/milestone.py
================
from fastapi import APIRouter, HTTPException
from typing import List, Dict
from api.db.milestone import (
    get_all_milestones_for_org as get_all_milestones_for_org_from_db,
    update_milestone as update_milestone_in_db,
    delete_milestone as delete_milestone_from_db,
    get_user_metrics_for_all_milestones as get_user_metrics_for_all_milestones_from_db,
)
from api.db.course import get_milestones_for_course as get_milestones_for_course_from_db
from api.models import UpdateMilestoneRequest

router = APIRouter()


@router.get("/")
async def get_all_milestones_for_org(org_id: int) -> List[Dict]:
    return await get_all_milestones_for_org_from_db(org_id)


@router.put("/{milestone_id}")
async def update_milestone(milestone_id: int, request: UpdateMilestoneRequest):
    await update_milestone_in_db(milestone_id, request.name)
    return {"message": "Milestone updated"}


@router.delete("/{milestone_id}")
async def delete_milestone(milestone_id: int):
    await delete_milestone_from_db(milestone_id)
    return {"message": "Milestone deleted"}


@router.get("/metrics/user/{user_id}/course/{course_id}")
async def get_user_metrics_for_all_milestones(
    user_id: int, course_id: int
) -> List[Dict]:
    return await get_user_metrics_for_all_milestones_from_db(user_id, course_id)


@router.get("/course/{course_id}")
async def get_milestones_for_course(course_id: int) -> List[Dict]:
    return await get_milestones_for_course_from_db(course_id)

================
File: src/api/routes/user.py
================
# --- START OF FILE sensai-api/sensai_backend/routes/user_routes.py ---
from fastapi import APIRouter, HTTPException
from typing import List, Dict
from datetime import datetime
from api.db.user import (
    get_user_by_id as get_user_by_id_from_db,
    update_user as update_user_in_db,
    get_user_cohorts as get_user_cohorts_from_db,
    get_user_activity_for_year as get_user_activity_for_year_from_db,
    get_user_active_in_last_n_days as get_user_active_in_last_n_days_from_db,
    get_user_streak as get_user_streak_from_db,
    get_user_organizations,
    get_user_org_cohorts as get_user_org_cohorts_from_db,
)
from api.db.course import get_user_courses as get_user_courses_from_db
from api.db.cohort import is_user_in_cohort as is_user_in_cohort_from_db
from api.utils.db import get_new_db_connection
from api.models import UserCourse, UserCohort, GetUserStreakResponse

router = APIRouter()


@router.get("/{user_id}")
async def get_user_by_id(user_id: int) -> Dict:
    user = await get_user_by_id_from_db(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    return user


@router.put("/{user_id}")
async def update_user(
    user_id: int,
    first_name: str,
    middle_name: str,
    last_name: str,
    default_dp_color: str,
):
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()
        user = await update_user_in_db(
            cursor, user_id, first_name, middle_name, last_name, default_dp_color
        )
        await conn.commit()

        if not user:
            raise HTTPException(status_code=404, detail="User not found")

    return user


@router.get("/{user_id}/cohorts")
async def get_user_cohorts(user_id: int) -> List[Dict]:
    return await get_user_cohorts_from_db(user_id)


@router.get("/{user_id}/activity/{year}")
async def get_user_activity_for_year(user_id: int, year: int) -> List[int]:
    return await get_user_activity_for_year_from_db(user_id, year)


@router.get("/{user_id}/active_days")
async def get_user_active_days(user_id: int, days: int, cohort_id: int) -> List[str]:
    return await get_user_active_in_last_n_days_from_db(user_id, days, cohort_id)


@router.get("/{user_id}/streak")
async def get_user_streak(user_id: int, cohort_id: int) -> GetUserStreakResponse:
    streak_days = await get_user_streak_from_db(user_id, cohort_id)

    streak_count = len(streak_days)

    # Get the user's activity for the last 3 days as we are displaying a week's activity
    # with the current day in the center
    active_days = await get_user_active_in_last_n_days_from_db(user_id, 3, cohort_id)

    return {
        "streak_count": streak_count,
        "active_days": active_days,
    }


@router.get("/{user_id}/cohort/{cohort_id}/present")
async def is_user_present_in_cohort(user_id: int, cohort_id: int) -> bool:
    return await is_user_in_cohort_from_db(user_id, cohort_id)


@router.get("/{user_id}/courses", response_model=List[UserCourse])
async def get_user_courses(user_id: int) -> List[UserCourse]:
    return await get_user_courses_from_db(user_id)


@router.get("/{user_id}/org/{org_id}/cohorts", response_model=List[UserCohort])
async def get_user_org_cohorts(user_id: int, org_id: int) -> List[UserCohort]:
    return await get_user_org_cohorts_from_db(user_id, org_id)


@router.get("/{user_id}/orgs")
async def get_user_orgs(user_id: int) -> List[Dict]:
    return await get_user_organizations(user_id)

================
File: src/api/.env.example
================
GOOGLE_CLIENT_ID=
OPENAI_API_KEY=

================
File: .dockerignore
================
venv/
src/lib/db/
__pycache__/
src/experimental/
*.pkl
src/venv/
src/logs/app.log
api/.env
**/.env
**/.env.aws
api/.env.aws
src/db/

================
File: codecov.yml
================
codecov:
  require_ci_to_pass: yes

comment:
  layout: "reach,diff,flags,files,footer"
  behavior: default
  require_changes: no
  require_base: no
  require_head: yes

================
File: pytest.ini
================
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
asyncio_mode = auto

[coverage:run]
source = src
omit = 
    */__pycache__/*
    */venv/*
    */test/*
    */tests/*
    setup.py
    src/api/websockets.py
    src/startup.py
    src/api/utils/phoenix.py
    src/api/db/migration.py
    src/api/routes/ai.py
    src/api/main.py
    src/api/config.py

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise NotImplementedError
    if __name__ == .__main__.:
    pass
    raise ImportError

================
File: README.md
================
# SensAI backend

[![codecov](https://codecov.io/gl/hvacademy/sensai-ai/branch/main/graph/badge.svg)](https://codecov.io/gl/hvacademy/sensai-ai)

SensAI is an AI-first Learning Management System (LMS) which enables educators to help them teacher smarter and reach further. SensAI coaches your students through questions that develop deeper thinking—just like you would, but for every student and all the time. This repository is the backend for SensAI. The frontend repository can be found [here](https://gitlab.com/hvacademy/sensai-frontend).

If you are using SensAI and have any feedback for us or want any help with using SensAI, please consider [joining our community](https://chat.whatsapp.com/LmiulDbWpcXIgqNK6fZyxe) of AI + Education builders and reaching out to us.

If you want to contribute to SensAI, please look at the `Contributing` section below.

<!-- 
To get started with using SensAI, please refer to our [Documentation](https://docs.sensai.hyperverge.org) which explains all the key features of SensAI along with demo videos and a step-by-step guide for common use cases. -->

Our public roadmap is live [here](https://hyperverge.notion.site/fa1dd0cef7194fa9bf95c28820dca57f?v=ec52c6a716e94df180dcc8ced3d87610). Go check it out and let us know what you think we should build next!

## Contributing
To learn more about making a contribution to SensAI, please see our [Contributing guide](./docs/CONTRIBUTING.md).

## Installation
Refer to the [INSTALL.md](./docs/INSTALL.md) file for instructions on how to install and run the backend locally.

## Testing
SensAI uses pytest for testing the API endpoints and measuring code coverage. To run the tests and generate coverage reports, follow these instructions:

### Installing Test Dependencies
```bash
pip install -r requirements-dev.txt
```

### Running Tests
To run all tests and generate a coverage report:
```bash
./run_tests.sh
```

### Coverage Reports
After running the full test suite with `run_tests.sh`, a HTML coverage report will be generated in the `coverage_html` directory. Open `coverage_html/index.html` in your browser to view the report.

### Codecov Integration
This project is integrated with [Codecov](https://codecov.io) for continuous monitoring of code coverage. Coverage reports are automatically generated and uploaded to Codecov when tests are run in the GitLab CI pipeline. The Codecov badge at the top of this README shows the current coverage status.

<!-- ## Deployment
Use the `Dockerfile` provided to build a docker image and deploy the image to whatever infra makes sense for you. We use an EC2 instance and you can refer to the `.gitlab-ci.yml` and `docker-compose.ai.demo.yml` files to understand how we do Continuous Deployment (CD). -->

## Community
We are building a community of creators, builders, teachers, learners, parents, entrepreneurs, non-profits and volunteers who are excited about the future of AI and education. If you identify as one and want to be part of it, consider [joining our community](https://chat.whatsapp.com/LmiulDbWpcXIgqNK6fZyxe).

================
File: requirements-dev.txt
================
-r requirements.txt
locust==2.32.4
gevent==24.11.1
pre-commit==4.0.1
pytest==8.3.5
pytest-cov==6.1.1
pytest-asyncio==0.26.0
codecov-cli==10.4.0

================
File: run_tests.sh
================
#!/bin/bash
set -e

# Activate virtual environment if needed
# source venv/bin/activate

# Run API tests with coverage
python -m pytest tests/api/ -v --cov=src --cov-report=term --cov-report=html:coverage_html --cov-report=xml:coverage.xml --cov-config=pytest.ini

echo "API tests completed successfully. Coverage reports are available in the coverage_html directory and coverage.xml file."

================
File: docs/INSTALL.md
================
## Installation

- Install `virtualenv`
- Create a new virtual environment (choose Python3.13+)
  ```
  virtualenv -p python3.13 venv
  ```
- Activate the virtual environment
  ```
  source venv/bin/activate
  ```
- Clone the repository:
  ```
  git clone https://gitlab.com/hvacademy/sensai-ai.git
  cd sensai-ai
  ```
- Install packages
  ```
  pip install -r requirements-dev.txt
  ```
- Install `ffmpeg` and `poppler`

  For Ubuntu:
  ```
  sudo apt-get update && sudo apt-get install ffmpeg poppler-utils
  ```
  For MacOS:
  ```
  brew install ffmpeg poppler
  export PATH="/path/to/poppler/bin:$PATH"
  ```
  You can get the path to poppler using `brew list poppler`
- Copy `src/api/.env.example` to `src/api/.env` and set the OpenAI credentials. Refer to [ENV.md](./ENV.md) for more details on the environment variables. 
- Copy `src/api/.env.aws.example` to `src/api/.env.aws` and set the AWS credentials.
- Initialize the database
  ```
  cd src; python startup.py
  ```
- Running the API locally
    ```
    cd src; uvicorn api.main:app --reload --port 8001
    ```

    The api will be hosted on http://localhost:8001.
    The docs will be available on http://localhost:8001/docs

### Additional steps for contributors
- Set up `pre-commit` hooks. `pre-commit` should already be installed while installing requirements from the `requirements-dev.txt` file.
  ```
  pre-commit install
  ```

================
File: src/api/db/__init__.py
================
import os
from os.path import exists
from api.utils.db import get_new_db_connection, check_table_exists, set_db_defaults
from api.config import (
    sqlite_db_path,
    chat_history_table_name,
    tasks_table_name,
    questions_table_name,
    cohorts_table_name,
    user_cohorts_table_name,
    milestones_table_name,
    users_table_name,
    organizations_table_name,
    user_organizations_table_name,
    courses_table_name,
    course_cohorts_table_name,
    course_tasks_table_name,
    uncategorized_milestone_name,
    course_milestones_table_name,
    group_role_learner,
    group_role_mentor,
    uncategorized_milestone_color,
    task_completions_table_name,
    scorecards_table_name,
    question_scorecards_table_name,
    course_generation_jobs_table_name,
    task_generation_jobs_table_name,
    org_api_keys_table_name,
    code_drafts_table_name,
)


async def create_organizations_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {organizations_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                slug TEXT NOT NULL UNIQUE,
                name TEXT NOT NULL,
                default_logo_color TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                openai_api_key TEXT,
                openai_free_trial BOOLEAN
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_org_slug ON {organizations_table_name} (slug)"""
    )


async def create_org_api_keys_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {org_api_keys_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                org_id INTEGER NOT NULL,
                hashed_key TEXT NOT NULL UNIQUE,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (org_id) REFERENCES {organizations_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_org_api_key_org_id ON {org_api_keys_table_name} (org_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_org_api_key_hashed_key ON {org_api_keys_table_name} (hashed_key)"""
    )


async def create_users_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {users_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                email TEXT NOT NULL UNIQUE,
                first_name TEXT,
                middle_name TEXT,
                last_name TEXT,
                default_dp_color TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )"""
    )


async def create_user_organizations_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {user_organizations_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER NOT NULL,
                org_id INTEGER NOT NULL,
                role TEXT NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(user_id, org_id),
                FOREIGN KEY (user_id) REFERENCES {users_table_name}(id) ON DELETE CASCADE,
                FOREIGN KEY (org_id) REFERENCES {organizations_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_user_org_user_id ON {user_organizations_table_name} (user_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_user_org_org_id ON {user_organizations_table_name} (org_id)"""
    )


async def create_cohort_tables(cursor):
    # Create a table to store cohorts
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {cohorts_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                org_id INTEGER NOT NULL,
                FOREIGN KEY (org_id) REFERENCES {organizations_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_cohort_org_id ON {cohorts_table_name} (org_id)"""
    )

    # Create a table to store users in cohorts
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {user_cohorts_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER NOT NULL,
                cohort_id INTEGER NOT NULL,
                role TEXT NOT NULL,
                joined_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(user_id, cohort_id),
                FOREIGN KEY (user_id) REFERENCES {users_table_name}(id) ON DELETE CASCADE,
                FOREIGN KEY (cohort_id) REFERENCES {cohorts_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_user_cohort_user_id ON {user_cohorts_table_name} (user_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_user_cohort_cohort_id ON {user_cohorts_table_name} (cohort_id)"""
    )


async def create_course_tasks_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {course_tasks_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                course_id INTEGER NOT NULL,
                ordering INTEGER NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                milestone_id INTEGER,
                UNIQUE(task_id, course_id),
                FOREIGN KEY (task_id) REFERENCES {tasks_table_name}(id) ON DELETE CASCADE,
                FOREIGN KEY (course_id) REFERENCES {courses_table_name}(id) ON DELETE CASCADE,
                FOREIGN KEY (milestone_id) REFERENCES {milestones_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_course_task_task_id ON {course_tasks_table_name} (task_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_course_task_course_id ON {course_tasks_table_name} (course_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_course_task_milestone_id ON {course_tasks_table_name} (milestone_id)"""
    )


async def create_course_milestones_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {course_milestones_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                course_id INTEGER NOT NULL,
                milestone_id INTEGER,
                ordering INTEGER NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(course_id, milestone_id),
                FOREIGN KEY (course_id) REFERENCES {courses_table_name}(id) ON DELETE CASCADE,
                FOREIGN KEY (milestone_id) REFERENCES {milestones_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_course_milestone_course_id ON {course_milestones_table_name} (course_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_course_milestone_milestone_id ON {course_milestones_table_name} (milestone_id)"""
    )


async def create_milestones_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {milestones_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                org_id INTEGER NOT NULL,
                name TEXT NOT NULL,
                color TEXT,
                FOREIGN KEY (org_id) REFERENCES {organizations_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_milestone_org_id ON {milestones_table_name} (org_id)"""
    )


async def create_courses_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {courses_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                org_id INTEGER NOT NULL,
                name TEXT NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (org_id) REFERENCES {organizations_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_course_org_id ON {courses_table_name} (org_id)"""
    )


async def create_course_cohorts_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {course_cohorts_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                course_id INTEGER NOT NULL,
                cohort_id INTEGER NOT NULL,
                is_drip_enabled BOOLEAN DEFAULT FALSE,
                frequency_value INTEGER,
                frequency_unit TEXT,
                publish_at DATETIME,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(course_id, cohort_id),
                FOREIGN KEY (course_id) REFERENCES {courses_table_name}(id) ON DELETE CASCADE,
                FOREIGN KEY (cohort_id) REFERENCES {cohorts_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_course_cohort_course_id ON {course_cohorts_table_name} (course_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_course_cohort_cohort_id ON {course_cohorts_table_name} (cohort_id)"""
    )


async def create_tasks_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {tasks_table_name} (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    org_id INTEGER NOT NULL,
                    type TEXT NOT NULL,
                    blocks TEXT,
                    title TEXT NOT NULL,
                    status TEXT NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    deleted_at DATETIME,
                    scheduled_publish_at DATETIME,
                    FOREIGN KEY (org_id) REFERENCES {organizations_table_name}(id) ON DELETE CASCADE
                )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_task_org_id ON {tasks_table_name} (org_id)"""
    )


async def create_questions_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {questions_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                type TEXT NOT NULL,
                blocks TEXT,
                answer TEXT,
                input_type TEXT NOT NULL,
                coding_language TEXT,
                generation_model TEXT,
                response_type TEXT NOT NULL,
                position INTEGER NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                deleted_at DATETIME,
                max_attempts INTEGER,
                is_feedback_shown BOOLEAN NOT NULL,
                context TEXT,
                title TEXT NOT NULL,
                FOREIGN KEY (task_id) REFERENCES {tasks_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_question_task_id ON {questions_table_name} (task_id)"""
    )


async def create_scorecards_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {scorecards_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                org_id INTEGER NOT NULL,
                title TEXT NOT NULL,
                criteria TEXT NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                status TEXT,
                FOREIGN KEY (org_id) REFERENCES {organizations_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_scorecard_org_id ON {scorecards_table_name} (org_id)"""
    )


async def create_question_scorecards_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {question_scorecards_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                question_id INTEGER NOT NULL,
                scorecard_id INTEGER NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (question_id) REFERENCES {questions_table_name}(id) ON DELETE CASCADE,
                FOREIGN KEY (scorecard_id) REFERENCES {scorecards_table_name}(id) ON DELETE CASCADE,
                UNIQUE(question_id, scorecard_id)
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_question_scorecard_question_id ON {question_scorecards_table_name} (question_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_question_scorecard_scorecard_id ON {question_scorecards_table_name} (scorecard_id)"""
    )


async def create_chat_history_table(cursor):
    await cursor.execute(
        f"""
                CREATE TABLE IF NOT EXISTS {chat_history_table_name} (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER NOT NULL,
                    question_id INTEGER NOT NULL,
                    role TEXT NOT NULL,
                    content TEXT,
                    response_type TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (question_id) REFERENCES {questions_table_name}(id),
                    FOREIGN KEY (user_id) REFERENCES {users_table_name}(id) ON DELETE CASCADE
                )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_chat_history_user_id ON {chat_history_table_name} (user_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_chat_history_question_id ON {chat_history_table_name} (question_id)"""
    )


async def create_task_completion_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {task_completions_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER NOT NULL,
                task_id INTEGER,
                question_id INTEGER,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (user_id) REFERENCES {users_table_name}(id) ON DELETE CASCADE,
                FOREIGN KEY (task_id) REFERENCES {tasks_table_name}(id) ON DELETE CASCADE,
                FOREIGN KEY (question_id) REFERENCES {questions_table_name}(id) ON DELETE CASCADE,
                UNIQUE(user_id, task_id),
                UNIQUE(user_id, question_id)
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_task_completion_user_id ON {task_completions_table_name} (user_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_task_completion_task_id ON {task_completions_table_name} (task_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_task_completion_question_id ON {task_completions_table_name} (question_id)"""
    )


async def create_course_generation_jobs_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {course_generation_jobs_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                uuid TEXT NOT NULL,
                course_id INTEGER NOT NULL,
                status TEXT NOT NULL,
                job_details TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (course_id) REFERENCES {courses_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_course_generation_job_course_id ON {course_generation_jobs_table_name} (course_id)"""
    )


async def create_task_generation_jobs_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {task_generation_jobs_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                uuid TEXT NOT NULL,
                task_id INTEGER NOT NULL,
                course_id INTEGER NOT NULL,
                status TEXT NOT NULL,
                job_details TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (task_id) REFERENCES {tasks_table_name}(id) ON DELETE CASCADE,
                FOREIGN KEY (course_id) REFERENCES {courses_table_name}(id) ON DELETE CASCADE
            )"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_task_generation_job_task_id ON {task_generation_jobs_table_name} (task_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX idx_task_generation_job_course_id ON {task_generation_jobs_table_name} (course_id)"""
    )


async def create_code_drafts_table(cursor):
    await cursor.execute(
        f"""CREATE TABLE IF NOT EXISTS {code_drafts_table_name} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_id INTEGER NOT NULL,
                question_id INTEGER NOT NULL,
                code TEXT NOT NULL,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(user_id, question_id),
                FOREIGN KEY (user_id) REFERENCES {users_table_name}(id) ON DELETE CASCADE,
                FOREIGN KEY (question_id) REFERENCES {questions_table_name}(id) ON DELETE CASCADE
            )"""
    )

    # Useful indexes for faster lookup
    await cursor.execute(
        f"""CREATE INDEX IF NOT EXISTS idx_code_drafts_user_id ON {code_drafts_table_name} (user_id)"""
    )

    await cursor.execute(
        f"""CREATE INDEX IF NOT EXISTS idx_code_drafts_question_id ON {code_drafts_table_name} (question_id)"""
    )


async def init_db():
    # Ensure the database folder exists
    db_folder = os.path.dirname(sqlite_db_path)
    if not os.path.exists(db_folder):
        os.makedirs(db_folder)

    if not exists(sqlite_db_path):
        # only set the defaults the first time
        set_db_defaults()

    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        if exists(sqlite_db_path):
            if not await check_table_exists(code_drafts_table_name, cursor):
                await create_code_drafts_table(cursor)

            await conn.commit()
            return

        try:
            await create_organizations_table(cursor)

            await create_org_api_keys_table(cursor)

            await create_users_table(cursor)

            await create_user_organizations_table(cursor)

            await create_milestones_table(cursor)

            await create_cohort_tables(cursor)

            await create_courses_table(cursor)

            await create_course_cohorts_table(cursor)

            await create_tasks_table(cursor)

            await create_questions_table(cursor)

            await create_scorecards_table(cursor)

            await create_question_scorecards_table(cursor)

            await create_chat_history_table(cursor)

            await create_task_completion_table(cursor)

            await create_course_tasks_table(cursor)

            await create_course_milestones_table(cursor)

            await create_course_generation_jobs_table(cursor)

            await create_task_generation_jobs_table(cursor)

            await create_code_drafts_table(cursor)

            await conn.commit()

        except Exception as exception:
            # delete db
            os.remove(sqlite_db_path)
            raise exception


async def delete_useless_tables():
    from api.config import (
        tags_table_name,
        task_tags_table_name,
        groups_table_name,
        user_groups_table_name,
        badges_table_name,
        task_scoring_criteria_table_name,
        cv_review_usage_table_name,
        tests_table_name,
    )

    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(f"DROP TABLE IF EXISTS {tags_table_name}")
        await cursor.execute(f"DROP TABLE IF EXISTS {task_tags_table_name}")
        await cursor.execute(f"DROP TABLE IF EXISTS {tests_table_name}")
        await cursor.execute(f"DROP TABLE IF EXISTS {groups_table_name}")
        await cursor.execute(f"DROP TABLE IF EXISTS {user_groups_table_name}")
        await cursor.execute(f"DROP TABLE IF EXISTS {badges_table_name}")
        await cursor.execute(f"DROP TABLE IF EXISTS {task_scoring_criteria_table_name}")
        await cursor.execute(f"DROP TABLE IF EXISTS {cv_review_usage_table_name}")

    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()
        await cursor.execute(f"PRAGMA table_info({user_cohorts_table_name})")
        user_columns = [col[1] for col in await cursor.fetchall()]

        if "joined_at" not in user_columns:
            await cursor.execute(f"DROP TABLE IF EXISTS {user_cohorts_table_name}_temp")
            await cursor.execute(
                f"""
                CREATE TABLE {user_cohorts_table_name}_temp (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER NOT NULL,
                    cohort_id INTEGER NOT NULL,
                    role TEXT NOT NULL,
                    joined_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(user_id, cohort_id),
                    FOREIGN KEY (user_id) REFERENCES {users_table_name}(id) ON DELETE CASCADE,
                    FOREIGN KEY (cohort_id) REFERENCES {cohorts_table_name}(id) ON DELETE CASCADE
                )
            """
            )
            await cursor.execute(
                f"INSERT INTO {user_cohorts_table_name}_temp (id, user_id, cohort_id, role) SELECT id, user_id, cohort_id, role FROM {user_cohorts_table_name}"
            )
            await cursor.execute(f"DROP TABLE {user_cohorts_table_name}")
            await cursor.execute(
                f"ALTER TABLE {user_cohorts_table_name}_temp RENAME TO {user_cohorts_table_name}"
            )

            # Recreate the indexes that were lost during table recreation
            await cursor.execute(
                f"CREATE INDEX idx_user_cohort_user_id ON {user_cohorts_table_name} (user_id)"
            )
            await cursor.execute(
                f"CREATE INDEX idx_user_cohort_cohort_id ON {user_cohorts_table_name} (cohort_id)"
            )

        await cursor.execute(f"PRAGMA table_info({course_cohorts_table_name})")
        course_columns = [col[1] for col in await cursor.fetchall()]

        for col, col_type, default in [
            ("is_drip_enabled", "BOOLEAN", "FALSE"),
            ("frequency_value", "INTEGER", None),
            ("frequency_unit", "TEXT", None),
            ("publish_at", "DATETIME", None),
        ]:
            if col not in course_columns:
                default_str = f" DEFAULT {default}" if default else ""
                await cursor.execute(
                    f"ALTER TABLE {course_cohorts_table_name} ADD COLUMN {col} {col_type}{default_str}"
                )

        await conn.commit()

================
File: src/api/db/migration.py
================
from typing import Dict, List
from api.db.task import (
    prepare_blocks_for_publish,
    update_learning_material_task,
    update_draft_quiz,
    create_draft_task_for_course,
)
from api.db.course import (
    update_course_name,
    add_course_modules,
)
from api.models import TaskStatus, TaskType, QuestionType, TaskAIResponseType
from api.utils.db import get_new_db_connection
from api.config import questions_table_name


def convert_content_to_blocks(content: str) -> List[Dict]:
    lines = content.split("\n")
    blocks = []
    for line in lines:
        blocks.append(
            {
                "type": "paragraph",
                "props": {
                    "textColor": "default",
                    "backgroundColor": "default",
                    "textAlignment": "left",
                },
                "content": [{"type": "text", "text": line, "styles": {}}],
                "children": [],
            }
        )

    return blocks


def convert_task_description_to_blocks(course_details: Dict):
    for milestone in course_details["milestones"]:
        for task in milestone["tasks"]:
            task["blocks"] = convert_content_to_blocks(task["description"])

    return course_details


async def migrate_learning_material(task_id: int, task_details: Dict):
    await update_learning_material_task(
        task_id,
        task_details["name"],
        task_details["blocks"],
        None,
        TaskStatus.PUBLISHED,  # TEMP: turn to draft later
    )


async def migrate_quiz(task_id: int, task_details: Dict):
    scorecards = []

    question = {}

    question["type"] = (
        QuestionType.OPEN_ENDED
        if task_details["response_type"] == "report"
        else QuestionType.OBJECTIVE
    )

    question["blocks"] = task_details["blocks"]

    question["answer"] = (
        convert_content_to_blocks(task_details["answer"])
        if task_details.get("answer")
        else None
    )
    question["input_type"] = (
        "audio" if task_details["input_type"] == "audio" else "text"
    )
    question["response_type"] = task_details["response_type"]
    question["coding_languages"] = task_details.get("coding_language", None)
    question["generation_model"] = None
    question["context"] = (
        {
            "blocks": prepare_blocks_for_publish(
                convert_content_to_blocks(task_details["context"])
            ),
            "linkedMaterialIds": None,
        }
        if task_details.get("context")
        else None
    )
    question["max_attempts"] = (
        1 if task_details["response_type"] == TaskAIResponseType.EXAM else None
    )
    question["is_feedback_shown"] = (
        False if task_details["response_type"] == TaskAIResponseType.EXAM else True
    )

    if task_details["response_type"] == "report":
        scoring_criteria = task_details["scoring_criteria"]

        scorecard_criteria = []

        for criterion in scoring_criteria:
            scorecard_criteria.append(
                {
                    "name": criterion["category"],
                    "description": criterion["description"],
                    "min_score": criterion["range"][0],
                    "max_score": criterion["range"][1],
                }
            )

        is_new_scorecard = True
        scorecard_id = None
        for index, existing_scorecard in enumerate(scorecards):
            if existing_scorecard == scorecard_criteria:
                is_new_scorecard = False
                scorecard_id = index
                break

        question["scorecard"] = {
            "id": len(scorecards) if is_new_scorecard else scorecard_id,
            "title": "Scorecard",
            "criteria": scorecard_criteria,
        }

        if is_new_scorecard:
            scorecards.append(scorecard_criteria)
    else:
        question["scorecard"] = None

    question["scorecard_id"] = None

    await update_draft_quiz(
        task_id,
        task_details["name"],
        [question],
        None,
        TaskStatus.PUBLISHED,  # TEMP: turn to draft later
    )


async def migrate_course(course_id: int, course_details: Dict):
    await update_course_name(course_id, course_details["name"])

    module_ids = await add_course_modules(course_id, course_details["milestones"])

    for index, milestone in enumerate(course_details["milestones"]):
        for task in milestone["tasks"]:
            if task["type"] == "reading_material":
                task["type"] = str(TaskType.LEARNING_MATERIAL)
            else:
                task["type"] = str(TaskType.QUIZ)

            task_id, _ = await create_draft_task_for_course(
                task["name"],
                task["type"],
                course_id,
                module_ids[index],
            )

            if task["type"] == TaskType.LEARNING_MATERIAL:
                await migrate_learning_material(task_id, task)
            else:
                await migrate_quiz(task_id, task)


async def migrate_task_description_to_blocks(course_details: Dict):
    from api.routes.ai import migrate_content_to_blocks
    from api.utils.concurrency import async_batch_gather

    coroutines = []

    for milestone in course_details["milestones"]:
        for task in milestone["tasks"]:
            coroutines.append(migrate_content_to_blocks(task["description"]))
        #     break
        # break

    results = await async_batch_gather(coroutines)

    current_index = 0
    for milestone in course_details["milestones"]:
        for task in milestone["tasks"]:
            task["blocks"] = results[current_index]
            current_index += 1
        #     break
        # break

    return course_details


async def add_title_column_to_questions():
    """
    Migration: Adds a 'title' column to the questions table and updates all existing rows
    to have title = f"Question {position+1}".
    """
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()
        # Check if 'title' column already exists
        await cursor.execute(f"PRAGMA table_info({questions_table_name})")
        columns = [col[1] for col in await cursor.fetchall()]
        if "title" not in columns:
            await cursor.execute(f"ALTER TABLE {questions_table_name} ADD COLUMN title TEXT NOT NULL DEFAULT ''")
        # Update all rows to set title = 'Question {position+1}'
        await cursor.execute(f"UPDATE {questions_table_name} SET title = 'Question ' || (position + 1)")
        await conn.commit()

================
File: src/api/db/task.py
================
from typing import Tuple, List, Dict
import json
from datetime import datetime, timedelta, timezone
import uuid
from api.db.utils import get_org_id_for_course
from api.config import (
    tasks_table_name,
    course_tasks_table_name,
    scorecards_table_name,
    question_scorecards_table_name,
    courses_table_name,
    milestones_table_name,
    organizations_table_name,
    questions_table_name,
    chat_history_table_name,
    course_cohorts_table_name,
    task_completions_table_name,
    task_generation_jobs_table_name,
)
from api.utils.db import (
    get_new_db_connection,
    execute_db_operation,
    serialise_list_to_str,
)
from api.models import (
    TaskType,
    TaskStatus,
    ScorecardStatus,
    LearningMaterialTask,
    LeaderboardViewType,
    GenerateTaskJobStatus,
    TaskAIResponseType,
    BaseScorecard,
)
from api.db.utils import convert_blocks_to_right_format


async def create_draft_task_for_course(
    title: str,
    type: str,
    course_id: int,
    milestone_id: int,
    ordering: int = None,
) -> Tuple[int, int]:
    org_id = await get_org_id_for_course(course_id)

    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        query = f"INSERT INTO {tasks_table_name} (org_id, type, title, status) VALUES (?, ?, ?, ?)"

        await cursor.execute(
            query,
            (org_id, str(type), title, "draft"),
        )

        task_id = cursor.lastrowid

        if ordering is not None:
            # Shift all tasks at or after the given ordering down by 1
            await cursor.execute(
                f"""
                UPDATE {course_tasks_table_name}
                SET ordering = ordering + 1
                WHERE course_id = ? AND milestone_id = ? AND ordering >= ?
                """,
                (course_id, milestone_id, ordering),
            )
            insert_ordering = ordering
        else:
            # Get the maximum ordering value for this milestone
            await cursor.execute(
                f"SELECT COALESCE(MAX(ordering), -1) FROM {course_tasks_table_name} WHERE course_id = ? AND milestone_id = ?",
                (course_id, milestone_id),
            )
            max_ordering = await cursor.fetchone()
            insert_ordering = max_ordering[0] + 1 if max_ordering else 0

        await cursor.execute(
            f"INSERT INTO {course_tasks_table_name} (course_id, task_id, milestone_id, ordering) VALUES (?, ?, ?, ?)",
            (course_id, task_id, milestone_id, insert_ordering),
        )

        await conn.commit()

        # Compute the "visible" ordering (i.e., the index among non-deleted tasks)
        visible_ordering_row = await execute_db_operation(
            f"""
            SELECT COUNT(*) FROM {course_tasks_table_name} ct
            INNER JOIN {tasks_table_name} t ON ct.task_id = t.id
            WHERE ct.course_id = ? AND ct.milestone_id = ? AND ct.ordering < ? AND t.deleted_at IS NULL
            """,
            (course_id, milestone_id, insert_ordering),
            fetch_one=True,
        )

        visible_ordering = (
            visible_ordering_row[0] if visible_ordering_row else insert_ordering
        )

        return task_id, visible_ordering


async def get_all_learning_material_tasks_for_course(course_id: int):
    query = f"""
    SELECT t.id, t.title, t.type, t.status, t.scheduled_publish_at
    FROM {tasks_table_name} t
    INNER JOIN {course_tasks_table_name} ct ON t.id = ct.task_id
    WHERE ct.course_id = ? AND t.deleted_at IS NULL AND t.type = '{TaskType.LEARNING_MATERIAL}' AND t.status = '{TaskStatus.PUBLISHED}'
    ORDER BY ct.ordering ASC
    """

    query_params = (course_id,)

    tasks = await execute_db_operation(query, query_params, fetch_all=True)

    return [
        {
            "id": task[0],
            "title": task[1],
            "type": task[2],
            "status": task[3],
            "scheduled_publish_at": task[4],
        }
        for task in tasks
    ]


def convert_question_db_to_dict(question) -> Dict:
    result = {
        "id": question[0],
        "type": question[1],
        "blocks": json.loads(question[2]) if question[2] else [],
        "answer": json.loads(question[3]) if question[3] else None,
        "input_type": question[4],
        "response_type": question[5],
        "scorecard_id": question[6],
        "context": json.loads(question[7]) if question[7] else None,
        "coding_languages": json.loads(question[8]) if question[8] else None,
        "max_attempts": question[9],
        "is_feedback_shown": question[10],
        "title": question[11],
    }

    return result


async def get_scorecard(scorecard_id: int) -> Dict:
    if scorecard_id is None:
        return

    scorecard = await execute_db_operation(
        f"SELECT id, title, criteria, status FROM {scorecards_table_name} WHERE id = ?",
        (scorecard_id,),
        fetch_one=True,
    )

    if not scorecard:
        return None

    return {
        "id": scorecard[0],
        "title": scorecard[1],
        "criteria": json.loads(scorecard[2]),
        "status": scorecard[3],
    }


async def get_question(question_id: int) -> Dict:
    question = await execute_db_operation(
        f"""
        SELECT q.id, q.type, q.blocks, q.answer, q.input_type, q.response_type, qs.scorecard_id, q.context, q.coding_language, q.max_attempts, q.is_feedback_shown, q.title
        FROM {questions_table_name} q
        LEFT JOIN {question_scorecards_table_name} qs ON q.id = qs.question_id
        WHERE q.id = ?
        """,
        (question_id,),
        fetch_one=True,
    )

    if not question:
        return None

    question = convert_question_db_to_dict(question)

    if question["scorecard_id"] is not None:
        question["scorecard"] = await get_scorecard(question["scorecard_id"])

    return question


async def get_basic_task_details(task_id: int) -> Dict:
    task = await execute_db_operation(
        f"""
        SELECT id, title, type, status, org_id, scheduled_publish_at
        FROM {tasks_table_name}
        WHERE id = ? AND deleted_at IS NULL
        """,
        (task_id,),
        fetch_one=True,
    )

    if not task:
        return None

    return {
        "id": task[0],
        "title": task[1],
        "type": task[2],
        "status": task[3],
        "org_id": task[4],
        "scheduled_publish_at": task[5],
    }


async def get_task(task_id: int):
    task_data = await get_basic_task_details(task_id)

    if not task_data:
        return None

    if task_data["type"] == TaskType.LEARNING_MATERIAL:
        result = await execute_db_operation(
            f"SELECT blocks FROM {tasks_table_name} WHERE id = ?",
            (task_id,),
            fetch_one=True,
        )

        task_data["blocks"] = json.loads(result[0]) if result[0] else []

    elif task_data["type"] == TaskType.QUIZ:
        questions = await execute_db_operation(
            f"""
            SELECT q.id, q.type, q.blocks, q.answer, q.input_type, q.response_type, qs.scorecard_id, q.context, q.coding_language, q.max_attempts, q.is_feedback_shown, q.title
            FROM {questions_table_name} q
            LEFT JOIN {question_scorecards_table_name} qs ON q.id = qs.question_id
            WHERE task_id = ? ORDER BY position ASC
            """,
            (task_id,),
            fetch_all=True,
        )

        task_data["questions"] = [
            convert_question_db_to_dict(question) for question in questions
        ]

    return task_data


async def get_task_metadata(task_id: int) -> Dict:
    result = await execute_db_operation(
        f"""
        SELECT c.id as course_id, c.name as course_name, m.id as milestone_id, m.name as milestone_name, o.id as org_id, o.name as org_name
        FROM {course_tasks_table_name} ct
        JOIN {courses_table_name} c ON ct.course_id = c.id
        JOIN {milestones_table_name} m ON ct.milestone_id = m.id
        JOIN {organizations_table_name} o ON c.org_id = o.id
        WHERE ct.task_id = ?
        LIMIT 1
        """,
        (task_id,),
        fetch_one=True,
    )

    if not result:
        return None

    return {
        "course": {
            "id": result[0],
            "name": result[1],
        },
        "milestone": {
            "id": result[2],
            "name": result[3],
        },
        "org": {
            "id": result[4],
            "name": result[5],
        },
    }


async def does_task_exist(task_id: int) -> bool:
    task = await execute_db_operation(
        f"""
        SELECT id
        FROM {tasks_table_name}
        WHERE id = ? AND deleted_at IS NULL
        """,
        (task_id,),
        fetch_one=True,
    )

    return task is not None


def prepare_blocks_for_publish(blocks: List[Dict]) -> List[Dict]:
    for index, block in enumerate(blocks):
        if "id" not in block or block["id"] is None:
            block["id"] = str(uuid.uuid4())

        block["position"] = index

    return blocks


async def update_learning_material_task(
    task_id: int,
    title: str,
    blocks: List[Dict],
    scheduled_publish_at: datetime,
    status: TaskStatus = TaskStatus.PUBLISHED,
) -> LearningMaterialTask:
    if not await does_task_exist(task_id):
        return False

    # Execute all operations in a single transaction
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"UPDATE {tasks_table_name} SET blocks = ?, status = ?, title = ?, scheduled_publish_at = ? WHERE id = ?",
            (
                json.dumps(prepare_blocks_for_publish(blocks)),
                str(status),
                title,
                scheduled_publish_at,
                task_id,
            ),
        )

        await conn.commit()

        return await get_task(task_id)


async def update_draft_quiz(
    task_id: int,
    title: str,
    questions: List[Dict],
    scheduled_publish_at: datetime,
    status: TaskStatus = TaskStatus.PUBLISHED,
):
    if not await does_task_exist(task_id):
        return False

    task = await get_basic_task_details(task_id)

    if not task:
        return False

    org_id = task["org_id"]

    scorecard_uuid_to_id = {}

    # Execute all operations in a single transaction
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"DELETE FROM {question_scorecards_table_name} WHERE question_id IN (SELECT id FROM {questions_table_name} WHERE task_id = ?)",
            (task_id,),
        )

        await cursor.execute(
            f"DELETE FROM {questions_table_name} WHERE task_id = ?",
            (task_id,),
        )

        scorecards_to_publish = []

        for index, question in enumerate(questions):
            if not isinstance(question, dict):
                question = question.model_dump()

            await cursor.execute(
                f"""
                INSERT INTO {questions_table_name} (task_id, type, blocks, answer, input_type, response_type, coding_language, generation_model, context, position, max_attempts, is_feedback_shown, title) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    task_id,
                    str(question["type"]),
                    json.dumps(prepare_blocks_for_publish(question["blocks"])),
                    (
                        json.dumps(prepare_blocks_for_publish(question["answer"]))
                        if question["answer"]
                        else None
                    ),
                    str(question["input_type"]),
                    str(question["response_type"]),
                    (
                        json.dumps(question["coding_languages"])
                        if question["coding_languages"]
                        else None
                    ),
                    None,
                    json.dumps(question["context"]) if question["context"] else None,
                    index,
                    question["max_attempts"],
                    question["is_feedback_shown"],
                    question["title"],
                ),
            )

            question_id = cursor.lastrowid

            scorecard_id = None
            if question.get("scorecard_id") is not None:
                scorecard_id = question["scorecard_id"]

                await cursor.execute(
                    f"""
                    INSERT INTO {question_scorecards_table_name} (question_id, scorecard_id) VALUES (?, ?)
                    """,
                    (question_id, scorecard_id),
                )

                await cursor.execute(
                    f"SELECT id FROM {scorecards_table_name} WHERE id = ? AND status = ?",
                    (question["scorecard_id"], str(ScorecardStatus.DRAFT)),
                )

                result = await cursor.fetchone()

                if result:
                    scorecards_to_publish.append(question["scorecard_id"])

        if scorecards_to_publish:
            await cursor.execute(
                f"UPDATE {scorecards_table_name} SET status = ? WHERE id IN ({','.join(map(str, scorecards_to_publish))})",
                (str(ScorecardStatus.PUBLISHED),),
            )

        # Update task status to published
        await cursor.execute(
            f"UPDATE {tasks_table_name} SET status = ?, title = ?, scheduled_publish_at = ? WHERE id = ?",
            (str(status), title, scheduled_publish_at, task_id),
        )

        await conn.commit()

        return await get_task(task_id)


async def update_published_quiz(
    task_id: int, title: str, questions: List[Dict], scheduled_publish_at: datetime
):
    if not await does_task_exist(task_id):
        return False

    # Execute all operations in a single transaction
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        scorecards_to_publish = []

        for question in questions:
            question = question.model_dump()

            await cursor.execute(
                f"""
                UPDATE {questions_table_name} SET blocks = ?, answer = ?, input_type = ?, coding_language = ?, context = ?, response_type = ?, type = ?, title = ? WHERE id = ?
                """,
                (
                    json.dumps(prepare_blocks_for_publish(question["blocks"])),
                    (
                        json.dumps(prepare_blocks_for_publish(question["answer"]))
                        if question["answer"]
                        else None
                    ),
                    str(question["input_type"]),
                    (
                        json.dumps(question["coding_languages"])
                        if question["coding_languages"]
                        else None
                    ),
                    json.dumps(question["context"]) if question["context"] else None,
                    str(question["response_type"]),
                    str(question["type"]),
                    question["title"],
                    question["id"],
                ),
            )

            if question.get("scorecard_id") is not None:
                # First check if there's an existing scorecard mapping
                await cursor.execute(
                    f"SELECT scorecard_id FROM {question_scorecards_table_name} WHERE question_id = ?",
                    (question["id"],),
                )
                existing_mapping = await cursor.fetchone()

                if existing_mapping:
                    # Update existing mapping
                    await cursor.execute(
                        f"UPDATE {question_scorecards_table_name} SET scorecard_id = ? WHERE question_id = ?",
                        (question["scorecard_id"], question["id"]),
                    )
                else:
                    # Insert new mapping
                    await cursor.execute(
                        f"INSERT INTO {question_scorecards_table_name} (question_id, scorecard_id) VALUES (?, ?)",
                        (question["id"], question["scorecard_id"]),
                    )

                await cursor.execute(
                    f"SELECT id FROM {scorecards_table_name} WHERE id = ? AND status = ?",
                    (question["scorecard_id"], str(ScorecardStatus.DRAFT)),
                )

                result = await cursor.fetchone()

                if result:
                    scorecards_to_publish.append(question["scorecard_id"])

        if scorecards_to_publish:
            await cursor.execute(
                f"UPDATE {scorecards_table_name} SET status = ? WHERE id IN ({','.join(map(str, scorecards_to_publish))})",
                (str(ScorecardStatus.PUBLISHED),),
            )

        # Update task status to published
        await cursor.execute(
            f"UPDATE {tasks_table_name} SET title = ?, scheduled_publish_at = ? WHERE id = ?",
            (title, scheduled_publish_at, task_id),
        )

        await conn.commit()

        return await get_task(task_id)


async def duplicate_task(task_id: int, course_id: int, milestone_id: int) -> int:
    task = await get_basic_task_details(task_id)

    if not task:
        raise ValueError("Task does not exist")

    task_ordering_in_module = await execute_db_operation(
        f"SELECT ordering FROM {course_tasks_table_name} WHERE course_id = ? AND milestone_id = ? AND task_id = ?",
        (course_id, milestone_id, task_id),
        fetch_one=True,
    )

    if task_ordering_in_module is None:
        raise ValueError("Task is not in this module")

    new_task_ordering = task_ordering_in_module[0] + 1

    new_task_id, visible_ordering = await create_draft_task_for_course(
        task["title"],
        str(task["type"]),
        course_id,
        milestone_id,
        new_task_ordering,
    )

    task = await get_task(task["id"])

    if task["type"] == TaskType.LEARNING_MATERIAL:
        await update_learning_material_task(
            new_task_id,
            task["title"],
            task["blocks"],
            None,
            TaskStatus.DRAFT,
        )
    elif task["type"] == TaskType.QUIZ:
        for question in task["questions"]:
            if question["scorecard_id"] is not None:
                question["scorecard"] = await get_scorecard(
                    question.pop("scorecard_id")
                )

        await update_draft_quiz(
            new_task_id,
            task["title"],
            task["questions"],
            None,
            TaskStatus.DRAFT,
        )
    else:
        raise ValueError("Task type not supported")

    task = await get_task(new_task_id)

    return {
        "task": task,
        "ordering": visible_ordering,
    }


async def delete_task(task_id: int):
    await execute_db_operation(
        f"""
        UPDATE {tasks_table_name} SET deleted_at = ? WHERE id = ? AND deleted_at IS NULL
        """,
        (datetime.now(), task_id),
    )


async def delete_tasks(task_ids: List[int]):
    task_ids_as_str = serialise_list_to_str(map(str, task_ids))

    await execute_db_operation(
        f"""
        UPDATE {tasks_table_name} SET deleted_at = ? WHERE id IN ({task_ids_as_str}) AND deleted_at IS NULL
        """,
        (datetime.now(),),
    )


async def get_solved_tasks_for_user(
    user_id: int,
    cohort_id: int,
    view_type: LeaderboardViewType = LeaderboardViewType.ALL_TIME,
):
    if view_type == LeaderboardViewType.ALL_TIME:
        results = await execute_db_operation(
            f"""
        SELECT DISTINCT ch.task_id 
        FROM {chat_history_table_name} ch
        JOIN {tasks_table_name} t ON t.id = ch.task_id
        JOIN {course_tasks_table_name} ct ON t.id = ct.task_id
        JOIN {course_cohorts_table_name} cc ON ct.course_id = cc.course_id
        WHERE ch.user_id = ? AND ch.is_solved = 1 AND cc.cohort_id = ? AND t.deleted_at IS NULL
        """,
            (user_id, cohort_id),
            fetch_all=True,
        )
    else:
        ist = timezone(timedelta(hours=5, minutes=30))
        now = datetime.now(ist)
        if view_type == LeaderboardViewType.WEEKLY:
            start_date = now - timedelta(days=now.weekday())
            start_date = start_date.replace(hour=0, minute=0, second=0, microsecond=0)
        else:  # MONTHLY
            start_date = now.replace(day=1, hour=0, minute=0, second=0, microsecond=0)

        results = await execute_db_operation(
            f"""
        WITH FirstSolved AS (
            SELECT ch.task_id, MIN(datetime(ch.timestamp, '+5 hours', '+30 minutes')) as first_solved_time
            FROM {chat_history_table_name} ch
            JOIN {tasks_table_name} t ON t.id = ch.task_id
            JOIN {course_tasks_table_name} ct ON t.id = ct.task_id
            JOIN {course_cohorts_table_name} cc ON ct.course_id = cc.course_id
            WHERE ch.user_id = ? AND ch.is_solved = 1 AND cc.cohort_id = ? AND t.deleted_at IS NULL
            GROUP BY ch.task_id
        )
        SELECT DISTINCT task_id 
        FROM FirstSolved
        WHERE first_solved_time >= ?
        """,
            (user_id, cohort_id, start_date),
            fetch_all=True,
        )

    return [task[0] for task in results]


async def mark_task_completed(task_id: int, user_id: int):
    # Update task completion table using INSERT OR IGNORE to handle duplicates gracefully
    await execute_db_operation(
        f"""
        INSERT OR IGNORE INTO {task_completions_table_name} (user_id, task_id)
        VALUES (?, ?)
        """,
        (user_id, task_id),
    )


async def delete_completion_history_for_task(
    task_id: int, question_id: int, user_id: int
):
    if task_id is not None:
        await execute_db_operation(
            f"DELETE FROM {chat_history_table_name} WHERE task_id = ? AND user_id = ?",
            (task_id, user_id),
        )

    await execute_db_operation(
        f"DELETE FROM {chat_history_table_name} WHERE question_id = ? AND user_id = ?",
        (question_id, user_id),
    )


async def schedule_module_tasks(
    course_id: int, module_id: int, scheduled_publish_at: datetime
):
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"SELECT t.id FROM {tasks_table_name} t INNER JOIN {course_tasks_table_name} ct ON t.id = ct.task_id WHERE ct.course_id = ? AND ct.milestone_id = ? AND t.status = '{TaskStatus.PUBLISHED}'",
            (course_id, module_id),
        )

        course_module_tasks = await cursor.fetchall()

        if not course_module_tasks:
            return

        for task in course_module_tasks:
            await cursor.execute(
                f"UPDATE {tasks_table_name} SET scheduled_publish_at = ? WHERE id = ?",
                (scheduled_publish_at, task[0]),
            )

        await conn.commit()


async def drop_task_generation_jobs_table():
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(f"DROP TABLE IF EXISTS {task_generation_jobs_table_name}")


async def store_task_generation_request(
    task_id: int, course_id: int, job_details: Dict
) -> str:
    job_uuid = str(uuid.uuid4())

    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"INSERT INTO {task_generation_jobs_table_name} (uuid, task_id, course_id, status, job_details) VALUES (?, ?, ?, ?, ?)",
            (
                job_uuid,
                task_id,
                course_id,
                str(GenerateTaskJobStatus.STARTED),
                json.dumps(job_details),
            ),
        )

        await conn.commit()

    return job_uuid


async def update_task_generation_job_status(
    job_uuid: str, status: GenerateTaskJobStatus
):
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"UPDATE {task_generation_jobs_table_name} SET status = ? WHERE uuid = ?",
            (str(status), job_uuid),
        )

        await conn.commit()


async def get_course_task_generation_jobs_status(course_id: int) -> List[str]:
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"SELECT status FROM {task_generation_jobs_table_name} WHERE course_id = ?",
            (course_id,),
        )

        statuses = [row[0] for row in await cursor.fetchall()]

        return {
            str(GenerateTaskJobStatus.COMPLETED): statuses.count(
                str(GenerateTaskJobStatus.COMPLETED)
            ),
            str(GenerateTaskJobStatus.STARTED): statuses.count(
                str(GenerateTaskJobStatus.STARTED)
            ),
        }


async def get_all_pending_task_generation_jobs() -> List[Dict]:
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"SELECT uuid, job_details FROM {task_generation_jobs_table_name} WHERE status = ?",
            (str(GenerateTaskJobStatus.STARTED),),
        )

        return [
            {
                "uuid": row[0],
                "job_details": json.loads(row[1]),
            }
            for row in await cursor.fetchall()
        ]


async def drop_task_completions_table():
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(f"DROP TABLE IF EXISTS {task_completions_table_name}")

        await conn.commit()


async def get_all_scorecards_for_org(org_id: int) -> List[Dict]:
    scorecards = await execute_db_operation(
        f"SELECT id, title, criteria, status FROM {scorecards_table_name} WHERE org_id = ?",
        (org_id,),
        fetch_all=True,
    )

    return [
        {
            "id": scorecard[0],
            "title": scorecard[1],
            "criteria": [
                {
                    **criterion,
                    "pass_score": criterion.get("pass_score", criterion["max_score"]),
                }
                for criterion in json.loads(scorecard[2])
            ],
            "status": scorecard[3],
        }
        for scorecard in scorecards
    ]


async def create_scorecard(
    scorecard: Dict, status: ScorecardStatus = ScorecardStatus.DRAFT
):
    scorecard_id = await execute_db_operation(
        f"INSERT INTO {scorecards_table_name} (org_id, title, criteria, status) VALUES (?, ?, ?, ?)",
        (
            scorecard["org_id"],
            scorecard["title"],
            json.dumps(scorecard["criteria"]),
            str(status),
        ),
        get_last_row_id=True,
    )

    return await get_scorecard(scorecard_id)


async def update_scorecard(scorecard_id: int, scorecard: BaseScorecard):
    scorecard = scorecard.model_dump()

    await execute_db_operation(
        f"UPDATE {scorecards_table_name} SET title = ?, criteria = ? WHERE id = ?",
        (scorecard["title"], json.dumps(scorecard["criteria"]), scorecard_id),
    )

    return await get_scorecard(scorecard_id)


async def undo_task_delete(task_id: int):
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()

        await cursor.execute(
            f"UPDATE {tasks_table_name} SET deleted_at = NULL WHERE id = ?",
            (task_id,),
        )

        await conn.commit()


async def publish_scheduled_tasks():
    """Publish all tasks whose scheduled time has arrived"""
    current_time = datetime.now()
    # Ensure we're using UTC time for consistency
    current_time = datetime.now(timezone.utc)

    # Get all tasks that should be published now
    tasks = await execute_db_operation(
        f"""
        UPDATE {tasks_table_name}
        SET scheduled_publish_at = NULL
        WHERE status = '{TaskStatus.PUBLISHED}'
        AND scheduled_publish_at IS NOT NULL AND deleted_at IS NULL
        AND scheduled_publish_at <= ?
        RETURNING id
        """,
        (current_time,),
        fetch_all=True,
    )

    return [task[0] for task in tasks] if tasks else []


async def add_generated_learning_material(task_id: int, task_details: Dict):
    await update_learning_material_task(
        task_id,
        task_details["name"],
        convert_blocks_to_right_format(task_details["details"]["blocks"]),
        None,
        TaskStatus.PUBLISHED,  # TEMP: turn to draft later
    )


async def add_generated_quiz(task_id: int, task_details: Dict):
    current_scorecard_index = 0

    for question in task_details["details"]["questions"]:
        question["type"] = question.pop("question_type")

        question["blocks"] = convert_blocks_to_right_format(question["blocks"])

        question["answer"] = (
            convert_blocks_to_right_format(question["correct_answer"])
            if question.get("correct_answer")
            else None
        )
        question["input_type"] = (
            question.pop("answer_type") if question.get("answer_type") else "text"
        )
        question["response_type"] = (
            TaskAIResponseType.CHAT
        )  # not getting exams to be generated in course generation
        question["generation_model"] = None
        question["context"] = (
            {
                "blocks": prepare_blocks_for_publish(
                    convert_blocks_to_right_format(question["context"])
                ),
                "linkedMaterialIds": None,
            }
            if question.get("context")
            else None
        )
        question["max_attempts"] = (
            1 if question["response_type"] == TaskAIResponseType.EXAM else None
        )
        question["is_feedback_shown"] = (
            question["response_type"] != TaskAIResponseType.EXAM
        )
        if question.get("scorecard"):
            question["scorecard"]["id"] = current_scorecard_index
            current_scorecard_index += 1
        else:
            question["scorecard"] = None
        question["scorecard_id"] = None
        question["coding_languages"] = question.get("coding_languages", None)

    await update_draft_quiz(
        task_id,
        task_details["name"],
        task_details["details"]["questions"],
        None,
        TaskStatus.PUBLISHED,  # TEMP: turn to draft later
    )

================
File: src/api/routes/auth.py
================
from fastapi import APIRouter, Depends, HTTPException
from typing import List, Dict
from api.db.user import insert_or_return_user
from api.utils.db import get_new_db_connection
from api.models import UserLoginData
from google.oauth2 import id_token
from google.auth.transport import requests
from api.settings import settings
import os

router = APIRouter()


@router.post("/login")
async def login_or_signup_user(user_data: UserLoginData) -> Dict:
    # Verify the Google ID token
    try:
        # Get Google Client ID from environment variable
        if not settings.google_client_id:
            raise HTTPException(
                status_code=500, detail="Google Client ID not configured"
            )

        # Verify the token with Google
        id_info = id_token.verify_oauth2_token(
            user_data.id_token, requests.Request(), settings.google_client_id
        )

        # Check that the email in the token matches the provided email
        if id_info["email"] != user_data.email:
            raise HTTPException(
                status_code=401, detail="Email in token doesn't match provided email"
            )

    except ValueError as e:
        # Invalid token
        raise HTTPException(
            status_code=401, detail=f"Invalid authentication token: {str(e)}"
        )

    # If token is valid, proceed with user creation/retrieval
    async with get_new_db_connection() as conn:
        cursor = await conn.cursor()
        user = await insert_or_return_user(
            cursor,
            user_data.email,
            user_data.given_name,
            user_data.family_name,
        )
        await conn.commit()

    return user

================
File: src/api/routes/code.py
================
from fastapi import APIRouter, HTTPException
from typing import Optional

from api.db.code_draft import (
    upsert_user_code_draft as upsert_code_draft_in_db,
    get_user_code_draft as get_code_draft_from_db,
    delete_user_code_draft as delete_code_draft_in_db,
)
from api.models import SaveCodeDraftRequest, CodeDraft

router = APIRouter()


@router.post("/")
async def save_code_draft(request: SaveCodeDraftRequest):
    await upsert_code_draft_in_db(
        user_id=request.user_id,
        question_id=request.question_id,
        code=[lang_code.model_dump() for lang_code in request.code],
    )
    return {"success": True}


@router.get(
    "/user/{user_id}/question/{question_id}", response_model=Optional[CodeDraft]
)
async def get_code_draft(user_id: int, question_id: int):
    return await get_code_draft_from_db(user_id, question_id)


@router.delete("/user/{user_id}/question/{question_id}")
async def delete_code_draft(user_id: int, question_id: int):
    await delete_code_draft_in_db(user_id, question_id)
    return {"success": True}

================
File: src/api/routes/file.py
================
import os
import traceback
import uuid
from fastapi import APIRouter, HTTPException, File, UploadFile, Form
from fastapi.responses import FileResponse
from pydantic import BaseModel
import boto3
from botocore.exceptions import ClientError
from api.settings import settings
from api.utils.logging import logger
from api.utils.s3 import (
    generate_s3_uuid,
    get_media_upload_s3_key_from_uuid,
)
from api.models import (
    PresignedUrlRequest,
    PresignedUrlResponse,
    S3FetchPresignedUrlResponse,
)

router = APIRouter()


@router.put("/presigned-url/create", response_model=PresignedUrlResponse)
async def get_upload_presigned_url(
    request: PresignedUrlRequest,
) -> PresignedUrlResponse:
    if not settings.s3_folder_name:
        raise HTTPException(status_code=500, detail="S3 folder name is not set")

    try:
        s3_client = boto3.client(
            "s3",
            region_name="ap-south-1",
            config=boto3.session.Config(signature_version="s3v4"),
        )

        uuid = generate_s3_uuid()
        key = get_media_upload_s3_key_from_uuid(
            uuid, request.content_type.split("/")[1]
        )

        presigned_url = s3_client.generate_presigned_url(
            "put_object",
            Params={
                "Bucket": settings.s3_bucket_name,
                "Key": key,
                "ContentType": request.content_type,
            },
            ExpiresIn=600,  # URL expires in 1 hour
        )

        return {
            "presigned_url": presigned_url,
            "file_key": key,
            "file_uuid": uuid,
        }

    except ClientError as e:
        logger.error(f"Error generating presigned URL: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to generate presigned URL")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="An unexpected error occurred")


@router.get("/presigned-url/get")
async def get_download_presigned_url(
    uuid: str,
    file_extension: str,
) -> S3FetchPresignedUrlResponse:
    if not settings.s3_folder_name:
        raise HTTPException(status_code=500, detail="S3 folder name is not set")

    try:
        s3_client = boto3.client(
            "s3",
            region_name="ap-south-1",
            config=boto3.session.Config(signature_version="s3v4"),
        )

        key = get_media_upload_s3_key_from_uuid(uuid, file_extension)

        presigned_url = s3_client.generate_presigned_url(
            "get_object",
            Params={
                "Bucket": settings.s3_bucket_name,
                "Key": key,
            },
            ExpiresIn=600,  # URL expires in 1 hour
        )

        return {"url": presigned_url}

    except ClientError as e:
        logger.error(f"Error generating download presigned URL: {str(e)}")
        raise HTTPException(
            status_code=500, detail="Failed to generate download presigned URL"
        )
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="An unexpected error occurred")


@router.post("/upload-local")
async def upload_file_locally(
    file: UploadFile = File(...), content_type: str = Form(...)
):
    try:
        # Create the folder if it doesn't exist
        os.makedirs(settings.local_upload_folder, exist_ok=True)

        # Generate a unique filename
        file_uuid = str(uuid.uuid4())
        file_extension = content_type.split("/")[1]
        filename = f"{file_uuid}.{file_extension}"
        file_path = os.path.join(settings.local_upload_folder, filename)

        # Save the file
        contents = await file.read()
        with open(file_path, "wb") as f:
            f.write(contents)

        # Generate the URL to access the file statically
        static_url = f"/uploads/{filename}"

        return {
            "file_key": filename,
            "file_path": file_path,
            "file_uuid": file_uuid,
            "static_url": static_url,
        }

    except Exception as e:
        logger.error(f"Error uploading file locally: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="Failed to upload file locally")


@router.get("/download-local/")
async def download_file_locally(
    uuid: str,
    file_extension: str,
):
    try:
        file_path = os.path.join(
            settings.local_upload_folder, f"{uuid}.{file_extension}"
        )

        # Check if file exists
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="File not found")

        # Return the file as a response
        return FileResponse(
            path=file_path,
            filename=f"{uuid}.{file_extension}",
            media_type="application/octet-stream",
        )

    except HTTPException:
        # Re-raise HTTP exceptions
        raise
    except Exception as e:
        logger.error(f"Error downloading file locally: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="Failed to download file locally")

================
File: src/api/routes/org.py
================
# --- START OF FILE sensai-api/sensai_backend/routes/org_routes.py ---
from fastapi import APIRouter, HTTPException, Body
import traceback
from typing import List, Dict, Annotated
from api.db.org import (
    create_organization_with_user,
    get_org_by_id as get_org_by_id_from_db,
    update_org as update_org_in_db,
    update_org_openai_api_key as update_org_openai_api_key_in_db,
    add_users_to_org_by_email as add_users_to_org_by_email_in_db,
    remove_members_from_org as remove_members_from_org_from_db,
    get_org_members as get_org_members_from_db,
    get_org_by_slug as get_org_by_slug_from_db,
    get_all_orgs as get_all_orgs_from_db,
)
from api.utils.db import get_new_db_connection
from api.models import (
    CreateOrganizationRequest,
    CreateOrganizationResponse,
    RemoveMembersFromOrgRequest,
    AddUsersToOrgRequest,
    UpdateOrgRequest,
    UpdateOrgOpenaiApiKeyRequest,
)

router = APIRouter()


@router.post("/")
async def create_organization(
    request: CreateOrganizationRequest,
) -> CreateOrganizationResponse:
    try:
        org_id = await create_organization_with_user(
            request.name,
            request.slug,
            request.user_id,
        )

        return {"id": org_id}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/{org_id}")
async def get_org_by_id(org_id: int) -> Dict:
    org_details = await get_org_by_id_from_db(org_id)

    if not org_details:
        raise HTTPException(status_code=404, detail="Organization not found")

    return org_details


@router.get("/slug/{slug}")
async def get_org_by_slug(slug: str) -> Dict:
    org_details = await get_org_by_slug_from_db(slug)

    if not org_details:
        raise HTTPException(status_code=404, detail="Organization not found")

    return org_details


@router.put("/{org_id}")
async def update_org(org_id: int, request: UpdateOrgRequest):
    await update_org_in_db(org_id, request.name)
    return {"success": True}


@router.put("/{org_id}/openai_api_key")
async def update_org_openai_api_key(org_id: int, request: UpdateOrgOpenaiApiKeyRequest):
    await update_org_openai_api_key_in_db(
        org_id, request.encrypted_openai_api_key, request.is_free_trial
    )
    return {"success": True}


@router.post("/{org_id}/members")
async def add_users_to_org_by_email(org_id: int, request: AddUsersToOrgRequest):
    try:
        await add_users_to_org_by_email_in_db(org_id, request.emails)
        return {"success": True}
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=400, detail=str(e))


@router.delete("/{org_id}/members")
async def remove_members_from_org(org_id: int, request: RemoveMembersFromOrgRequest):
    await remove_members_from_org_from_db(org_id, request.user_ids)
    return {"success": True}


@router.get("/{org_id}/members")
async def get_org_members(org_id: int) -> List[Dict]:
    return await get_org_members_from_db(org_id)


@router.get("/")
async def get_all_orgs() -> List[Dict]:
    return await get_all_orgs_from_db()

================
File: src/api/utils/s3.py
================
import os
from os.path import join
import uuid
import boto3
import boto3.session
from api.settings import settings


def upload_file_to_s3(
    file_path: str,
    key: str,
    content_type: str = None,
):
    bucket_name = settings.s3_bucket_name

    session = boto3.Session()
    s3_client = session.client("s3")

    extra_args = {}
    if content_type:
        extra_args["ContentType"] = content_type

    response = s3_client.upload_file(file_path, bucket_name, key, ExtraArgs=extra_args)

    if response is not None:
        raise Exception(f"Failed to upload to S3. Response: {response}")

    return key


def upload_audio_data_to_s3(
    audio_data: bytes,
    key: str,
):
    """
    Upload audio data to S3 bucket

    Args:
        audio_data: Audio data in bytes
        key: S3 key ending in .wav

    Returns:
        str: The S3 key where the audio was uploaded
    """
    if not key.endswith(".wav"):
        raise ValueError("Key must end with .wav extension")

    bucket_name = settings.s3_bucket_name

    session = boto3.Session()
    s3_client = session.client("s3")

    response = s3_client.put_object(
        Bucket=bucket_name, Key=key, Body=audio_data, ContentType="audio/wav"
    )

    status_code = response["ResponseMetadata"]["HTTPStatusCode"]
    if status_code != 200:
        raise Exception(f"Failed to upload to S3. Status code: {status_code}")

    return key


def download_file_from_s3_as_bytes(key: str):
    """
    Download a file from S3 bucket
    """
    bucket_name = settings.s3_bucket_name
    session = boto3.Session()
    s3_client = session.client("s3")

    response = s3_client.get_object(Bucket=bucket_name, Key=key)
    return response["Body"].read()


def generate_s3_uuid():
    return str(uuid.uuid4())


def get_media_upload_s3_dir():
    return join(settings.s3_folder_name, "media")


def get_media_upload_s3_key_from_uuid(uuid: str, extension: str):
    return join(get_media_upload_s3_dir(), f"{uuid}.{extension}")

================
File: src/startup.py
================
from api.db import init_db
import os
import asyncio
from api.config import UPLOAD_FOLDER_NAME

root_dir = os.path.dirname(os.path.abspath(__file__))

if __name__ == "__main__":
    asyncio.run(init_db())

    # create uploads folder
    if not os.path.exists("/appdata"):
        upload_folder = os.path.join(root_dir, UPLOAD_FOLDER_NAME)
        if not os.path.exists(upload_folder):
            os.makedirs(upload_folder)

================
File: src/api/utils/concurrency.py
================
from typing import List, Coroutine
import asyncio
from tqdm.asyncio import tqdm_asyncio


async def async_batch_gather(
    coroutines: List[Coroutine],
    batch_size: int = 25,
    description: str = "Processing batch",
):
    total_num = len(coroutines)
    # outputs = [None] * total_num
    results = []

    # Process in batches to limit memory usage
    for i in range(0, total_num, batch_size):
        batch = coroutines[i : i + batch_size]
        batch_results = await tqdm_asyncio.gather(
            *batch,
            desc=f"{description} {i}-{i+len(batch)}/{total_num}",
        )
        results.extend(batch_results)

        # for completed_task in asyncio.as_completed(batch):
        #     task_row_index, output = await completed_task

        #     outputs[task_row_index] = output
        #     pbar.update(1)

        # Give a little time for memory to be freed up
        await asyncio.sleep(1)

    return results


async def async_index_wrapper(func, index, *args, **kwargs):
    output = await func(*args, **kwargs)
    return index, output

================
File: src/api/websockets.py
================
from typing import Dict, Set
from fastapi import WebSocket, WebSocketDisconnect
from fastapi.routing import APIRouter

router = APIRouter()


# WebSocket connection manager to handle multiple client connections
class ConnectionManager:
    def __init__(self):
        # Dictionary to store WebSocket connections by course_id
        self.active_connections: Dict[int, Set[WebSocket]] = {}

    async def connect(self, websocket: WebSocket, course_id: int):
        await websocket.accept()
        if course_id not in self.active_connections:
            self.active_connections[course_id] = set()
        self.active_connections[course_id].add(websocket)

    def disconnect(self, websocket: WebSocket, course_id: int):
        if course_id in self.active_connections:
            self.active_connections[course_id].discard(websocket)
            if not self.active_connections[course_id]:
                del self.active_connections[course_id]

    async def send_item_update(self, course_id: int, item_data: Dict):
        if course_id in self.active_connections:
            disconnected_websockets = set()
            for websocket in self.active_connections[course_id]:
                try:
                    await websocket.send_json(item_data)
                except Exception as exception:
                    print(exception)

                    # Mark for removal if sending fails
                    disconnected_websockets.add(websocket)

            # Remove disconnected websockets
            for websocket in disconnected_websockets:
                self.disconnect(websocket, course_id)


# Create a connection manager instance
manager = ConnectionManager()


# WebSocket endpoint for course generation updates
@router.websocket("/course/{course_id}/generation")
async def websocket_course_generation(websocket: WebSocket, course_id: int):
    try:
        await manager.connect(websocket, course_id)

        # Keep the connection alive until client disconnects
        while True:
            # Wait for any message from the client to detect disconnection
            await websocket.receive_text()
    except WebSocketDisconnect:
        manager.disconnect(websocket, course_id)


# Function to get the connection manager instance
def get_manager() -> ConnectionManager:
    return manager

================
File: .gitignore
================
__pycache__
*.pyc
.DS_Store
wandb
logs
venv
data_map.txt
faiss.index
sqlite.db
experiments
question_generator
archive
.env
demo/english_activity.csv
demo/db/
src/db
scripts
.env.*
!.env.*example
*.pkl
secrets.toml
secrets.*.toml
coverage_html
coverage.xml
.coverage
uploads
db.sqlite
evals

================
File: src/api/routes/cohort.py
================
from collections import defaultdict
from datetime import datetime
from fastapi import APIRouter, HTTPException, Query
from typing import List, Dict

import numpy as np
from api.db.cohort import (
    get_all_cohorts_for_org as get_all_cohorts_for_org_from_db,
    create_cohort as create_cohort_in_db,
    get_cohort_by_id as get_cohort_by_id_from_db,
    add_members_to_cohort as add_members_to_cohort_in_db,
    remove_members_from_cohort as remove_members_from_cohort_in_db,
    delete_cohort as delete_cohort_from_db,
    update_cohort_name as update_cohort_name_in_db,
    add_courses_to_cohort as add_courses_to_cohort_in_db,
    remove_courses_from_cohort as remove_courses_from_cohort_in_db,
    get_cohort_analytics_metrics_for_tasks as get_cohort_analytics_metrics_for_tasks_from_db,
    get_cohort_attempt_data_for_tasks as get_cohort_attempt_data_for_tasks_from_db,
)
from api.db.course import get_courses_for_cohort as get_courses_for_cohort_from_db
from api.db.analytics import (
    get_cohort_completion as get_cohort_completion_from_db,
    get_cohort_course_attempt_data as get_cohort_course_attempt_data_from_db,
    get_cohort_streaks as get_cohort_streaks_from_db,
)
from api.db.course import get_course as get_course_from_db
from api.models import (
    CreateCohortRequest,
    CreateCohortGroupRequest,
    AddMembersToCohortRequest,
    RemoveMembersFromCohortRequest,
    UpdateCohortGroupRequest,
    AddMembersToCohortGroupRequest,
    RemoveMembersFromCohortGroupRequest,
    UpdateCohortRequest,
    UpdateCohortGroupRequest,
    AddCoursesToCohortRequest,
    CreateCohortResponse,
    RemoveCoursesFromCohortRequest,
    Streaks,
    LeaderboardViewType,
    CohortCourse,
    CourseWithMilestonesAndTasks,
    UserCourseRole,
)
from api.utils.db import get_new_db_connection

router = APIRouter()


@router.get("/")
async def get_all_cohorts_for_org(org_id: int) -> List[Dict]:
    return await get_all_cohorts_for_org_from_db(org_id)


@router.post("/", response_model=CreateCohortResponse)
async def create_cohort(request: CreateCohortRequest) -> CreateCohortResponse:
    return {"id": await create_cohort_in_db(request.name, request.org_id)}


@router.get("/{cohort_id}")
async def get_cohort_by_id(cohort_id: int) -> Dict:
    cohort_data = await get_cohort_by_id_from_db(cohort_id)
    if not cohort_data:
        raise HTTPException(status_code=404, detail="Cohort not found")

    return cohort_data


@router.post("/{cohort_id}/members")
async def add_members_to_cohort(cohort_id: int, request: AddMembersToCohortRequest):
    try:
        await add_members_to_cohort_in_db(
            cohort_id, request.org_slug, request.org_id, request.emails, request.roles
        )
        return {"success": True}
    except Exception as e:
        if "User already exists in cohort" in str(e):
            raise HTTPException(status_code=400, detail=str(e))
        elif "Cannot add an admin to the cohort" in str(e):
            raise HTTPException(status_code=401, detail=str(e))
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/{cohort_id}/members")
async def remove_members_from_cohort(
    cohort_id: int, request: RemoveMembersFromCohortRequest
):
    try:
        await remove_members_from_cohort_in_db(cohort_id, request.member_ids)
        return {"success": True}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.delete("/{cohort_id}")
async def delete_cohort(cohort_id: int):
    await delete_cohort_from_db(cohort_id)
    return {"success": True}


@router.put("/{cohort_id}")
async def update_cohort_name(cohort_id: int, request: UpdateCohortRequest):
    await update_cohort_name_in_db(cohort_id, request.name)
    return {"success": True}


@router.post("/{cohort_id}/courses")
async def add_courses_to_cohort(cohort_id: int, request: AddCoursesToCohortRequest):
    await add_courses_to_cohort_in_db(
        cohort_id,
        request.course_ids,
        is_drip_enabled=request.drip_config.is_drip_enabled,
        frequency_value=request.drip_config.frequency_value,
        frequency_unit=request.drip_config.frequency_unit,
        publish_at=request.drip_config.publish_at,
    )
    return {"success": True}


@router.delete("/{cohort_id}/courses")
async def remove_courses_from_cohort(
    cohort_id: int, request: RemoveCoursesFromCohortRequest
):
    await remove_courses_from_cohort_in_db(cohort_id, request.course_ids)
    return {"success": True}


@router.get(
    "/{cohort_id}/courses",
    response_model=List[CourseWithMilestonesAndTasks | CohortCourse],
)
async def get_courses_for_cohort(
    cohort_id: int, include_tree: bool = False, joined_at: datetime | None = None
) -> List[CourseWithMilestonesAndTasks | CohortCourse]:
    return await get_courses_for_cohort_from_db(cohort_id, include_tree, joined_at)


@router.get(
    "/{cohort_id}/completion",
    response_model=Dict,
)
async def get_cohort_completion(cohort_id: int, user_id: int) -> Dict:
    results = await get_cohort_completion_from_db(cohort_id, [user_id])
    return results[user_id]


@router.get("/{cohort_id}/leaderboard")
async def get_leaderboard_data(cohort_id: int):
    leaderboard_data = await get_cohort_streaks_from_db(cohort_id=cohort_id)

    user_ids = [streak["user"]["id"] for streak in leaderboard_data]

    if not user_ids:
        return {}

    task_completions = await get_cohort_completion_from_db(cohort_id, user_ids)

    num_tasks = len(task_completions[user_ids[0]])

    for user_data in leaderboard_data:
        user_id = user_data["user"]["id"]
        num_tasks_completed = 0

        for task_completion_data in task_completions[user_id].values():
            if task_completion_data["is_complete"]:
                num_tasks_completed += 1

        user_data["tasks_completed"] = num_tasks_completed

    leaderboard_data = sorted(
        leaderboard_data,
        key=lambda x: (x["streak_count"], x["tasks_completed"]),
        reverse=True,
    )

    return {
        "stats": leaderboard_data,
        "metadata": {
            "num_tasks": num_tasks,
        },
    }


@router.get("/{cohort_id}/courses/{course_id}/metrics")
async def get_cohort_metrics_for_course(cohort_id: int, course_id: int):
    course_data = await get_course_from_db(course_id, only_published=True)
    cohort_data = await get_cohort_by_id_from_db(cohort_id)

    if not course_data:
        raise HTTPException(status_code=404, detail="Course not found")

    if not cohort_data:
        raise HTTPException(status_code=404, detail="Cohort not found")

    task_id_to_metadata = {}
    task_type_counts = defaultdict(int)

    for milestone in course_data["milestones"]:
        for task in milestone["tasks"]:
            task_id_to_metadata[task["id"]] = {
                "milestone_id": milestone["id"],
                "milestone_name": milestone["name"],
                "type": task["type"],
            }
            task_type_counts[task["type"]] += 1

    learner_ids = [
        member["id"]
        for member in cohort_data["members"]
        if member["role"] == UserCourseRole.LEARNER
    ]

    if not learner_ids:
        return {}

    task_completions = await get_cohort_completion_from_db(
        cohort_id, learner_ids, course_id
    )

    course_attempt_data = await get_cohort_course_attempt_data_from_db(
        learner_ids, course_id
    )

    num_tasks = len(task_completions[learner_ids[0]])

    if not num_tasks:
        return {}

    task_type_completions = defaultdict(lambda: defaultdict(int))
    task_type_completion_rates = defaultdict(list)

    user_data = defaultdict(lambda: defaultdict(int))

    for learner_id in learner_ids:
        num_tasks_completed = 0

        for task_id, task_completion_data in task_completions[learner_id].items():
            if task_completion_data["is_complete"]:
                if task_id not in task_id_to_metadata:
                    continue

                num_tasks_completed += 1
                task_type_completions[task_id_to_metadata[task_id]["type"]][
                    learner_id
                ] += 1

        user_data[learner_id]["completed"] = num_tasks_completed
        user_data[learner_id]["completion_percentage"] = num_tasks_completed / num_tasks

        for task_type in task_type_counts.keys():
            task_type_completion_rates[task_type].append(
                task_type_completions[task_type][learner_id]
                / task_type_counts[task_type]
            )

    is_learner_active = {
        learner_id: course_attempt_data[learner_id][course_id]["has_attempted"]
        for learner_id in learner_ids
    }

    return {
        "average_completion": np.mean(
            [
                user_data[learner_id]["completion_percentage"]
                for learner_id in learner_ids
            ]
        ),
        "num_tasks": num_tasks,
        "num_active_learners": sum(is_learner_active.values()),
        "task_type_metrics": {
            task_type: {
                "completion_rate": (
                    np.mean(task_type_completion_rates[task_type])
                    if task_type in task_type_completion_rates
                    else 0
                ),
                "count": task_type_counts[task_type],
                "completions": (
                    task_type_completions[task_type]
                    if task_type in task_type_completions
                    else {learner_id: 0 for learner_id in learner_ids}
                ),
            }
            for task_type in task_type_counts.keys()
        },
    }


@router.get("/{cohort_id}/streaks", response_model=Streaks)
async def get_all_streaks_for_cohort(
    cohort_id: int = None, view: LeaderboardViewType = str(LeaderboardViewType.ALL_TIME)
) -> Streaks:
    return await get_cohort_streaks_from_db(view=view, cohort_id=cohort_id)


@router.get("/{cohort_id}/task_metrics")
async def get_cohort_analytics_metrics_for_tasks(
    cohort_id: int, task_ids: List[int] = Query(...)
):
    return await get_cohort_analytics_metrics_for_tasks_from_db(cohort_id, task_ids)


@router.get("/{cohort_id}/task_attempt_data")
async def get_cohort_attempt_data_for_tasks(
    cohort_id: int, task_ids: List[int] = Query(...)
):
    return await get_cohort_attempt_data_for_tasks_from_db(cohort_id, task_ids)

================
File: src/api/routes/scorecard.py
================
from fastapi import APIRouter
from typing import List
from api.db.task import (
    get_all_scorecards_for_org as get_all_scorecards_for_org_from_db,
    update_scorecard as update_scorecard_from_db,
    create_scorecard as create_scorecard_from_db,
)
from api.models import Scorecard, BaseScorecard, CreateScorecardRequest

router = APIRouter()


@router.get("/", response_model=List[Scorecard])
async def get_all_scorecards_for_org(org_id: int) -> List[Scorecard]:
    return await get_all_scorecards_for_org_from_db(org_id)


@router.put("/{scorecard_id}")
async def update_scorecard(scorecard_id: int, scorecard: BaseScorecard) -> Scorecard:
    return await update_scorecard_from_db(scorecard_id, scorecard)


@router.post("/", response_model=Scorecard)
async def create_scorecard(scorecard: CreateScorecardRequest) -> Scorecard:
    return await create_scorecard_from_db(scorecard.model_dump())

================
File: src/api/llm.py
================
from typing import Dict, List
import backoff
import openai
import instructor

from openai import OpenAI

from pydantic import BaseModel

from api.utils.logging import logger

# Test log message
logger.info("Logging system initialized")


def is_reasoning_model(model: str) -> bool:
    return model in [
        "o3-mini-2025-01-31",
        "o3-mini",
        "o1-preview-2024-09-12",
        "o1-preview",
        "o1-mini",
        "o1-mini-2024-09-12",
        "o1",
        "o1-2024-12-17",
    ]


def validate_openai_api_key(openai_api_key: str) -> bool:
    client = OpenAI(api_key=openai_api_key)
    try:
        models = client.models.list()
        model_ids = [model.id for model in models.data]

        if "gpt-4o-audio-preview-2024-12-17" in model_ids:
            return False  # paid account
        else:
            return True  # free trial account
    except Exception:
        return None


@backoff.on_exception(backoff.expo, Exception, max_tries=5, factor=2)
async def run_llm_with_instructor(
    api_key: str,
    model: str,
    messages: List,
    response_model: BaseModel,
    max_completion_tokens: int,
):
    client = instructor.from_openai(openai.AsyncOpenAI(api_key=api_key))

    model_kwargs = {}

    if not is_reasoning_model(model):
        model_kwargs["temperature"] = 0

    return await client.chat.completions.create(
        model=model,
        messages=messages,
        response_model=response_model,
        max_completion_tokens=max_completion_tokens,
        store=True,
        **model_kwargs,
    )


@backoff.on_exception(backoff.expo, Exception, max_tries=5, factor=2)
async def stream_llm_with_instructor(
    api_key: str,
    model: str,
    messages: List,
    response_model: BaseModel,
    max_completion_tokens: int,
    **kwargs,
):
    client = instructor.from_openai(openai.AsyncOpenAI(api_key=api_key))

    model_kwargs = {}

    if not is_reasoning_model(model):
        model_kwargs["temperature"] = 0

    model_kwargs.update(kwargs)

    return client.chat.completions.create_partial(
        model=model,
        messages=messages,
        response_model=response_model,
        stream=True,
        max_completion_tokens=max_completion_tokens,
        store=True,
        **model_kwargs,
    )


@backoff.on_exception(backoff.expo, Exception, max_tries=5, factor=2)
def stream_llm_with_openai(
    api_key: str,
    model: str,
    messages: List,
    max_completion_tokens: int,
):
    client = openai.OpenAI(api_key=api_key)

    model_kwargs = {}

    if not is_reasoning_model(model):
        model_kwargs["temperature"] = 0

    return client.chat.completions.create(
        model=model,
        messages=messages,
        stream=True,
        max_completion_tokens=max_completion_tokens,
        store=True,
        **model_kwargs,
    )

================
File: src/api/scheduler.py
================
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from api.db.task import publish_scheduled_tasks
from api.cron import send_usage_summary_stats, save_daily_traces
from api.settings import settings
from datetime import timezone, timedelta

# Create IST timezone
ist_timezone = timezone(timedelta(hours=5, minutes=30))

scheduler = AsyncIOScheduler(timezone=ist_timezone)


# Check for tasks to publish every minute
@scheduler.scheduled_job("interval", minutes=1)
async def check_scheduled_tasks():
    await publish_scheduled_tasks()


# Send usage summary stats every day at 9 AM IST
@scheduler.scheduled_job("cron", hour=9, minute=0, timezone=ist_timezone)
async def daily_usage_stats():
    if not settings.slack_usage_stats_webhook_url:
        return

    await send_usage_summary_stats()


@scheduler.scheduled_job("cron", hour=10, minute=0, timezone=ist_timezone)
async def daily_traces():
    save_daily_traces()

================
File: src/api/slack.py
================
from typing import Dict, List
import aiohttp
from api.settings import settings


async def send_slack_notification(message: Dict, webhook_url: str):
    async with aiohttp.ClientSession() as session:
        async with session.post(webhook_url, json=message) as response:
            if response.status >= 400:
                response_text = await response.text()
                print(
                    f"Failed to send Slack notification: {response.status} - {response_text}"
                )


async def send_slack_notification_for_new_user(user: Dict):
    """
    Send Slack notification when a new user is created.

    Args:
        user: Dictionary containing user information
    """
    # Check if Slack webhook URL is configured
    if not settings.slack_user_signup_webhook_url:
        return

    message = {"text": f"User created: {user['email']} UserId: {user['id']}"}

    # Send notification asynchronously
    await send_slack_notification(message, settings.slack_user_signup_webhook_url)


async def send_slack_notification_for_learner_added_to_cohort(
    user_invited: Dict,
    org_slug: str,
    org_id: int,
    cohort_name: str,
    cohort_id: int,
):
    # Check if Slack webhook URL is configured
    if not settings.slack_user_signup_webhook_url:
        return

    message = {
        "text": f"Learner added to cohort: {user_invited['email']} UserId: {user_invited['id']}\n"
        f"School: {org_slug} (SchoolId: {org_id})\n"
        f"Cohort: {cohort_name} (CohortId: {cohort_id})"
    }

    # Send notification asynchronously
    await send_slack_notification(message, settings.slack_user_signup_webhook_url)


async def send_slack_notification_for_member_added_to_org(
    user_added: Dict,
    org_slug: str,
    org_id: int,
):
    # Check if Slack webhook URL is configured
    if not settings.slack_user_signup_webhook_url:
        return

    message = {
        "text": f"User added as admin: {user_added['email']} UserId: {user_added['id']}\n"
        f"School: {org_slug} (SchoolId: {org_id})"
    }

    # Send notification asynchronously
    await send_slack_notification(message, settings.slack_user_signup_webhook_url)


async def send_slack_notification_for_new_org(
    org_slug: str,
    org_id: int,
    created_by: Dict,
):
    # Check if Slack webhook URL is configured
    if not settings.slack_user_signup_webhook_url:
        return

    message = {
        "text": f"New school created: {org_slug} (SchoolId: {org_id})\n"
        f"Created by: {created_by['email']} (UserId: {created_by['id']})"
    }

    # Send notification asynchronously
    await send_slack_notification(message, settings.slack_user_signup_webhook_url)


async def send_slack_notification_for_new_course(
    course_name: str,
    course_id: int,
    org_slug: str,
    org_id: int,
):
    # Check if Slack webhook URL is configured
    if not settings.slack_course_created_webhook_url:
        return

    message = {
        "text": f"New course created: {course_name} (CourseId: {course_id})\n"
        f"School: {org_slug} (SchoolId: {org_id})"
    }

    # Send notification asynchronously
    await send_slack_notification(message, settings.slack_course_created_webhook_url)


async def send_slack_notification_for_usage_stats(
    last_day_stats: Dict[str, List[Dict]],
    current_month_stats: Dict[str, List[Dict]],
    current_year_stats: Dict[str, List[Dict]],
):
    """
    Send Slack notification with usage statistics for different time periods.

    Args:
        last_day_stats: Usage stats for the last day
        current_month_stats: Usage stats for the current month
        current_year_stats: Usage stats for the current year
    """
    # Check if Slack webhook URL is configured
    if not settings.slack_usage_stats_webhook_url:
        return

    def format_period_stats(
        org_stats: List[Dict], model_stats: Dict[str, int], period: str
    ) -> str:
        # Use different emojis for different time periods
        emoji_map = {
            "Last 24 Hours": "⚡",
            "This Month": "📈",
            "This Year": "📊",
        }
        emoji = emoji_map.get(period, "📊")

        if not org_stats:
            total_messages = 0
        else:
            total_messages = sum(org["user_message_count"] for org in org_stats)

        formatted = f"{emoji} *{period}* (Total: {total_messages:,} messages)\n\n"

        # Organization stats column
        if not org_stats:
            org_section = "📊 *Organizations*: No usage data\n"
        else:
            top_orgs = org_stats[:5]  # Show top 5 organizations

            org_section = "```\n"
            org_section += f"{'Organization':<30} {'Messages':>8}\n"
            org_section += f"{'-' * 30} {'-' * 8}\n"

            for org in top_orgs:
                org_name = (
                    org["org_name"][:25] + ".."
                    if len(org["org_name"]) > 27
                    else org["org_name"]
                )
                org_section += f"{org_name:<30} {org['user_message_count']:>8,}\n"

            if len(org_stats) > 5:
                remaining_count = len(org_stats) - 5
                remaining_messages = sum(
                    org["user_message_count"] for org in org_stats[5:]
                )
                org_section += (
                    f"{f'+{remaining_count} more':<30} {remaining_messages:>8,}\n"
                )

            org_section += "```\n"

        # Model stats column
        if not model_stats:
            model_section = "🤖 *Models*: No model data\n"
        else:
            sorted_models = sorted(
                model_stats.items(), key=lambda x: x[1], reverse=True
            )
            top_models = sorted_models[:5]  # Show top 5 models

            model_section = "```\n"
            model_section += f"{'Model':<50} {'Count':>8}\n"
            model_section += f"{'-' * 50} {'-' * 8}\n"

            for model_name, count in top_models:
                model_display = (
                    model_name[:25] + ".." if len(model_name) > 27 else model_name
                )
                model_section += f"{model_display:<50} {count:>8,}\n"

            if len(sorted_models) > 5:
                remaining_count = len(sorted_models) - 5
                remaining_calls = sum(count for _, count in sorted_models[5:])
                model_section += (
                    f"{f'+{remaining_count} more':<50} {remaining_calls:>8,}\n"
                )

            model_section += "```\n"

        # Combine both sections side by side conceptually, but Slack doesn't support true columns
        # So we'll display them sequentially but clearly separated
        formatted += org_section + "\n" + model_section

        return formatted

    # Send separate messages for each time period
    periods = [
        ("Last 24 Hours", last_day_stats),
        ("This Month", current_month_stats),
        ("This Year", current_year_stats),
    ]

    message_text = ""
    for period_name, stats in periods:
        message_text += format_period_stats(stats["org"], stats["model"], period_name)

    message = {"text": message_text}

    # Send notification asynchronously
    await send_slack_notification(message, settings.slack_usage_stats_webhook_url)

================
File: src/api/routes/course.py
================
from fastapi import APIRouter, HTTPException
from typing import List, Dict
from api.db.course import (
    create_course as create_course_in_db,
    get_all_courses_for_org as get_all_courses_for_org_from_db,
    delete_course as delete_course_in_db,
    get_courses_for_cohort as get_courses_for_cohort_from_db,
    get_cohorts_for_course as get_cohorts_for_course_from_db,
    get_tasks_for_course as get_tasks_for_course_from_db,
    update_course_name as update_course_name_in_db,
    add_tasks_to_courses as add_tasks_to_courses_in_db,
    remove_tasks_from_courses as remove_tasks_from_courses_in_db,
    update_task_orders as update_task_orders_in_db,
    add_milestone_to_course as add_milestone_to_course_in_db,
    update_milestone_orders as update_milestone_orders_in_db,
    get_course as get_course_from_db,
    swap_milestone_ordering_for_course as swap_milestone_ordering_for_course_in_db,
    swap_task_ordering_for_course as swap_task_ordering_for_course_in_db,
)
from api.db.cohort import (
    add_course_to_cohorts as add_course_to_cohorts_in_db,
    remove_course_from_cohorts as remove_course_from_cohorts_from_db,
)
from api.models import (
    CreateCourseRequest,
    RemoveCourseFromCohortsRequest,
    AddCourseToCohortsRequest,
    UpdateCourseNameRequest,
    AddTasksToCoursesRequest,
    RemoveTasksFromCoursesRequest,
    UpdateTaskOrdersRequest,
    UpdateMilestoneOrdersRequest,
    CreateCourseResponse,
    Course,
    CourseWithMilestonesAndTasks,
    AddMilestoneToCourseRequest,
    AddMilestoneToCourseResponse,
    SwapMilestoneOrderingRequest,
    SwapTaskOrderingRequest,
    CourseCohort,
)

router = APIRouter()


@router.post("/", response_model=CreateCourseResponse)
async def create_course(request: CreateCourseRequest) -> CreateCourseResponse:
    return {"id": await create_course_in_db(request.name, request.org_id)}


@router.get("/")
async def get_all_courses_for_org(org_id: int) -> List[Course]:
    return await get_all_courses_for_org_from_db(org_id)


@router.get("/{course_id}", response_model=CourseWithMilestonesAndTasks)
async def get_course(
    course_id: int, only_published: bool = True
) -> CourseWithMilestonesAndTasks:
    return await get_course_from_db(course_id, only_published)


@router.post("/tasks")
async def add_tasks_to_courses(request: AddTasksToCoursesRequest):
    await add_tasks_to_courses_in_db(request.course_tasks)
    return {"success": True}


@router.delete("/tasks")
async def remove_tasks_from_courses(request: RemoveTasksFromCoursesRequest):
    await remove_tasks_from_courses_in_db(request.course_tasks)
    return {"success": True}


@router.put("/tasks/order")
async def update_task_orders(request: UpdateTaskOrdersRequest):
    await update_task_orders_in_db(request.task_orders)
    return {"success": True}


@router.post("/{course_id}/milestones")
async def add_milestone_to_course(
    course_id: int, request: AddMilestoneToCourseRequest
) -> AddMilestoneToCourseResponse:
    milestone_id, _ = await add_milestone_to_course_in_db(
        course_id,
        request.name,
        request.color,
    )
    return {"id": milestone_id}


@router.put("/milestones/order")
async def update_milestone_orders(request: UpdateMilestoneOrdersRequest):
    await update_milestone_orders_in_db(request.milestone_orders)
    return {"success": True}


@router.delete("/{course_id}")
async def delete_course(course_id: int):
    await delete_course_in_db(course_id)
    return {"success": True}


@router.post("/{course_id}/cohorts")
async def add_course_to_cohorts(course_id: int, request: AddCourseToCohortsRequest):
    await add_course_to_cohorts_in_db(
        course_id,
        request.cohort_ids,
        is_drip_enabled=request.drip_config.is_drip_enabled,
        frequency_value=request.drip_config.frequency_value,
        frequency_unit=request.drip_config.frequency_unit,
        publish_at=request.drip_config.publish_at,
    )
    return {"success": True}


@router.delete("/{course_id}/cohorts")
async def remove_course_from_cohorts(
    course_id: int, request: RemoveCourseFromCohortsRequest
):
    await remove_course_from_cohorts_from_db(course_id, request.cohort_ids)
    return {"success": True}


@router.get("/{course_id}/cohorts")
async def get_cohorts_for_course(course_id: int) -> List[CourseCohort]:
    return await get_cohorts_for_course_from_db(course_id)


@router.get("/{course_id}/tasks")
async def get_tasks_for_course(course_id: int) -> List[Dict]:
    return await get_tasks_for_course_from_db(course_id)


@router.put("/{course_id}")
async def update_course_name(course_id: int, request: UpdateCourseNameRequest):
    await update_course_name_in_db(course_id, request.name)
    return {"success": True}


@router.put("/{course_id}/milestones/swap")
async def swap_milestone_ordering(
    course_id: int, request: SwapMilestoneOrderingRequest
):
    await swap_milestone_ordering_for_course_in_db(
        course_id, request.milestone_1_id, request.milestone_2_id
    )
    return {"success": True}


@router.put("/{course_id}/tasks/swap")
async def swap_task_ordering(course_id: int, request: SwapTaskOrderingRequest):
    await swap_task_ordering_for_course_in_db(
        course_id, request.task_1_id, request.task_2_id
    )
    return {"success": True}

================
File: src/api/utils/phoenix.py
================
import os
from datetime import datetime, timedelta
from typing import Optional
from datetime import datetime, timedelta
import tempfile
import json
import math
import pandas as pd
from api.settings import settings
from api.utils.s3 import upload_file_to_s3, download_file_from_s3_as_bytes


def get_raw_traces(
    filter_period: Optional[str] = None, timeout: int = 120
) -> pd.DataFrame:
    from phoenix import Client

    os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = settings.phoenix_endpoint
    os.environ["PHOENIX_API_KEY"] = settings.phoenix_api_key
    project_name = f"sensai-{settings.env}"

    if not filter_period:
        return Client().get_spans_dataframe(project_name=project_name, timeout=timeout)

    if filter_period not in ["last_day", "current_month", "current_year"]:
        raise ValueError("Invalid filter period")

    if filter_period == "last_day":
        end_time = datetime.now()
        start_time = end_time - timedelta(days=1)
        return Client().get_spans_dataframe(
            project_name=project_name,
            start_time=start_time,
            end_time=end_time,
            timeout=timeout,
        )

    if filter_period == "current_month":
        end_time = datetime.now()
        start_time = end_time.replace(day=1)
        return Client().get_spans_dataframe(
            project_name=project_name,
            start_time=start_time,
            end_time=end_time,
            timeout=timeout,
        )

    end_time = datetime.now()
    start_time = end_time.replace(month=1, day=1)
    return Client().get_spans_dataframe(
        project_name=project_name,
        start_time=start_time,
        end_time=end_time,
        timeout=timeout,
    )


def prepare_feedback_traces_for_annotation(df: pd.DataFrame) -> pd.DataFrame:
    # Filter out feedback stage entries
    df_non_root = df[~df["attributes.metadata"].isna()].reset_index(drop=True)
    df_feedback = df_non_root[
        df_non_root["attributes.metadata"].apply(lambda x: x["stage"] == "feedback")
    ].reset_index(drop=True)

    # Function to get the last entry for each group and build chat history
    def get_last_entries_with_chat_history(df):
        # Separate learning_material and quiz types
        df_lm = df[
            df["attributes.metadata"].apply(
                lambda x: x.get("type") == "learning_material"
            )
        ]
        df_quiz = df[df["attributes.metadata"].apply(lambda x: x.get("type") == "quiz")]

        result_dfs = []

        # For learning_material: group by task_id and user_id
        if not df_lm.empty:
            df_lm_copy = df_lm.copy()
            df_lm_copy["task_id"] = df_lm_copy["attributes.metadata"].apply(
                lambda x: x.get("task_id")
            )

            # Group and process each group
            grouped_lm = df_lm_copy.groupby(["task_id", "attributes.user.id"])

            processed_rows = []
            for (task_id, user_id), group in grouped_lm:
                # Sort by start_time to ensure chronological order
                group_sorted = group.sort_values("start_time")

                # Build chat history from all entries in the group
                chat_history = []
                context = None
                for _, row in group_sorted.iterrows():
                    try:
                        input_messages = row["attributes.llm.input_messages"]
                        output_messages = row["attributes.llm.output_messages"]

                        # Find the second last user message (the actual user query)
                        user_messages = [
                            msg
                            for msg in input_messages
                            if msg.get("message.role") == "user"
                        ]
                        if (
                            "Reference Material"
                            not in user_messages[-1]["message.content"]
                        ):
                            continue

                        if context is None:
                            context = user_messages[-1]["message.content"]

                        if len(user_messages) >= 2:
                            user_message = user_messages[-2]["message.content"]

                            # Get AI response
                            if output_messages:
                                ai_message = json.loads(
                                    output_messages[0]["message.tool_calls"][0][
                                        "tool_call.function.arguments"
                                    ]
                                )

                                chat_history.append(
                                    {"role": "user", "content": user_message}
                                )
                                chat_history.append(
                                    {"role": "assistant", "content": ai_message}
                                )
                    except:
                        continue

                if not chat_history:
                    continue

                # Take the last entry and add chat history
                last_entry = group_sorted.iloc[-1].copy()
                last_entry["chat_history"] = chat_history
                last_entry["context"] = context
                processed_rows.append(last_entry)

            if processed_rows:
                df_lm_result = pd.DataFrame(processed_rows).drop(["task_id"], axis=1)
                result_dfs.append(df_lm_result)

        # For quiz: group by question_id and user_id
        if not df_quiz.empty:
            df_quiz_copy = df_quiz.copy()
            df_quiz_copy["question_id"] = df_quiz_copy["attributes.metadata"].apply(
                lambda x: x.get("question_id")
            )
            # Group and process each group
            grouped_quiz = df_quiz_copy.groupby(["question_id", "attributes.user.id"])

            processed_rows = []
            for (question_id, user_id), group in grouped_quiz:
                # Sort by start_time to ensure chronological order
                group_sorted = group.sort_values("start_time")

                # Build chat history from all entries in the group
                chat_history = []
                context = None
                for _, row in group_sorted.iterrows():
                    try:
                        input_messages = row["attributes.llm.input_messages"]

                        if isinstance(
                            row["attributes.llm.output_messages"], float
                        ) and math.isnan(row["attributes.llm.output_messages"]):
                            continue

                        output_messages = row["attributes.llm.output_messages"]

                        # Find the second last user message (the actual user query)
                        user_messages = [
                            msg
                            for msg in input_messages
                            if msg.get("message.role") == "user"
                        ]

                        if context is None:
                            context = user_messages[-1]["message.content"]

                        if len(user_messages) >= 2:
                            if "message.contents" in user_messages[-2]:
                                user_message = user_messages[-2]["message.contents"][0][
                                    "message_content.text"
                                ]
                            else:
                                user_message = user_messages[-2]["message.content"]

                            # Get AI response
                            if output_messages:
                                if "message.tool_calls" not in output_messages[0]:
                                    continue

                                try:
                                    ai_message = json.loads(
                                        output_messages[0]["message.tool_calls"][0][
                                            "tool_call.function.arguments"
                                        ]
                                    )
                                except:
                                    continue

                                chat_history.append(
                                    {"role": "user", "content": user_message}
                                )
                                chat_history.append(
                                    {"role": "assistant", "content": ai_message}
                                )
                    except Exception as e:
                        raise e

                if not chat_history:
                    print("no - Quiz 3")
                    continue

                # Take the last entry and add chat history
                last_entry = group_sorted.iloc[-1].copy()
                last_entry["chat_history"] = chat_history
                last_entry["context"] = context
                processed_rows.append(last_entry)

            if processed_rows:
                df_quiz_result = pd.DataFrame(processed_rows).drop(
                    ["question_id"], axis=1
                )
                result_dfs.append(df_quiz_result)

        # Combine all results
        if result_dfs:
            return pd.concat(result_dfs, ignore_index=True)
        else:
            return pd.DataFrame()

    return get_last_entries_with_chat_history(df_feedback)


def convert_feedback_span_to_conversations(row):
    conversation = {
        "id": row["context.span_id"],
        "start_time": row["start_time"].isoformat(),
        "end_time": row["end_time"].isoformat(),
        "uploaded_by": "Aman",
        "metadata": row["attributes.metadata"],
        "context": row["context"],
        "messages": row["chat_history"],
        "trace_id": row["context.trace_id"],
        "span_kind": row["span_kind"],
        "span_name": row["name"],
        "llm": {
            "model_name": row["attributes.llm.model_name"],
            "provider": row["attributes.llm.provider"],
        },
    }

    if isinstance(conversation["llm"]["provider"], float) and math.isnan(
        conversation["llm"]["provider"]
    ):
        conversation["llm"]["provider"] = None

    return conversation


def save_daily_traces():
    from phoenix.client import Client

    if settings.env != "production":
        # only run in production
        return

    # Process previous day from 00:00:00 to 23:59:59
    previous_day = datetime.now() - timedelta(days=1)
    start_date = previous_day.replace(hour=0, minute=0, second=0, microsecond=0)
    end_date = previous_day.replace(hour=23, minute=59, second=59, microsecond=0)

    print(
        f"Processing data for {start_date.strftime('%Y-%m-%d %H:%M')} to {end_date.strftime('%Y-%m-%d %H:%M')}",
        flush=True,
    )

    os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = settings.phoenix_endpoint
    os.environ["PHOENIX_API_KEY"] = settings.phoenix_api_key

    phoenix_client = Client()

    df = phoenix_client.spans.get_spans_dataframe(
        project_name=f"sensai-{settings.env}",
        start_time=start_date,
        end_time=end_date,
        timeout=1200,
        limit=100000,
    )

    print(f"Got {len(df)} spans", flush=True)

    # Save dataframe to temporary local file
    with tempfile.NamedTemporaryFile(
        mode="w", suffix=".csv", delete=False
    ) as temp_file:
        df.to_csv(temp_file.name, index=False)
        temp_filepath = temp_file.name

    # Upload to S3
    temp_filename = f"{start_date.strftime('%Y-%m-%d')}"
    s3_key = f"{settings.s3_folder_name}/phoenix/spans/{temp_filename}.csv"

    print(f"Saved to {temp_filepath}", flush=True)

    upload_file_to_s3(temp_filepath, s3_key)

    # Clean up temporary file
    os.remove(temp_filepath)

    print(f"Uploaded {len(df)} spans to S3 at key: {s3_key}", flush=True)

    feedback_traces_for_annotation_df = prepare_feedback_traces_for_annotation(df)

    feedback_conversations = feedback_traces_for_annotation_df.apply(
        convert_feedback_span_to_conversations, axis=1
    ).values.tolist()

    s3_key = f"{settings.s3_folder_name}/evals/conversations.json"

    with tempfile.NamedTemporaryFile(
        mode="wb", suffix=".json", delete=False
    ) as temp_file:
        file_bytes = download_file_from_s3_as_bytes(s3_key)
        temp_file.write(file_bytes)
        temp_filepath = temp_file.name

    conversations = json.loads(open(temp_filepath, "r").read())
    os.remove(temp_filepath)

    # Create backup with proper file handling
    with tempfile.NamedTemporaryFile(
        mode="w", suffix=".json", delete=False
    ) as temp_file:
        json.dump(conversations, temp_file, indent=4)
        temp_file.flush()  # Ensure all data is written to disk
        backup_filepath = temp_file.name

    upload_file_to_s3(
        backup_filepath,
        s3_key.replace(".json", "_backup.json"),
        content_type="application/json",
    )
    os.remove(backup_filepath)

    all_span_ids = set([c["id"] for c in conversations])
    new_count = 0

    for conversation in feedback_conversations:
        if conversation["id"] not in all_span_ids:
            new_count += 1
            conversations.append(conversation)

    # Save updated conversations with proper file handling
    with tempfile.NamedTemporaryFile(
        mode="w", suffix=".json", delete=False
    ) as temp_file:
        json.dump(conversations, temp_file, indent=4)
        temp_file.flush()  # Ensure all data is written to disk
        final_filepath = temp_file.name

    upload_file_to_s3(final_filepath, s3_key, content_type="application/json")
    os.remove(final_filepath)

    print(
        f"Uploaded {new_count} new feedback conversations to S3 at key: {s3_key}",
        flush=True,
    )

================
File: src/api/cron.py
================
from typing import Dict
from api.db.analytics import get_usage_summary_by_organization
from api.slack import send_slack_notification_for_usage_stats
from api.utils.phoenix import get_raw_traces, save_daily_traces


def get_model_summary_stats(filter_period: str) -> Dict[str, int]:
    df = get_raw_traces(filter_period)

    # Group by model name and count occurrences
    model_counts = df.groupby("attributes.llm.model_name").size().to_dict()
    return model_counts


async def send_usage_summary_stats():
    """
    Get usage summary statistics for different time periods and send them via Slack webhook.

    This function retrieves usage statistics for the last day, last month, and last year,
    then sends a formatted summary to a Slack channel via webhook.
    """
    try:
        # Get usage statistics for different time periods
        last_day_stats = {
            "org": await get_usage_summary_by_organization("last_day"),
            "model": get_model_summary_stats("last_day"),
        }
        current_month_stats = {
            "org": await get_usage_summary_by_organization("current_month"),
            "model": get_model_summary_stats("current_month"),
        }
        current_year_stats = {
            "org": await get_usage_summary_by_organization("current_year"),
            "model": get_model_summary_stats("current_year"),
        }

        # Send the statistics via Slack webhook
        await send_slack_notification_for_usage_stats(
            last_day_stats, current_month_stats, current_year_stats
        )

    except Exception as e:
        print(f"Error in get_usage_summary_stats: {e}")
        raise

================
File: docs/ENV.md
================
# Environment Variables

### OPENAI_API_KEY
The API key for the OpenAI API.

### GOOGLE_CLIENT_ID
The client ID for the Google OAuth2.0 client.

## Deployment-Only Variables

### S3_BUCKET_NAME
The name of the S3 bucket used for storing files uploaded by users.

### S3_FOLDER_NAME
The name of the S3 folder within the S3 bucket. We use the same bucket for dev and prod but with different folder names.

### BUGSNAG_API_KEY (optional)
The API key for the Bugsnag (used for error tracking).

### ENV (optional)
The environment the app is running in (staging/production).

### SLACK_USER_SIGNUP_WEBHOOK_URL (optional)
The Slack webhook URL for sending notifications to the user signup channel.

### SLACK_COURSE_CREATED_WEBHOOK_URL (optional)
The Slack webhook URL for sending notifications about new courses created.

### SLACK_USAGE_STATS_WEBHOOK_URL (optional)
The Slack webhook URL for sending platform usage statistics notifications.

### PHOENIX_ENDPOINT (optional)
The endpoint for the self-hosted Phoenix instance. This is only used for local development.

### PHOENIX_API_KEY (optional)
The API key for accessing the Phoenix API for a secure self-hosted instance.

================
File: src/api/routes/task.py
================
from fastapi import APIRouter, HTTPException, Query
from typing import List, Dict
from api.db.task import (
    get_solved_tasks_for_user as get_solved_tasks_for_user_from_db,
    get_task as get_task_from_db,
    delete_task as delete_task_in_db,
    delete_tasks as delete_tasks_in_db,
    create_draft_task_for_course as create_draft_task_for_course_in_db,
    update_learning_material_task as update_learning_material_task_in_db,
    update_draft_quiz as update_draft_quiz_in_db,
    update_published_quiz as update_published_quiz_in_db,
    mark_task_completed as mark_task_completed_in_db,
    duplicate_task as duplicate_task_in_db,
    get_all_learning_material_tasks_for_course as get_all_learning_material_tasks_for_course_from_db,
)
from api.models import (
    Task,
    LearningMaterialTask,
    QuizTask,
    LeaderboardViewType,
    UpdateDraftQuizRequest,
    CreateDraftTaskRequest,
    TaskCourseResponse,
    CreateDraftTaskResponse,
    PublishLearningMaterialTaskRequest,
    UpdateLearningMaterialTaskRequest,
    UpdatePublishedQuizRequest,
    DuplicateTaskRequest,
    DuplicateTaskResponse,
    MarkTaskCompletedRequest,
)

router = APIRouter()


@router.get("/course/{course_id}/learning_material")
async def get_learning_material_tasks_for_course(
    course_id: int,
) -> List[Task]:
    return await get_all_learning_material_tasks_for_course_from_db(course_id)


@router.post("/", response_model=CreateDraftTaskResponse)
async def create_draft_task_for_course(
    request: CreateDraftTaskRequest,
) -> CreateDraftTaskResponse:
    id, _ = await create_draft_task_for_course_in_db(
        request.title,
        str(request.type),
        request.course_id,
        request.milestone_id,
    )
    return {"id": id}


@router.post("/{task_id}/learning_material", response_model=LearningMaterialTask)
async def publish_learning_material_task(
    task_id: int, request: PublishLearningMaterialTaskRequest
) -> LearningMaterialTask:
    result = await update_learning_material_task_in_db(
        task_id,
        request.title,
        request.blocks,
        request.scheduled_publish_at,
    )
    if not result:
        raise HTTPException(status_code=404, detail="Task not found")
    return result


@router.put("/{task_id}/learning_material", response_model=LearningMaterialTask)
async def update_learning_material_task(
    task_id: int, request: UpdateLearningMaterialTaskRequest
) -> LearningMaterialTask:
    result = await update_learning_material_task_in_db(
        task_id,
        request.title,
        request.blocks,
        request.scheduled_publish_at,
        request.status,
    )
    if not result:
        raise HTTPException(status_code=404, detail="Task not found")
    return result


@router.post("/{task_id}/quiz", response_model=QuizTask)
async def update_draft_quiz(task_id: int, request: UpdateDraftQuizRequest) -> QuizTask:
    result = await update_draft_quiz_in_db(
        task_id=task_id,
        title=request.title,
        questions=request.questions,
        scheduled_publish_at=request.scheduled_publish_at,
        status=request.status,
    )
    if not result:
        raise HTTPException(status_code=404, detail="Task not found")
    return result


@router.put("/{task_id}/quiz", response_model=QuizTask)
async def update_published_quiz(
    task_id: int, request: UpdatePublishedQuizRequest
) -> QuizTask:
    result = await update_published_quiz_in_db(
        task_id=task_id,
        title=request.title,
        questions=request.questions,
        scheduled_publish_at=request.scheduled_publish_at,
    )
    if not result:
        raise HTTPException(status_code=404, detail="Task not found")
    return result


@router.post("/duplicate", response_model=DuplicateTaskResponse)
async def duplicate_task(
    request: DuplicateTaskRequest,
) -> DuplicateTaskResponse:
    return await duplicate_task_in_db(
        request.task_id, request.course_id, request.milestone_id
    )


@router.delete("/{task_id}")
async def delete_task(task_id: int):
    await delete_task_in_db(task_id)
    return {"success": True}


@router.delete("/")
async def delete_tasks(task_ids: List[int] = Query(...)):
    await delete_tasks_in_db(task_ids)
    return {"success": True}


@router.get("/cohort/{cohort_id}/user/{user_id}/completed", response_model=List[int])
async def get_tasks_completed_for_user(
    user_id: int,
    cohort_id: int,
    view: LeaderboardViewType = str(LeaderboardViewType.ALL_TIME),
) -> List[int]:
    return await get_solved_tasks_for_user_from_db(user_id, cohort_id, view)


@router.get("/{task_id}")
async def get_task(task_id: int) -> LearningMaterialTask | QuizTask:
    task = await get_task_from_db(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    return task


@router.post("/{task_id}/complete")
async def mark_task_completed(task_id: int, request: MarkTaskCompletedRequest):
    await mark_task_completed_in_db(task_id, request.user_id)
    return {"success": True}

================
File: src/api/public.py
================
from typing import Annotated, Dict, List, Optional, Tuple
from fastapi import FastAPI, Body, Header, HTTPException, Depends
from api.models import (
    PublicAPIChatMessage,
    CourseWithMilestonesAndTaskDetails,
    TaskType,
)
from api.db.chat import (
    get_all_chat_history as get_all_chat_history_from_db,
)
from api.db.course import (
    get_course as get_course_from_db,
    get_course_org_id,
)
from api.db.task import get_task as get_task_from_db
from api.db.org import get_org_id_from_api_key


app = FastAPI()


async def validate_api_key(api_key: str, org_id: int) -> None:
    """
    Validates if the provided API key is authorized to access data for the given organization ID.
    Raises an HTTP 403 exception if the API key is invalid or unauthorized.
    """
    try:
        # Get the org_id associated with the API key from the database
        key_org_id = await get_org_id_from_api_key(api_key)

        # If org_id is provided, check if it matches the org_id from the API key
        if not key_org_id or key_org_id != org_id:
            raise HTTPException(
                status_code=403,
                detail="Invalid API key",
            )
    except ValueError:
        raise HTTPException(status_code=403, detail="Invalid API key")


@app.get(
    "/chat_history",
    response_model=List[PublicAPIChatMessage],
)
async def get_all_chat_history(
    org_id: int,
    api_key: str = Header(...),
) -> List[PublicAPIChatMessage]:
    # Validate the API key for the given org_id
    await validate_api_key(api_key=api_key, org_id=org_id)
    return await get_all_chat_history_from_db(org_id)


@app.get(
    "/course/{course_id}",
    response_model=CourseWithMilestonesAndTaskDetails,
)
async def get_tasks_for_course(
    course_id: int,
    api_key: str = Header(...),
) -> CourseWithMilestonesAndTaskDetails:
    try:
        # Get the org_id from the API key
        org_id = await get_org_id_from_api_key(api_key)
    except ValueError:
        raise HTTPException(status_code=403, detail="Invalid API key")

    try:
        course_org_id = await get_course_org_id(course_id)
    except ValueError:
        raise HTTPException(status_code=404, detail="Course not found")

    if org_id != course_org_id:
        raise HTTPException(
            status_code=403,
            detail="Invalid API key",
        )

    # Validate the API key for the given org_id
    await validate_api_key(api_key=api_key, org_id=org_id)

    course = await get_course_from_db(course_id=course_id)

    for milestone in course["milestones"]:
        for task in milestone["tasks"]:
            task_details = await get_task_from_db(task["id"])

            if task["type"] == TaskType.LEARNING_MATERIAL:
                task["blocks"] = task_details["blocks"]
            else:
                task["questions"] = task_details["questions"]

    return course

================
File: .gitlab-ci.yml
================
stages:
  - test
  - build
  - deploy-dev
  - deploy-demo

test:
  stage: test
  image: python:3.10
  before_script:
    - pip install -r requirements-dev.txt
    - cp src/api/.env.example src/api/.env
  script:
    - python -m pytest tests/ -v --cov=src --cov-report=xml:coverage.xml --cov-config=pytest.ini
    - |
      # Pick the right variable for every pipeline type
      BRANCH_NAME="${CI_COMMIT_REF_NAME:-$CI_MERGE_REQUEST_SOURCE_BRANCH_NAME}"
      
      codecovcli upload-process -B $BRANCH_NAME -C $CI_COMMIT_SHA
  artifacts:
    paths:
      - coverage.xml
    expire_in: 1 week
  rules:
    - if: '$CI_PIPELINE_SOURCE == "merge_request_event" && $CI_MERGE_REQUEST_TARGET_BRANCH_NAME == "main"'
    - if: '$CI_COMMIT_BRANCH == "main"'
    - if: $CI_PIPELINE_SOURCE == "web"

build:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  script:
    - if [ "$CI_COMMIT_REF_NAME" == "v2-dev" ]; then export APP_URL=$DEV_APP_URL; else export APP_URL=$PROD_APP_URL; fi
    - if [ "$CI_COMMIT_REF_NAME" == "v2-dev" ]; then export ENV=$DEV_ENV; else export ENV=$PROD_ENV; fi
    - if [ "$CI_COMMIT_REF_NAME" == "v2-dev" ]; then export S3_BUCKET_NAME=$DEV_S3_BUCKET_NAME; else export S3_BUCKET_NAME=$PROD_S3_BUCKET_NAME; fi
    - if [ "$CI_COMMIT_REF_NAME" == "v2-dev" ]; then export S3_FOLDER_NAME=$DEV_S3_FOLDER_NAME; else export S3_FOLDER_NAME=$PROD_S3_FOLDER_NAME; fi
    - docker build --build-arg APP_URL=$APP_URL --build-arg S3_BUCKET_NAME=$S3_BUCKET_NAME --build-arg S3_FOLDER_NAME=$S3_FOLDER_NAME --build-arg ENV=$ENV --platform linux/amd64 -t $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME .
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD
    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME
  rules:
    # - if: '$CI_PIPELINE_SOURCE == "push" && ($CI_COMMIT_REF_NAME == "v2-dev" || $CI_COMMIT_REF_NAME == "v2")'
    - if: '$CI_PIPELINE_SOURCE == "schedule" && ($CI_COMMIT_BRANCH == "v2-dev" || $CI_COMMIT_BRANCH == "v2")'
    - if: '$CI_PIPELINE_SOURCE == "web" && ($CI_COMMIT_BRANCH == "v2-dev" || $CI_COMMIT_BRANCH == "v2")'

deploy-dev:
  stage: deploy-dev
  image: ubuntu:latest
  before_script:
    - apt-get update
    - apt-get install -y openssh-client
    - eval $(ssh-agent -s)
    - echo "$SSH_DEV_PRIVATE_KEY" | ssh-add -
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - ssh-keyscan -H $DEPLOY_DEMO_SERVER_IP >> ~/.ssh/known_hosts
  script:
    - scp -o StrictHostKeyChecking=no docker-compose.ai.dev.yml $DEPLOY_DEV_SERVER_USERNAME@$DEPLOY_DEMO_SERVER_IP:~/docker-compose.ai.dev.yml
    - ssh $DEPLOY_DEV_SERVER_USERNAME@$DEPLOY_DEMO_SERVER_IP "sudo docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD"
    - ssh $DEPLOY_DEV_SERVER_USERNAME@$DEPLOY_DEMO_SERVER_IP 'sudo docker compose -f docker-compose.ai.dev.yml -p sensai-dev down --rmi "all"'
    - ssh $DEPLOY_DEV_SERVER_USERNAME@$DEPLOY_DEMO_SERVER_IP 'sudo docker compose -f docker-compose.ai.dev.yml pull'
    - ssh $DEPLOY_DEV_SERVER_USERNAME@$DEPLOY_DEMO_SERVER_IP 'sudo docker compose -f docker-compose.ai.dev.yml -p sensai-dev up -d'
  rules:
    # - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_REF_NAME == "v2-dev" '
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $CI_COMMIT_BRANCH == "v2-dev"'
    - if: '$CI_PIPELINE_SOURCE == "web" && $CI_COMMIT_BRANCH == "v2-dev"'

deploy-demo:
  stage: deploy-demo
  image: ubuntu:latest
  before_script:
    - apt-get update
    - apt-get install -y openssh-client
    - eval $(ssh-agent -s)
    - echo "$SSH_DEV_PRIVATE_KEY" | ssh-add -
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh
    - ssh-keyscan -H $DEPLOY_DEMO_SERVER_IP >> ~/.ssh/known_hosts
  script:
    - scp -o StrictHostKeyChecking=no docker-compose.ai.demo.yml $DEPLOY_DEV_SERVER_USERNAME@$DEPLOY_DEMO_SERVER_IP:~/docker-compose.ai.demo.yml
    - ssh $DEPLOY_DEV_SERVER_USERNAME@$DEPLOY_DEMO_SERVER_IP "sudo docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD"
    - ssh $DEPLOY_DEV_SERVER_USERNAME@$DEPLOY_DEMO_SERVER_IP 'sudo docker compose -f docker-compose.ai.demo.yml -p sensai-prod down --rmi "all"'
    - ssh $DEPLOY_DEV_SERVER_USERNAME@$DEPLOY_DEMO_SERVER_IP 'sudo docker compose -f docker-compose.ai.demo.yml pull'
    - ssh $DEPLOY_DEV_SERVER_USERNAME@$DEPLOY_DEMO_SERVER_IP 'sudo docker compose -f docker-compose.ai.demo.yml -p sensai-prod up -d'
  rules:
    # - if: '$CI_PIPELINE_SOURCE == "push" && $CI_COMMIT_REF_NAME == "v2" '
    - if: '$CI_PIPELINE_SOURCE == "schedule" && $CI_COMMIT_BRANCH == "v2"'
    - if: '$CI_PIPELINE_SOURCE == "web" && $CI_COMMIT_BRANCH == "v2"'

================
File: Dockerfile
================
FROM python:3.13.1-slim-bookworm

RUN apt-get update && apt-get install -y \
    gcc \
    python3-dev \
    curl \
    wget \
    fontconfig \
    libfreetype6 \
    libjpeg62-turbo \
    libpng16-16 \
    libx11-6 \
    libxcb1 \
    libxext6 \
    libxrender1 \
    xfonts-75dpi \
    xfonts-base \
    ffmpeg \
    poppler-utils

# Install Node.js and npm
RUN curl -fsSL https://deb.nodesource.com/setup_lts.x | bash -
RUN apt-get install -y nodejs git

# Install libssl1.1
RUN wget http://archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb
RUN dpkg -i libssl1.1_1.1.1f-1ubuntu2_amd64.deb

# Install wkhtmltopdf
RUN wget https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6.1-2/wkhtmltox_0.12.6.1-2.bullseye_amd64.deb \
    && dpkg -i wkhtmltox_0.12.6.1-2.bullseye_amd64.deb \
    && apt-get install -f \
    && rm wkhtmltox_0.12.6.1-2.bullseye_amd64.deb

# Verify wkhtmltopdf installation
RUN wkhtmltopdf --version

# Copy requirements.txt to the container
COPY requirements.txt ./

# Install app dependencies
RUN pip install -r requirements.txt

COPY src /src

RUN test -f /src/api/.env && rm -f /src/api/.env || true
RUN test -f /src/api/.env.aws && rm -f /src/api/.env.aws || true

# Expose the port on which your FastAPI app listens
# EXPOSE 8001
EXPOSE 8002

# Only expose one port where everything is hosted
EXPOSE 8501

ARG S3_BUCKET_NAME

ARG S3_FOLDER_NAME

ARG ENV

ARG OPENAI_API_KEY

ARG GOOGLE_CLIENT_ID

ARG BUGSNAG_API_KEY

ARG SLACK_USER_SIGNUP_WEBHOOK_URL

ARG SLACK_COURSE_CREATED_WEBHOOK_URL

ARG SLACK_USAGE_STATS_WEBHOOK_URL

ARG PHOENIX_ENDPOINT

ARG PHOENIX_API_KEY

RUN printf "OPENAI_API_KEY=$OPENAI_API_KEY\nGOOGLE_CLIENT_ID=$GOOGLE_CLIENT_ID\nS3_BUCKET_NAME=$S3_BUCKET_NAME\nS3_FOLDER_NAME=$S3_FOLDER_NAME\nENV=$ENV\nBUGSNAG_API_KEY=$BUGSNAG_API_KEY\nSLACK_USER_SIGNUP_WEBHOOK_URL=$SLACK_USER_SIGNUP_WEBHOOK_URL\nSLACK_COURSE_CREATED_WEBHOOK_URL=$SLACK_COURSE_CREATED_WEBHOOK_URL\nSLACK_USAGE_STATS_WEBHOOK_URL=$SLACK_USAGE_STATS_WEBHOOK_URL\nPHOENIX_ENDPOINT=$PHOENIX_ENDPOINT\nPHOENIX_API_KEY=$PHOENIX_API_KEY" >> /src/api/.env

# Clean up
RUN apt-get clean && rm -rf /var/lib/apt/lists/*

================
File: src/api/config.py
================
import os
from os.path import exists
from api.models import LeaderboardViewType, TaskInputType, TaskAIResponseType, TaskType

if exists("/appdata"):
    data_root_dir = "/appdata"
    root_dir = "/demo"
    log_dir = "/appdata/logs"
else:
    root_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(root_dir)

    data_root_dir = f"{parent_dir}/db"
    log_dir = f"{parent_dir}/logs"

if not exists(data_root_dir):
    os.makedirs(data_root_dir)

if not exists(log_dir):
    os.makedirs(log_dir)


sqlite_db_path = f"{data_root_dir}/db.sqlite"
log_file_path = f"{log_dir}/backend.log"

chat_history_table_name = "chat_history"
tasks_table_name = "tasks"
questions_table_name = "questions"
blocks_table_name = "blocks"
tests_table_name = "tests"
cohorts_table_name = "cohorts"
course_tasks_table_name = "course_tasks"
course_milestones_table_name = "course_milestones"
courses_table_name = "courses"
course_cohorts_table_name = "course_cohorts"
task_scoring_criteria_table_name = "task_scoring_criteria"
groups_table_name = "groups"
user_cohorts_table_name = "user_cohorts"
user_groups_table_name = "user_groups"
milestones_table_name = "milestones"
tags_table_name = "tags"
task_tags_table_name = "task_tags"
users_table_name = "users"
badges_table_name = "badges"
cv_review_usage_table_name = "cv_review_usage"
organizations_table_name = "organizations"
user_organizations_table_name = "user_organizations"
task_completions_table_name = "task_completions"
scorecards_table_name = "scorecards"
question_scorecards_table_name = "question_scorecards"
group_role_learner = "learner"
group_role_mentor = "mentor"
course_generation_jobs_table_name = "course_generation_jobs"
task_generation_jobs_table_name = "task_generation_jobs"
org_api_keys_table_name = "org_api_keys"
code_drafts_table_name = "code_drafts"

UPLOAD_FOLDER_NAME = "uploads"

uncategorized_milestone_name = "[UNASSIGNED]"
uncategorized_milestone_color = "#808080"

openai_plan_to_model_name = {
    "reasoning": "o3-mini-2025-01-31",
    "text": "gpt-4.1-2025-04-14",
    "text-mini": "gpt-4.1-mini-2025-04-14",
    "audio": "gpt-4o-audio-preview-2024-12-17",
    "router": "gpt-4.1-mini-2025-04-14",
}

================
File: src/api/main.py
================
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.exceptions import RequestValidationError
import os
from os.path import exists
from api.config import UPLOAD_FOLDER_NAME
from api.routes import (
    auth,
    code,
    cohort,
    course,
    org,
    task,
    chat,
    user,
    milestone,
    hva,
    file,
    ai,
    scorecard,
)
from api.routes.ai import (
    resume_pending_task_generation_jobs,
    resume_pending_course_structure_generation_jobs,
)
from api.websockets import router as websocket_router
from api.scheduler import scheduler
from api.settings import settings
import bugsnag
from bugsnag.asgi import BugsnagMiddleware


@asynccontextmanager
async def lifespan(app: FastAPI):
    scheduler.start()

    # Create the uploads directory if it doesn't exist
    os.makedirs(settings.local_upload_folder, exist_ok=True)

    # Add recovery logic for interrupted tasks
    asyncio.create_task(resume_pending_task_generation_jobs())
    asyncio.create_task(resume_pending_course_structure_generation_jobs())

    yield
    scheduler.shutdown()


if settings.bugsnag_api_key:
    bugsnag.configure(
        api_key=settings.bugsnag_api_key,
        project_root=os.path.dirname(os.path.abspath(__file__)),
        release_stage=settings.env or "development",
        notify_release_stages=["development", "staging", "production"],
        auto_capture_sessions=True,
    )


app = FastAPI(lifespan=lifespan)

# Add Bugsnag middleware if configured
if settings.bugsnag_api_key:
    app.add_middleware(BugsnagMiddleware)

    @app.middleware("http")
    async def bugsnag_request_middleware(request: Request, call_next):
        # Add request metadata to Bugsnag context
        bugsnag.configure_request(
            context=f"{request.method} {request.url.path}",
            request_data={
                "url": str(request.url),
                "method": request.method,
                "headers": dict(request.headers),
                "query_params": dict(request.query_params),
                "path_params": request.path_params,
                "client": {
                    "host": request.client.host if request.client else None,
                    "port": request.client.port if request.client else None,
                },
            },
        )

        response = await call_next(request)
        return response


# Add CORS middleware to allow cross-origin requests (for frontend to access backend)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Replace with your frontend URL in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount the uploads folder as a static directory
if exists(settings.local_upload_folder):
    app.mount(
        f"/{UPLOAD_FOLDER_NAME}",
        StaticFiles(directory=settings.local_upload_folder),
        name="uploads",
    )

app.include_router(file.router, prefix="/file", tags=["file"])
app.include_router(ai.router, prefix="/ai", tags=["ai"])
app.include_router(auth.router, prefix="/auth", tags=["auth"])
app.include_router(task.router, prefix="/tasks", tags=["tasks"])
app.include_router(chat.router, prefix="/chat", tags=["chat"])
app.include_router(user.router, prefix="/users", tags=["users"])
app.include_router(org.router, prefix="/organizations", tags=["organizations"])
app.include_router(cohort.router, prefix="/cohorts", tags=["cohorts"])
app.include_router(course.router, prefix="/courses", tags=["courses"])
app.include_router(milestone.router, prefix="/milestones", tags=["milestones"])
app.include_router(scorecard.router, prefix="/scorecards", tags=["scorecards"])
app.include_router(code.router, prefix="/code", tags=["code"])
app.include_router(hva.router, prefix="/hva", tags=["hva"])
app.include_router(websocket_router, prefix="/ws", tags=["websockets"])


@app.get("/health")
async def health_check():
    return {"status": "ok"}

================
File: requirements.txt
================
langchain-core==0.3.40
openai==1.74.0
# streamlit==1.41.1
email_validator==2.2.0
python-dotenv==1.0.1
pydantic==2.8.2
pydantic_core==2.20.1
pydantic-settings==2.7.0
backoff==2.2.1
fastapi==0.114.1
uvicorn==0.30.6
streamlit-ace==0.1.1
streamlit-extras==0.5.0
boto3==1.37.18
botocore==1.37.18
httpx==0.27.0
st-theme==1.2.3
instructor==1.7.9
imgkit==1.2.3
pydub==0.25.1
pypdf==5.1.0
streamlit-pdf-viewer==0.0.19
pdf2image==1.17.0
unidecode==1.3.8
seaborn==0.13.2
aiosqlite==0.21.0
google-auth==2.38.0
pyasn1-modules==0.4.1
apscheduler==3.11.0
httptools==0.6.4
uvloop==0.21.0
watchfiles==1.0.5
websockets==15.0.1
python-multipart==0.0.20
bugsnag==4.7.1
arize-phoenix==10.10.0
arize-phoenix-otel==0.10.3
openinference-instrumentation-openai==0.1.30boto3

================
File: src/api/settings.py
================
import os
from os.path import join
from pydantic_settings import BaseSettings, SettingsConfigDict
from dotenv import load_dotenv
from functools import lru_cache
from api.config import UPLOAD_FOLDER_NAME
from phoenix.otel import register

root_dir = os.path.dirname(os.path.abspath(__file__))
env_path = join(root_dir, ".env.aws")
if os.path.exists(env_path):
    load_dotenv(env_path)


class Settings(BaseSettings):
    google_client_id: str
    openai_api_key: str
    s3_bucket_name: str | None = None  # only relevant when running the code remotely
    s3_folder_name: str | None = None  # only relevant when running the code remotely
    local_upload_folder: str = (
        UPLOAD_FOLDER_NAME  # hardcoded variable for local file storage
    )
    bugsnag_api_key: str | None = None
    env: str | None = None
    slack_user_signup_webhook_url: str | None = None
    slack_course_created_webhook_url: str | None = None
    slack_usage_stats_webhook_url: str | None = None
    phoenix_endpoint: str | None = None
    phoenix_api_key: str | None = None

    model_config = SettingsConfigDict(env_file=join(root_dir, ".env"))


@lru_cache
def get_settings():
    return Settings()


settings = get_settings()

if settings.phoenix_api_key is not None:
    os.environ["PHOENIX_API_KEY"] = settings.phoenix_api_key

tracer_provider = register(
    protocol="http/protobuf",
    project_name=f"sensai-{settings.env}",
    auto_instrument=True,
    batch=True,
    endpoint=(
        f"{settings.phoenix_endpoint}/v1/traces" if settings.phoenix_endpoint else None
    ),
)
tracer = tracer_provider.get_tracer(__name__)

================
File: src/api/models.py
================
from enum import Enum
from pydantic import BaseModel
from typing import List, Tuple, Optional, Dict, Literal
from datetime import datetime


class UserLoginData(BaseModel):
    email: str
    given_name: str
    family_name: str | None = None
    id_token: str  # Google authentication token


class CreateOrganizationRequest(BaseModel):
    name: str
    slug: str
    user_id: int


class CreateOrganizationResponse(BaseModel):
    id: int


class RemoveMembersFromOrgRequest(BaseModel):
    user_ids: List[int]


class AddUsersToOrgRequest(BaseModel):
    emails: List[str]


class UpdateOrgRequest(BaseModel):
    name: str


class UpdateOrgOpenaiApiKeyRequest(BaseModel):
    encrypted_openai_api_key: str
    is_free_trial: bool


class AddMilestoneRequest(BaseModel):
    name: str
    color: str
    org_id: int


class UpdateMilestoneRequest(BaseModel):
    name: str


class CreateTagRequest(BaseModel):
    name: str
    org_id: int


class CreateBulkTagsRequest(BaseModel):
    tag_names: List[str]
    org_id: int


class CreateBadgeRequest(BaseModel):
    user_id: int
    value: str
    badge_type: str
    image_path: str
    bg_color: str
    cohort_id: int


class UpdateBadgeRequest(BaseModel):
    value: str
    badge_type: str
    image_path: str
    bg_color: str


class CreateCohortRequest(BaseModel):
    name: str
    org_id: int


class CreateCohortResponse(BaseModel):
    id: int


class AddMembersToCohortRequest(BaseModel):
    org_slug: Optional[str] = None
    org_id: Optional[int] = None
    emails: List[str]
    roles: List[str]


class RemoveMembersFromCohortRequest(BaseModel):
    member_ids: List[int]


class UpdateCohortRequest(BaseModel):
    name: str


class UpdateCohortGroupRequest(BaseModel):
    name: str


class CreateCohortGroupRequest(BaseModel):
    name: str
    member_ids: List[int]


class AddMembersToCohortGroupRequest(BaseModel):
    member_ids: List[int]


class RemoveMembersFromCohortGroupRequest(BaseModel):
    member_ids: List[int]


class RemoveCoursesFromCohortRequest(BaseModel):
    course_ids: List[int]


class DripConfig(BaseModel):
    is_drip_enabled: Optional[bool] = False
    frequency_value: Optional[int] = None
    frequency_unit: Optional[str] = None
    publish_at: Optional[datetime] = None


class AddCoursesToCohortRequest(BaseModel):
    course_ids: List[int]
    drip_config: Optional[DripConfig] = DripConfig()


class CreateCourseRequest(BaseModel):
    name: str
    org_id: int


class CreateCourseResponse(BaseModel):
    id: int


class Course(BaseModel):
    id: int
    name: str


class CourseCohort(Course):
    drip_config: DripConfig


class CohortCourse(Course):
    drip_config: DripConfig


class Milestone(BaseModel):
    id: int
    name: str | None
    color: Optional[str] = None
    ordering: Optional[int] = None
    unlock_at: Optional[datetime] = None


class TaskType(Enum):
    QUIZ = "quiz"
    LEARNING_MATERIAL = "learning_material"

    def __str__(self):
        return self.value

    def __eq__(self, other):
        if isinstance(other, str):
            return self.value == other
        elif isinstance(other, TaskType):
            return self.value == other.value
        return False


class TaskStatus(Enum):
    DRAFT = "draft"
    PUBLISHED = "published"

    def __str__(self):
        return self.value

    def __eq__(self, other):
        if isinstance(other, str):
            return self.value == other
        elif isinstance(other, TaskStatus):
            return self.value == other.value

        return False


class Task(BaseModel):
    id: int
    title: str
    type: TaskType
    status: TaskStatus
    scheduled_publish_at: datetime | None


class Block(BaseModel):
    id: Optional[str] = None
    type: str
    props: Optional[Dict] = {}
    content: Optional[List] = []
    children: Optional[List] = []
    position: Optional[int] = (
        None  # not present when sent from frontend at the time of publishing
    )


class LearningMaterialTask(Task):
    blocks: List[Block]


class TaskInputType(Enum):
    CODE = "code"
    TEXT = "text"
    AUDIO = "audio"

    def __str__(self):
        return self.value

    def __eq__(self, other):
        if isinstance(other, str):
            return self.value == other
        elif isinstance(other, TaskInputType):
            return self.value == other.value

        return False


class TaskAIResponseType(Enum):
    CHAT = "chat"
    EXAM = "exam"

    def __str__(self):
        return self.value

    def __eq__(self, other):
        if isinstance(other, str):
            return self.value == other
        elif isinstance(other, TaskAIResponseType):
            return self.value == other.value

        return False


class QuestionType(Enum):
    OPEN_ENDED = "subjective"
    OBJECTIVE = "objective"

    def __str__(self):
        return self.value

    def __eq__(self, other):
        if isinstance(other, str):
            return self.value == other
        elif isinstance(other, QuestionType):
            return self.value == other.value

        return False


class ScorecardCriterion(BaseModel):
    name: str
    description: str
    min_score: float
    max_score: float
    pass_score: float


class ScorecardStatus(Enum):
    PUBLISHED = "published"
    DRAFT = "draft"

    def __str__(self):
        return self.value

    def __eq__(self, other):
        if isinstance(other, str):
            return self.value == other
        elif isinstance(other, ScorecardStatus):
            return self.value == other.value

        return False


class BaseScorecard(BaseModel):
    title: str
    criteria: List[ScorecardCriterion]


class CreateScorecardRequest(BaseScorecard):
    org_id: int


class NewScorecard(BaseScorecard):
    id: str | int


class Scorecard(BaseScorecard):
    id: int
    status: ScorecardStatus


class DraftQuestion(BaseModel):
    blocks: List[Block]
    answer: List[Block] | None
    type: QuestionType
    input_type: TaskInputType
    response_type: TaskAIResponseType
    context: Dict | None
    coding_languages: List[str] | None
    scorecard_id: Optional[int] = None
    title: str


class PublishedQuestion(DraftQuestion):
    id: int
    scorecard_id: Optional[int] = None
    max_attempts: Optional[int] = None
    is_feedback_shown: Optional[bool] = None


class QuizTask(Task):
    questions: List[PublishedQuestion]


class GenerateCourseJobStatus(str, Enum):
    STARTED = "started"
    PENDING = "pending"
    COMPLETED = "completed"
    FAILED = "failed"

    def __str__(self):
        return self.value

    def __eq__(self, other):
        if isinstance(other, str):
            return self.value == other
        elif isinstance(other, GenerateCourseJobStatus):
            return self.value == other.value
        return self == other


class GenerateTaskJobStatus(str, Enum):
    STARTED = "started"
    COMPLETED = "completed"
    FAILED = "failed"

    def __str__(self):
        return self.value

    def __eq__(self, other):
        if isinstance(other, str):
            return self.value == other
        elif isinstance(other, GenerateTaskJobStatus):
            return self.value == other.value

        return False


class MilestoneTask(Task):
    ordering: int
    num_questions: int | None
    is_generating: bool


class MilestoneTaskWithDetails(MilestoneTask):
    blocks: Optional[List[Block]] = None
    questions: Optional[List[PublishedQuestion]] = None


class MilestoneWithTasks(Milestone):
    tasks: List[MilestoneTask]


class MilestoneWithTaskDetails(Milestone):
    tasks: List[MilestoneTaskWithDetails]


class CourseWithMilestonesAndTasks(Course):
    milestones: List[MilestoneWithTasks]
    course_generation_status: GenerateCourseJobStatus | None


class CourseWithMilestonesAndTaskDetails(CourseWithMilestonesAndTasks):
    milestones: List[MilestoneWithTaskDetails]
    course_generation_status: GenerateCourseJobStatus | None


class UserCourseRole(str, Enum):
    ADMIN = "admin"
    LEARNER = "learner"
    MENTOR = "mentor"

    def __str__(self):
        return self.value

    def __eq__(self, other):
        if isinstance(other, str):
            return self.value == other
        elif isinstance(other, UserCourseRole):
            return self.value == other.value

        return False


class Organization(BaseModel):
    id: int
    name: str
    slug: str


class UserCourse(Course):
    role: UserCourseRole
    org: Organization
    cohort_id: Optional[int] = None


class AddCourseToCohortsRequest(BaseModel):
    cohort_ids: List[int]
    drip_config: Optional[DripConfig] = DripConfig()


class RemoveCourseFromCohortsRequest(BaseModel):
    cohort_ids: List[int]


class UpdateCourseNameRequest(BaseModel):
    name: str


class ChatRole(str, Enum):
    USER = "user"
    ASSISTANT = "assistant"


class ChatResponseType(str, Enum):
    TEXT = "text"
    CODE = "code"
    AUDIO = "audio"


class ChatMessage(BaseModel):
    id: int
    created_at: str
    user_id: int
    question_id: int
    role: ChatRole | None
    content: Optional[str] | None
    response_type: Optional[ChatResponseType] | None


class PublicAPIChatMessage(ChatMessage):
    task_id: int
    user_email: str


class Tag(BaseModel):
    id: int
    name: str


class User(BaseModel):
    id: int
    email: str
    first_name: str | None
    middle_name: str | None
    last_name: str | None


class UserStreak(BaseModel):
    user: User
    count: int


Streaks = List[UserStreak]


class LeaderboardViewType(Enum):
    ALL_TIME = "All time"
    WEEKLY = "This week"
    MONTHLY = "This month"

    def __str__(self):
        return self.value

    def __eq__(self, other):
        if isinstance(other, str):
            return self.value == other
        elif isinstance(other, LeaderboardViewType):
            return self.value == other.value
        raise NotImplementedError


class CreateDraftTaskRequest(BaseModel):
    course_id: int
    milestone_id: int
    type: TaskType
    title: str


class CreateDraftTaskResponse(BaseModel):
    id: int


class PublishLearningMaterialTaskRequest(BaseModel):
    title: str
    blocks: List[dict]
    scheduled_publish_at: datetime | None


class UpdateLearningMaterialTaskRequest(PublishLearningMaterialTaskRequest):
    status: TaskStatus


class CreateQuestionRequest(DraftQuestion):
    generation_model: str | None
    max_attempts: int | None
    is_feedback_shown: bool | None
    context: Dict | None


class UpdateDraftQuizRequest(BaseModel):
    title: str
    questions: List[CreateQuestionRequest]
    scheduled_publish_at: datetime | None
    status: TaskStatus


class UpdateQuestionRequest(BaseModel):
    id: int
    blocks: List[dict]
    coding_languages: List[str] | None
    answer: List[Block] | None
    scorecard_id: Optional[int] = None
    input_type: TaskInputType | None
    context: Dict | None
    response_type: TaskAIResponseType | None
    type: QuestionType | None
    title: str


class UpdatePublishedQuizRequest(BaseModel):
    title: str
    questions: List[UpdateQuestionRequest]
    scheduled_publish_at: datetime | None


class DuplicateTaskRequest(BaseModel):
    task_id: int
    course_id: int
    milestone_id: int


class DuplicateTaskResponse(BaseModel):
    task: LearningMaterialTask | QuizTask
    ordering: int


class StoreMessageRequest(BaseModel):
    role: str
    content: str | None
    response_type: ChatResponseType | None = None
    created_at: datetime


class StoreMessagesRequest(BaseModel):
    messages: List[StoreMessageRequest]
    user_id: int
    question_id: int
    is_complete: bool


class GetUserChatHistoryRequest(BaseModel):
    task_ids: List[int]


class TaskTagsRequest(BaseModel):
    tag_ids: List[int]


class AddScoringCriteriaToTasksRequest(BaseModel):
    task_ids: List[int]
    scoring_criteria: List[Dict]


class AddTasksToCoursesRequest(BaseModel):
    course_tasks: List[Tuple[int, int, int | None]]


class RemoveTasksFromCoursesRequest(BaseModel):
    course_tasks: List[Tuple[int, int]]


class UpdateTaskOrdersRequest(BaseModel):
    task_orders: List[Tuple[int, int]]


class AddMilestoneToCourseRequest(BaseModel):
    name: str
    color: str


class AddMilestoneToCourseResponse(BaseModel):
    id: int


class UpdateMilestoneOrdersRequest(BaseModel):
    milestone_orders: List[Tuple[int, int]]


class UpdateTaskTestsRequest(BaseModel):
    tests: List[dict]


class TaskCourse(Course):
    milestone: Milestone | None


class TaskCourseResponse(BaseModel):
    task_id: int
    courses: List[TaskCourse]


class AddCVReviewUsageRequest(BaseModel):
    user_id: int
    role: str
    ai_review: str


class UserCohort(BaseModel):
    id: int
    name: str
    role: Literal[UserCourseRole.LEARNER, UserCourseRole.MENTOR]
    joined_at: Optional[datetime] = None


class AIChatRequest(BaseModel):
    user_response: str
    task_type: TaskType
    question: Optional[DraftQuestion] = None
    chat_history: Optional[List[Dict]] = None
    question_id: Optional[int] = None
    user_id: int
    task_id: int
    response_type: Optional[ChatResponseType] = None


class MarkTaskCompletedRequest(BaseModel):
    user_id: int


class GetUserStreakResponse(BaseModel):
    streak_count: int
    active_days: List[str]


class PresignedUrlRequest(BaseModel):
    content_type: str = "audio/wav"


class PresignedUrlResponse(BaseModel):
    presigned_url: str
    file_key: str
    file_uuid: str


class S3FetchPresignedUrlResponse(BaseModel):
    url: str


class SwapMilestoneOrderingRequest(BaseModel):
    milestone_1_id: int
    milestone_2_id: int


class SwapTaskOrderingRequest(BaseModel):
    task_1_id: int
    task_2_id: int


class GenerateCourseStructureRequest(BaseModel):
    course_description: str
    intended_audience: str
    instructions: Optional[str] = None
    reference_material_s3_key: str


class LanguageCodeDraft(BaseModel):
    language: str
    value: str


class SaveCodeDraftRequest(BaseModel):
    user_id: int
    question_id: int
    code: List[LanguageCodeDraft]


class CodeDraft(BaseModel):
    id: int
    code: List[LanguageCodeDraft]

================
File: src/api/routes/ai.py
================
from ast import List
import os
import tempfile
import random
from collections import defaultdict
import asyncio
from fastapi import APIRouter, HTTPException, Body, BackgroundTasks
from fastapi.responses import StreamingResponse
from typing import List, Optional, Dict, Literal, AsyncGenerator
import json
import instructor
import openai
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from api.config import openai_plan_to_model_name
from api.models import (
    TaskAIResponseType,
    AIChatRequest,
    ChatResponseType,
    TaskType,
    GenerateCourseStructureRequest,
    GenerateCourseJobStatus,
    GenerateTaskJobStatus,
    QuestionType,
)
from api.llm import run_llm_with_instructor, stream_llm_with_instructor
from api.settings import settings
from api.utils.logging import logger
from api.utils.concurrency import async_batch_gather
from api.websockets import get_manager
from api.db.task import (
    get_task_metadata,
    get_question,
    get_task,
    get_scorecard,
    create_draft_task_for_course,
    store_task_generation_request,
    update_task_generation_job_status,
    get_course_task_generation_jobs_status,
    add_generated_learning_material,
    add_generated_quiz,
    get_all_pending_task_generation_jobs,
)
from api.db.course import (
    store_course_generation_request,
    get_course_generation_job_details,
    update_course_generation_job_status_and_details,
    update_course_generation_job_status,
    get_all_pending_course_structure_generation_jobs,
    add_milestone_to_course,
)
from api.db.chat import get_question_chat_history_for_user
from api.db.utils import construct_description_from_blocks
from api.utils.s3 import (
    download_file_from_s3_as_bytes,
    get_media_upload_s3_key_from_uuid,
)
from api.utils.audio import prepare_audio_input_for_ai
from api.settings import tracer
from opentelemetry.trace import StatusCode, Status
from openinference.instrumentation import using_attributes

router = APIRouter()


def get_user_audio_message_for_chat_history(uuid: str) -> List[Dict]:
    if settings.s3_folder_name:
        audio_data = download_file_from_s3_as_bytes(
            get_media_upload_s3_key_from_uuid(uuid, "wav")
        )
    else:
        with open(os.path.join(settings.local_upload_folder, f"{uuid}.wav"), "rb") as f:
            audio_data = f.read()

    return [
        {
            "type": "text",
            "text": "Student's Response:",
        },
        {
            "type": "input_audio",
            "input_audio": {
                "data": prepare_audio_input_for_ai(audio_data),
                "format": "wav",
            },
        },
    ]


def get_ai_message_for_chat_history(ai_message: Dict) -> str:
    message = json.loads(ai_message)

    if "scorecard" not in message or not message["scorecard"]:
        return message["feedback"]

    scorecard_as_prompt = []
    for criterion in message["scorecard"]:
        row_as_prompt = ""
        row_as_prompt += f"""- **{criterion['category']}**\n"""
        if criterion["feedback"].get("correct"):
            row_as_prompt += (
                f"""  What worked well: {criterion['feedback']['correct']}\n"""
            )
        if criterion["feedback"].get("wrong"):
            row_as_prompt += (
                f"""  What needs improvement: {criterion['feedback']['wrong']}\n"""
            )
        row_as_prompt += f"""  Score: {criterion['score']}"""
        scorecard_as_prompt.append(row_as_prompt)

    scorecard_as_prompt = "\n".join(scorecard_as_prompt)
    return f"""Feedback:\n```\n{message['feedback']}\n```\n\nScorecard:\n```\n{scorecard_as_prompt}\n```"""


def get_user_message_for_chat_history(user_response: str) -> str:
    return f"""Student's Response:\n```\n{user_response}\n```"""


@router.post("/chat")
async def ai_response_for_question(request: AIChatRequest):
    metadata = {"task_id": request.task_id, "user_id": request.user_id}

    if request.task_type == TaskType.QUIZ:
        if request.question_id is None and request.question is None:
            raise HTTPException(
                status_code=400,
                detail=f"Question ID or question is required for {request.task_type} tasks",
            )

        if request.question_id is not None and request.user_id is None:
            raise HTTPException(
                status_code=400,
                detail="User ID is required when question ID is provided",
            )

        if request.question and request.chat_history is None:
            raise HTTPException(
                status_code=400,
                detail="Chat history is required when question is provided",
            )
        if request.question_id is None:
            session_id = f"quiz_{request.task_id}_preview_{request.user_id}"
        else:
            session_id = (
                f"quiz_{request.task_id}_{request.question_id}_{request.user_id}"
            )
    else:
        if request.task_id is None:
            raise HTTPException(
                status_code=400,
                detail="Task ID is required for learning material tasks",
            )

        if request.chat_history is None:
            raise HTTPException(
                status_code=400,
                detail="Chat history is required for learning material tasks",
            )
        session_id = f"lm_{request.task_id}_{request.user_id}"

    if request.task_type == TaskType.LEARNING_MATERIAL:
        metadata["type"] = "learning_material"
        task = await get_task(request.task_id)
        if not task:
            raise HTTPException(status_code=404, detail="Task not found")

        chat_history = request.chat_history

        reference_material = construct_description_from_blocks(task["blocks"])
        question_details = f"""Reference Material:\n```\n{reference_material}\n```"""
    else:
        metadata["type"] = "quiz"

        if request.question_id:
            question = await get_question(request.question_id)
            if not question:
                raise HTTPException(status_code=404, detail="Question not found")

            metadata["question_id"] = request.question_id

            chat_history = await get_question_chat_history_for_user(
                request.question_id, request.user_id
            )
            chat_history = [
                {"role": message["role"], "content": message["content"]}
                for message in chat_history
            ]
        else:
            question = request.question.model_dump()
            chat_history = request.chat_history

            question["scorecard"] = await get_scorecard(question["scorecard_id"])

            metadata["question_id"] = None

        metadata["question_type"] = question["type"]
        metadata["question_purpose"] = (
            "practice" if question["response_type"] == "chat" else "exam"
        )
        metadata["question_input_type"] = question["input_type"]
        metadata["question_has_context"] = bool(question["context"])

        question_description = construct_description_from_blocks(question["blocks"])
        question_details = f"""Task:\n```\n{question_description}\n```"""

    task_metadata = await get_task_metadata(request.task_id)
    if task_metadata:
        metadata.update(task_metadata)

    for message in chat_history:
        if message["role"] == "user":
            if request.response_type == ChatResponseType.AUDIO:
                message["content"] = get_user_audio_message_for_chat_history(
                    message["content"]
                )
            else:
                message["content"] = get_user_message_for_chat_history(
                    message["content"]
                )
        else:
            if request.task_type == TaskType.LEARNING_MATERIAL:
                message["content"] = json.dumps({"feedback": message["content"]})

            message["content"] = get_ai_message_for_chat_history(message["content"])

    user_message = (
        get_user_audio_message_for_chat_history(request.user_response)
        if request.response_type == ChatResponseType.AUDIO
        else get_user_message_for_chat_history(request.user_response)
    )

    user_message = {"role": "user", "content": user_message}

    if request.task_type == TaskType.QUIZ:
        if question["type"] == QuestionType.OBJECTIVE:
            answer_as_prompt = construct_description_from_blocks(question["answer"])
            question_details += f"""\n\nReference Solution (never to be shared with the learner):\n```\n{answer_as_prompt}\n```"""
        else:
            scoring_criteria_as_prompt = ""

            for criterion in question["scorecard"]["criteria"]:
                scoring_criteria_as_prompt += f"""- **{criterion['name']}** [min: {criterion['min_score']}, max: {criterion['max_score']}, pass: {criterion.get('pass_score', criterion['max_score'])}]: {criterion['description']}\n"""

            question_details += (
                f"""\n\nScoring Criteria:\n```\n{scoring_criteria_as_prompt}\n```"""
            )

    chat_history = (
        chat_history
        + [user_message]
        + [
            {
                "role": "user",
                "content": question_details,
            }
        ]
    )

    # Define an async generator for streaming
    async def stream_response() -> AsyncGenerator[str, None]:
        with tracer.start_as_current_span(
            "ai_chat", openinference_span_kind="llm"
        ) as span:
            span.set_input(chat_history)

            if request.task_type == TaskType.LEARNING_MATERIAL:
                with using_attributes(
                    session_id=session_id,
                    user_id=str(request.user_id),
                    metadata={"stage": "query_rewrite", **metadata},
                ):
                    system_prompt = f"""You are a very good communicator.\n\nYou will receive:\n- A Reference Material\n- Conversation history with a student\n- The student's latest query/message.\n\nYour role: You need to rewrite the student's latest query/message by taking the reference material and the conversation history into consideration so that the query becomes more specific, detailed and clear, reflecting the actual intent of the student."""

                    model = openai_plan_to_model_name["text-mini"]

                    messages = [
                        {"role": "system", "content": system_prompt}
                    ] + chat_history

                    class Output(BaseModel):
                        rewritten_query: str = Field(
                            description="The rewritten query/message of the student"
                        )

                    pred = await run_llm_with_instructor(
                        api_key=settings.openai_api_key,
                        model=model,
                        messages=messages,
                        response_model=Output,
                        max_completion_tokens=8192,
                    )

                    chat_history[-2]["content"] = get_user_message_for_chat_history(
                        pred.rewritten_query
                    )

            output_buffer = []

            try:
                if request.response_type == ChatResponseType.AUDIO:
                    model = openai_plan_to_model_name["audio"]
                else:

                    class Output(BaseModel):
                        use_reasoning_model: bool = Field(
                            description="Whether to use a reasoning model to evaluate the student's response"
                        )

                    format_instructions = PydanticOutputParser(
                        pydantic_object=Output
                    ).get_format_instructions()

                    system_prompt = f"""You are an intelligent routing agent that decides which type of language model should be used to evaluate a student's response to a given task. You will receive the details of a task, the conversation history with the student and the student's latest query/message.\n\nYou have two options:\n- Reasoning Model (e.g. o3): Best for complex tasks involving logical deduction, problem-solving, code generation, mathematics, research reasoning, multi-step analysis, or edge-case handling.\n- General-Purpose Model (e.g. gpt-4o): Best for everyday conversation, writing help, summaries, rephrasing, explanations, casual queries, grammar correction, and general knowledge Q&A.\n\nYour job is to classify which of the two options is best suited to evaluate the student's response for the given task. If a task can be solved by a general purpose model, avoid using a reasoning model as it takes longer and costs more. At the same time, accuracy cannot be compromised.\n\n{format_instructions}"""

                    messages = [
                        {
                            "role": "system",
                            "content": system_prompt,
                        }
                    ] + chat_history

                    with using_attributes(
                        session_id=session_id,
                        user_id=str(request.user_id),
                        metadata={"stage": "router", **metadata},
                    ):
                        router_output = await run_llm_with_instructor(
                            api_key=settings.openai_api_key,
                            model=openai_plan_to_model_name["router"],
                            messages=messages,
                            response_model=Output,
                            max_completion_tokens=4096,
                        )

                    if router_output.use_reasoning_model:
                        model = openai_plan_to_model_name["reasoning"]
                    else:
                        model = openai_plan_to_model_name["text"]

                # print(f"Using model: {model}")

                if request.task_type == TaskType.QUIZ:
                    if question["type"] == QuestionType.OBJECTIVE:

                        class Output(BaseModel):
                            analysis: str = Field(
                                description="A detailed analysis of the student's response"
                            )
                            feedback: str = Field(
                                description="Feedback on the student's response; add newline characters to the feedback to make it more readable where necessary"
                            )
                            is_correct: bool = Field(
                                description="Whether the student's response correctly solves the original task that the student is supposed to solve. For this to be true, the original task needs to be completely solved and not just partially solved. Giving the right answer to one step of the task does not count as solving the entire task."
                            )

                    else:

                        class Feedback(BaseModel):
                            correct: Optional[str] = Field(
                                description="What worked well in the student's response for this category based on the scoring criteria"
                            )
                            wrong: Optional[str] = Field(
                                description="What needs improvement in the student's response for this category based on the scoring criteria"
                            )

                        class Row(BaseModel):
                            category: str = Field(
                                description="Category from the scoring criteria for which the feedback is being provided"
                            )
                            feedback: Feedback = Field(
                                description="Detailed feedback for the student's response for this category"
                            )
                            score: int = Field(
                                description="Score given within the min/max range for this category based on the student's response - the score given should be in alignment with the feedback provided"
                            )
                            max_score: int = Field(
                                description="Maximum score possible for this category as per the scoring criteria"
                            )
                            pass_score: int = Field(
                                description="Pass score possible for this category as per the scoring criteria"
                            )

                        class Output(BaseModel):
                            feedback: str = Field(
                                description="A single, comprehensive summary based on the scoring criteria"
                            )
                            scorecard: Optional[List[Row]] = Field(
                                description="List of rows with one row for each category from scoring criteria; only include this in the response if the student's response is an answer to the task"
                            )

                else:

                    class Output(BaseModel):
                        response: str = Field(
                            description="Response to the student's query; add proper formatting to the response to make it more readable where necessary"
                        )

                parser = PydanticOutputParser(pydantic_object=Output)
                format_instructions = parser.get_format_instructions()

                if request.task_type == TaskType.QUIZ:
                    knowledge_base = None

                    if question["context"]:
                        linked_learning_material_ids = question["context"][
                            "linkedMaterialIds"
                        ]
                        knowledge_blocks = question["context"]["blocks"]

                        if linked_learning_material_ids:
                            for id in linked_learning_material_ids:
                                task = await get_task(int(id))
                                if task:
                                    knowledge_blocks += task["blocks"]

                        knowledge_base = construct_description_from_blocks(
                            knowledge_blocks
                        )

                    context_instructions = ""
                    if knowledge_base:
                        context_instructions = f"""\n\nMake sure to use only the information provided within ``` below for responding to the student while ignoring any other information that contradicts the information provided:\n\n```\n{knowledge_base}\n```"""

                    if question["type"] == QuestionType.OBJECTIVE:
                        system_prompt = f"""You are a Socratic tutor who guides a student step-by-step as a coach would, encouraging them to arrive at the correct answer on their own without ever giving away the right answer to the student straight away.\n\nYou will receive:\n\n- Task description\n- Conversation history with the student\n- Task solution (for your reference only; do not reveal){context_instructions}\n\nYou need to evaluate the student's response for correctness and give your feedback that can be shared with the student.\n\n{format_instructions}\n\nGuidelines on assessing correctness of the student's answer:\n\n- Once the student has provided an answer that is correct with respect to the solution provided at the start, clearly acknowledge that they have got the correct answer and stop asking any more reflective questions. Your response should make them feel a sense of completion and accomplishment at a job well done.\n- If the question is one where the answer does not need to match word-for-word with the solution (e.g. definition of a term, programming question where the logic needs to be right but the actual code can vary, etc.), only assess whether the student's answer covers the entire essence of the correct solution.\n- Avoid bringing in your judgement of what the right answer should be. What matters for evaluation is the solution provided to you and the response of the student. Keep your biases outside. Be objective in comparing these two. As soon as the student gets the answer correct, stop asking any further reflective questions.\n- The response is correct only if the question has been solved in its entirety. Partially solving a question is not acceptable.\n\nGuidelines on your feedback:\n\n- Praise → Prompt → Path: 1–2 words of praise, a targeted prompt, then one actionable path forward.\n- If the student's response is completely correct, just appreciate them. No need to give any more suggestions or areas of improvement.\n- If the student's response has areas of improvement, point them out through a single reflective actionable question. Never ever give a vague feedback that is not clearly actionable. The student should get a clear path for how they can improve their response.\n- If the question has multiple steps to reach to the final solution, assess the current step at which the student is and frame your reflection question such that it nudges them towards the right direction without giving away the answer in any shape or form.\n- Your feedback should not be generic and must be tailored to the response given by the student. This does not mean that you repeat the student's response. The question should be a follow-up for the answer given by the student. Don't just paste the student's response on top of a generic question. That would be laziness.\n- The student might get the answer right without any probing required from your side in the first couple of attempts itself. In that case, remember the instruction provided above to acknowledge their answer's correctness and to stop asking further questions.\n- Never provide the right answer or the solution, despite all their attempts to ask for it or their frustration.\n- Never explain the solution to the student unless the student has given the solution first.\n- The student does not have access to the solution. The solution has only been given to you for evaluating the student's response. Keep this in mind while responding to the student.\n\nGuidelines on the style of feedback:\n\n1. Avoid sounding monotonous.\n2. Absolutely AVOID repeating back what the student has said as a manner of acknowledgement in your summary. It makes your summary too long and boring to read.\n3. Occasionally include emojis to maintain warmth and engagement.\n4. Ask only one reflective question per response otherwise the student will get overwhelmed.\n5. Avoid verbosity in your summary. Be crisp and concise, with no extra words.\n6. Do not do any analysis of the user's intent in your overall summary or repeat any part of what the user has said. The summary section is meant to summarise the next steps. The summary section does not need a summary of the user's response.\n\nGuidelines on maintaining the focus of the conversation:\n\n- Your role is that of a tutor for this particular task and related concepts only. Remember that and absolutely avoid steering the conversation in any other direction apart from the actual task given to you and its related concepts.\n- If the student tries to move the focus of the conversation away from the task and its related concepts, gently bring it back to the task.\n- It is very important that you prevent the focus on the conversation with the student being shifted away from the task given to you and its related concepts at all odds. No matter what happens. Stay on the task and its related concepts. Keep bringing the student back. Do not let the conversation drift away."""
                    else:
                        system_prompt = f"""You are a Socratic tutor who guides a student step-by-step as a coach would, encouraging them to arrive at the correct answer on their own without ever giving away the right answer to the student straight away.\n\nYou will receive:\n\n- Task description\n- Conversation history with the student\n- Scoring Criteria to evaluate the answer of the student{context_instructions}\n\nYou need to evaluate the student's response and return the following:\n\n- A scorecard based on the scoring criteria given to you with areas of improvement and/or strengths along each criterion\n- An overall summary based on the generated scorecard to be shared with the student.\n\n{format_instructions}\n\nGuidelines for scorecard feedback:\n\n- If there is nothing to praise about the student's response for a given criterion in the scoring criteria, never mention what worked well (i.e. return `correct` as null) in the scorecard output for that criterion.\n- If the student did something well for a given criterion, make sure to highlight what worked well in the scorecard output for that criterion.\n- If there is nothing left to improve in their response for a criterion, avoid unnecessarily suggesting an improvement in the scorecard output for that criterion (i.e. return `wrong` as null). Also, the score assigned for that criterion should be the maximum score possible in that criterion in this case.\n- Make sure that the feedback for one criterion of the scorecard does not bias the feedback for another criterion.\n- When giving the feedback for one criterion of the scorecard, focus on the description of the criterion provided in the scoring criteria and only evaluate the student's response based on that.\n- For every criterion of the scorecard, your feedback for that criterion in the scorecard output must cite specific words or phrases from the student's response to back your feedback so that the student understands it better and give concrete examples for how they can improve their response as well.\n- Never ever give a vague feedback that is not clearly actionable. The student should get a clear path for how they can improve their response.\n- Avoid bringing your judgement of what the right answer should be. What matters for feedback is the scoring criteria provided to you and the response of the student. Keep your biases outside. Be objective in comparing these two.\n- The student might get the answer right without any probing required from your side in the first couple of attempts itself. In that case, remember the instruction provided above to acknowledge their answer's correctness and to stop asking further questions.\n- If you don't assign the maximum score to the student's response for any criterion in the scorecard, make sure to always include the area of improvement containing concrete steps they can take to improve their response in your feedback for that criterion in the scorecard output (i.e. `wrong` cannot be null).\n\nGuidelines for scorecard feedback style:\n\n1. Avoid sounding monotonous.\n2. Be crisp and concise, with no extra words.\n\nGuidelines for summary:\n- Praise → Prompt → Path: 1–2 words of praise, a targeted prompt, then one actionable path forward.\n- It should clearly outline what the next steps need to be based on the scoring criteria. It should be very crisp and only contain the summary of the next steps outlined in the scorecard feedback.\n- Your overall summary does not need to quote specific words from the user's response or reflect back what the user's response means. Keep that for the feedback in the scorecard output.\n- If the student's response is completely correct, just appreciate them. No need to give any more suggestions or areas of improvement.\n- If the student's response has areas of improvement, point them out through a single reflective actionable question.\n- Your summary and follow-up question should not be generic and must be tailored to the response given by the student. This does not mean that you repeat the student's response. The question should be a follow-up for the answer given by the student. Don't just paste the student's response on top of a generic question. That would be laziness.\n- Never provide the right answer or the solution, despite all their attempts to ask for it or their frustration.\n- Never explain the solution to the student unless the student has given the solution first.\n\nGuidelines for style of summary:\n\n1. Avoid sounding monotonous.\n2. Absolutely AVOID repeating back what the student has said as a manner of acknowledgement in your summary. It makes your summary too long and boring to read.\n3. Occasionally include emojis to maintain warmth and engagement.\n4. Ask only one reflective question per response otherwise the student will get overwhelmed.\n5. Avoid verbosity in your summary.\n6. Do not do any analysis of the user's intent in your overall summary or repeat any part of what the user has said. The summary section is meant to summarise the next steps. The summary section does not need a summary of the user's response.\n\nGuidelines on maintaining the focus of the conversation:\n\n- Your role is that of a tutor for this particular task and related concepts only. Remember that and absolutely avoid steering the conversation in any other direction apart from the actual task given to you and its related concepts.\n- If the student tries to move the focus of the conversation away from the task and its related concepts, gently bring it back to the task.\n- It is very important that you prevent the focus on the conversation with the student being shifted away from the task given to you and its related concepts at all odds. No matter what happens. Stay on the task and its related concepts. Keep bringing the student back. Do not let the conversation drift away.\n\nGuidelines on when to show the scorecard:\n\n- If the response by the student is not a valid answer to the actual task given to them (e.g. if their response is an acknowledgement of the previous messages or a doubt or a question or something irrelevant to the task), do not provide any scorecard in that case and only return a summary addressing their response.\n- For messages of acknowledgement, you do not need to explicitly call it out as an acknowledgement. Simply respond to it normally."""
                else:
                    system_prompt = f"""You are a teaching assistant.\n\nYou will receive:\n- A Reference Material\n- Conversation history with a student\n- The student's latest query/message.\n\nYour role:\n- You need to respond to the student's message based on the content in the reference material provided to you.\n- If the student's query is absolutely not relevant to the reference material or goes beyond the scope of the reference material, clearly saying so without indulging their irrelevant queries. The only exception is when they are asking deeper questions related to the learning material that might not be mentioned in the reference material itself to clarify their conceptual doubts. In this case, you can provide the answer and help them.\n- Remember that the reference material is in read-only mode for the student. So, they cannot make any changes to it.\n\n{format_instructions}\n\nGuidelines on your response style:\n- Be crisp, concise and to the point.\n- Vary your phrasing to avoid monotony; occasionally include emojis to maintain warmth and engagement.\n- Playfully redirect irrelevant responses back to the task without judgment.\n- If the task involves code, format code snippets or variable/function names with backticks (`example`).\n- If including HTML, wrap tags in backticks (`<html>`).\n- If your response includes rich text format like lists, font weights, tables, etc. always render them as markdown.\n- Avoid being unnecessarily verbose in your response.\n\nGuideline on maintaining focus:\n- Your role is that of a teaching assistant for this particular task and its related concepts only. Remember that and absolutely avoid steering the conversation in any other direction apart from the actual task and its related concepts give to you.\n- If the student tries to move the focus of the conversation away from the task and its related concepts, gently bring it back.\n- It is very important that you prevent the focus on the conversation with the student being shifted away from the task and its related concepts given to you at all odds. No matter what happens. Stay on the task and its related concepts. Keep bringing the student back to the task and its related concepts. Do not let the conversation drift away."""

                messages = [{"role": "system", "content": system_prompt}] + chat_history

                with using_attributes(
                    session_id=f"{session_id}",
                    user_id=str(request.user_id),
                    metadata={"stage": "feedback", **metadata},
                ):
                    stream = await stream_llm_with_instructor(
                        api_key=settings.openai_api_key,
                        model=model,
                        messages=messages,
                        response_model=Output,
                        max_completion_tokens=4096,
                    )
                    # Process the async generator
                    async for chunk in stream:
                        content = json.dumps(chunk.model_dump()) + "\n"
                        output_buffer = content
                        yield content
            except Exception as error:
                span.record_exception(error)
                span.set_status(Status(StatusCode.ERROR))
                raise error
            else:
                span.set_output("".join(output_buffer))
                span.set_status(Status(StatusCode.OK))

    # Return a streaming response
    return StreamingResponse(
        stream_response(),
        media_type="application/x-ndjson",
    )


async def migrate_content_to_blocks(content: str) -> List[Dict]:
    class BlockProps(BaseModel):
        level: Optional[Literal[1, 2, 3]] = Field(
            description="The level of a heading block"
        )
        checked: Optional[bool] = Field(
            description="Whether the block is checked (for a checkListItem block)"
        )
        language: Optional[str] = Field(
            description="The language of the code block (for a codeBlock block); always the full name of the language in lowercase (e.g. python, javascript, sql, html, css, etc.)"
        )
        name: Optional[str] = Field(
            description="The name of the image (for an image block)"
        )
        url: Optional[str] = Field(
            description="The URL of the image (for an image block)"
        )

    class BlockContentStyle(BaseModel):
        bold: Optional[bool] = Field(description="Whether the text is bold")
        italic: Optional[bool] = Field(description="Whether the text is italic")
        underline: Optional[bool] = Field(description="Whether the text is underlined")

    class BlockContentText(BaseModel):
        type: Literal["text"] = Field(description="The type of the block content")
        text: str = Field(
            description="The text of the block; if the block is a code block, this should contain the code with newlines and tabs as appropriate"
        )
        styles: BlockContentStyle | dict = Field(
            default={}, description="The styles of the block content"
        )

    class BlockContentLink(BaseModel):
        type: Literal["link"] = Field(description="The type of the block content")
        href: str = Field(description="The URL of the link")
        content: List[BlockContentText] = Field(description="The content of the link")

    class Block(BaseModel):
        type: Literal[
            "heading",
            "paragraph",
            "bulletListItem",
            "numberedListItem",
            "codeBlock",
            "checkListItem",
            "image",
        ] = Field(description="The type of block")
        props: Optional[BlockProps | dict] = Field(
            default={}, description="The properties of the block"
        )
        content: List[BlockContentText | BlockContentLink] = Field(
            description="The content of the block; empty for image blocks"
        )

    class Output(BaseModel):
        blocks: List[Block] = Field(description="The blocks of the content")

    system_prompt = f"""You are an expert course converter. The user will give you a content in markdown format. You will need to convert the content into a structured format as given below.

Never modify the actual content given to you. Just convert it into the structured format.

The `content` field of each block should have multiple blocks only when parts of the same line in the markdown content have different parameters or styles (e.g. some part of the line is bold and some is italic or some part of the line is a link and some is not).

The final output should be a JSON in the following format:

{Output.model_json_schema()}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": content},
    ]

    output = await run_llm_with_instructor(
        api_key=settings.openai_api_key,
        model=openai_plan_to_model_name["text-mini"],
        messages=messages,
        response_model=Output,
        max_completion_tokens=16000,
    )

    blocks = output.model_dump(exclude_none=True)["blocks"]

    for block in blocks:
        if block["type"] == "image":
            block["props"].update(
                {"showPreview": True, "caption": "", "previewWidth": 512}
            )

    return blocks


async def add_generated_module(course_id: int, module: BaseModel):
    websocket_manager = get_manager()
    color = random.choice(
        [
            "#2d3748",  # Slate blue
            "#433c4c",  # Deep purple
            "#4a5568",  # Cool gray
            "#312e51",  # Indigo
            "#364135",  # Forest green
            "#4c393a",  # Burgundy
            "#334155",  # Navy blue
            "#553c2d",  # Rust brown
            "#37303f",  # Plum
            "#3c4b64",  # Steel blue
            "#463c46",  # Mauve
            "#3c322d",  # Coffee
        ]
    )
    module_id, ordering = await add_milestone_to_course(course_id, module.name, color)

    # Send WebSocket update after each module is created
    await websocket_manager.send_item_update(
        course_id,
        {
            "event": "module_created",
            "module": {
                "id": module_id,
                "name": module.name,
                "color": color,
                "ordering": ordering,
            },
        },
    )

    return module_id


async def add_generated_draft_task(course_id: int, module_id: int, task: BaseModel):
    task_id, task_ordering = await create_draft_task_for_course(
        task.name,
        task.type,
        course_id,
        module_id,
    )

    websocket_manager = get_manager()

    await websocket_manager.send_item_update(
        course_id,
        {
            "event": "task_created",
            "task": {
                "id": task_id,
                "module_id": module_id,
                "ordering": task_ordering,
                "type": str(task.type),
                "name": task.name,
            },
        },
    )
    return task_id


async def _generate_course_structure(
    course_description: str,
    intended_audience: str,
    instructions: str,
    openai_file_id: str,
    course_id: int,
    course_job_uuid: str,
    job_details: Dict,
):
    class Task(BaseModel):
        name: str = Field(description="The name of the task")
        description: str = Field(
            description="a detailed description of what should the content of that task be"
        )
        type: Literal[TaskType.LEARNING_MATERIAL, TaskType.QUIZ] | str = Field(
            description="The type of task"
        )

    class Concept(BaseModel):
        name: str = Field(description="The name of the concept")
        description: str = Field(
            description="The description for what the concept is about"
        )
        tasks: List[Task] = Field(description="A list of tasks for the concept")

    class Module(BaseModel):
        name: str = Field(description="The name of the module")
        concepts: List[Concept] = Field(description="A list of concepts for the module")

    class Output(BaseModel):
        # name: str = Field(description="The name of the course")
        modules: List[Module] = Field(description="A list of modules for the course")

    system_prompt = f"""You are an expert course creator. The user will give you some instructions for creating a course along with the reference material to be used as the source for the course content.

You need to thoroughly analyse the reference material given to you and come up with a structure for the course. Each course should be structured into modules where each modules represents a full topic.

With each modules, there must be a mix of learning materials and quizzes. A learning material is used for learning about a specific concept in the topic. Keep separate learning materials for different concepts in the same topic/module. For each concept, the learning material for that concept should be followed by one or more quizzes. Each quiz contains multiple questions for testing the understanding of the learner on the actual concept.

Quizzes are where learners can practice a concept. While testing theoretical understanding is important, quizzes should go beyond that and produce practical challenges for the students to apply what they have learnt. If the reference material already has examples/sample problems, include them in the quizzes for the students to practice. If no examples are present in the reference material, generate a few relevant problem statements to test the real-world understanding of each concept for the students.

All explanations should be present in the learning materials and all practice should be done in quizzes. Maintain this separation of purpose for each task type.

No need to come up with the questions inside the quizzes for now. Just focus on producing the right structure.
Don't keep any concept too big. Break a topic down into multiple smaller, ideally independent, concepts. For each concept, follow the sequence of learning material -> quiz before moving to the next concept in that topic.
End the course with a conclusion module (with the appropriate name for the module suited to the course) which ties everything taught in the course together and ideally ends with a capstone project where the learner has to apply everything they have learnt in the course.

Make sure to never skip a single concept from the reference material provided.

The final output should be a JSON in the following format:

{Output.model_json_schema()}

Keep the sequences of modules, concepts, and tasks in mind.

Do not include the type of task in the name of the task."""

    course_structure_generation_prompt = f"""About the course: {course_description}\n\nIntended audience: {intended_audience}"""

    if instructions:
        course_structure_generation_prompt += f"\n\nInstructions: {instructions}"

    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": [
                {
                    "type": "file",
                    "file": {
                        "file_id": openai_file_id,
                    },
                },
            ],
        },
        # separate into 2 user messages for prompt caching to work
        {"role": "user", "content": course_structure_generation_prompt},
    ]

    stream = await stream_llm_with_instructor(
        api_key=settings.openai_api_key,
        model=openai_plan_to_model_name["text"],
        messages=messages,
        response_model=Output,
        max_completion_tokens=16000,
    )

    module_ids = []

    module_concepts = defaultdict(lambda: defaultdict(list))

    output = None

    async for chunk in stream:
        if not chunk or not chunk.modules:
            continue

        for index, module in enumerate(chunk.modules):
            if not module or not module.name or not module.concepts:
                continue

            if index >= len(module_ids):
                module_id = await add_generated_module(course_id, module)
                module_ids.append(module_id)
            else:
                module_id = module_ids[index]

            task_index = 0

            for concept_index, concept in enumerate(module.concepts):
                if (
                    not concept
                    or not concept.tasks
                    or concept_index < len(module_concepts[module_id]) - 1
                ):
                    continue

                for task_index, task in enumerate(concept.tasks):
                    if (
                        not task
                        or not task.name
                        or not task.type
                        or task.type not in [TaskType.LEARNING_MATERIAL, TaskType.QUIZ]
                        or task_index < len(module_concepts[module_id][concept_index])
                    ):
                        continue

                    task_id = await add_generated_draft_task(course_id, module_id, task)
                    module_concepts[module_id][concept_index].append(task_id)

        # output = chunk

    output = chunk.model_dump()

    for index, module in enumerate(output["modules"]):
        module["id"] = module_ids[index]

        for concept_index, concept in enumerate(module["concepts"]):
            for task_index, task in enumerate(concept["tasks"]):
                task["id"] = module_concepts[module["id"]][concept_index][task_index]

    job_details["course_structure"] = output
    await update_course_generation_job_status_and_details(
        course_job_uuid,
        GenerateCourseJobStatus.PENDING,
        job_details,
    )

    websocket_manager = get_manager()

    await websocket_manager.send_item_update(
        course_id,
        {
            "event": "course_structure_completed",
            "job_id": course_job_uuid,
        },
    )


@router.post("/generate/course/{course_id}/structure")
async def generate_course_structure(
    course_id: int,
    background_tasks: BackgroundTasks,
    request: GenerateCourseStructureRequest,
):
    openai_client = openai.AsyncOpenAI(
        api_key=settings.openai_api_key,
    )

    if settings.s3_folder_name:
        reference_material = download_file_from_s3_as_bytes(
            request.reference_material_s3_key
        )
    else:
        with open(
            os.path.join(
                settings.local_upload_folder, request.reference_material_s3_key
            ),
            "rb",
        ) as f:
            reference_material = f.read()

    # Create a temporary file to pass to OpenAI
    with tempfile.NamedTemporaryFile(suffix=".pdf") as temp_file:
        temp_file.write(reference_material)
        temp_file.flush()

        file = await openai_client.files.create(
            file=open(temp_file.name, "rb"),
            purpose="user_data",
        )

    job_details = {**request.model_dump(), "openai_file_id": file.id}
    job_uuid = await store_course_generation_request(
        course_id,
        job_details,
    )

    background_tasks.add_task(
        _generate_course_structure,
        request.course_description,
        request.intended_audience,
        request.instructions,
        file.id,
        course_id,
        job_uuid,
        job_details,
    )

    return {"job_uuid": job_uuid}


def task_generation_schemas():

    class BlockProps(BaseModel):
        level: Optional[Literal[2, 3]] = Field(
            description="The level of a heading block"
        )
        checked: Optional[bool] = Field(
            description="Whether the block is checked (for a checkListItem block)"
        )
        language: Optional[str] = Field(
            description="The language of the code block (for a codeBlock block); always the full name of the language in lowercase (e.g. python, javascript, sql, html, css, etc.)"
        )

    class BlockContentStyle(BaseModel):
        bold: Optional[bool] = Field(description="Whether the text is bold")
        italic: Optional[bool] = Field(description="Whether the text is italic")
        underline: Optional[bool] = Field(description="Whether the text is underlined")

    class BlockContent(BaseModel):
        text: str = Field(
            description="The text of the block; if the block is a code block, this should contain the code with newlines and tabs as appropriate"
        )
        styles: BlockContentStyle | dict = Field(
            default={}, description="The styles of the block content"
        )

    class Block(BaseModel):
        type: Literal[
            "heading",
            "paragraph",
            "bulletListItem",
            "numberedListItem",
            "codeBlock",
            "checkListItem",
        ] = Field(description="The type of block")
        props: Optional[BlockProps | dict] = Field(
            default={}, description="The properties of the block"
        )
        content: Optional[List[BlockContent]] = Field(
            description="The content of the block"
        )

    class LearningMaterial(BaseModel):
        blocks: List[Block] = Field(
            description="The content of the learning material as blocks"
        )

    class Criterion(BaseModel):
        name: str = Field(
            description="The name of the criterion (e.g. grammar, relevance, clarity, confidence, pronunciation, brevity, etc.), keep it to 1-2 words unless absolutely necessary to extend beyond that"
        )
        description: str = Field(
            description="The description/rubric for how to assess this criterion - the more detailed it is, the better the evaluation will be, but avoid making it unnecessarily big - only as descriptive as it needs to be but nothing more"
        )
        min_score: int = Field(
            description="The minimum score possible to achieve for this criterion (e.g. 0)"
        )
        max_score: int = Field(
            description="The maximum score possible to achieve for this criterion (e.g. 5)"
        )

    class Scorecard(BaseModel):
        title: str = Field(
            description="what does the scorecard assess (e.g. written communication, interviewing skills, product pitch, etc.)"
        )
        criteria: List[Criterion] = Field(
            description="The list of criteria for the scorecard."
        )

    class Question(BaseModel):
        question_type: Literal["objective", "subjective", "coding"] = Field(
            description='The type of question; "objective" means that the question has a fixed correct answer and the learner\'s response must precisely match it. "subjective" means that the question is subjective, with no fixed correct answer. "coding" - a specific type of "objective" question for programming questions that require one to write code.'
        )
        answer_type: Optional[Literal["text", "audio"]] = Field(
            description='The type of answer; "text" means the student has to submit textual answer where "audio" means student has to submit audio answer. Ignore this field for questionType = "coding".',
        )
        coding_languages: Optional[
            List[Literal["HTML", "CSS", "JS", "Python", "React", "Node", "SQL"]]
        ] = Field(
            description='The languages that a student need to submit their code in for questionType=coding. It is a list because a student might have to submit their code in multiple languages as well (e.g. HTML, CSS, JS). This should only be included for questionType = "coding".',
        )
        blocks: List[Block] = Field(
            description="The actual question details as individual blocks. Every part of the question should be included here. Do not assume that there is another field to capture different parts of the question. This is the only field that should be used to capture the question details. This means that if the question is an MCQ, all the options should be included here and not in another field. Extend the same idea to other question types."
        )
        correct_answer: Optional[List[Block]] = Field(
            description='The actual correct answer to compare a student\'s response with. Ignore this field for questionType = "subjective".',
        )
        scorecard: Optional[Scorecard] = Field(
            description='The scorecard for subjective questions. Ignore this field for questionType = "objective" or "coding".',
        )
        context: List[Block] = Field(
            description="A short text that is not the question itself. This is used to add instructions for how the student should be given feedback or the overall purpose of that question. It can also include the raw content from the reference material to be used for giving feedback to the student that may not be present in the question content (hidden from the student) but is critical for providing good feedback."
        )

    class Quiz(BaseModel):
        questions: List[Question] = Field(
            description="A list of questions for the quiz"
        )

    return LearningMaterial, Quiz


def get_system_prompt_for_task_generation(task_type):
    LearningMaterial, Quiz = task_generation_schemas()
    schema = (
        LearningMaterial.model_json_schema()
        if task_type == "learning_material"
        else Quiz.model_json_schema()
    )

    quiz_prompt = """Each quiz/exam contains multiple questions for testing the understanding of the learner on the actual concept.

Important Instructions for Quiz Generation:
- For a quiz, each question must add a strong positive value to the overall learner's understanding. Do not unnecessarily add questions simply to increase the number of questions. If a quiz merits only a single question based on the reference material provided or your asseessment of how many questions are necessary for it, keep a single question itself. Only add multiple questions when the quiz merits so. 
- The `content` for each question is the only part of the question shown directly to the student. Add everything that the student needs to know to answer the question inside the `content` field for that question. Do not add anything there that should not be shown to the student (e.g. what is the correct answer). To add instructions for how the student should be given feedback or the overall purpose of that question or raw content from the reference material required as context to give adequate feedback, add it to the `context` field instead. 
- While testing theoretical understanding is important, a quiz should go beyond that and produce practical challenges for the students to apply what they have learnt. If the reference material already has examples/sample problems, include them in the a quiz for the students to practice. If no examples are present in the reference material, generate a few relevant problem statements to test the real-world understanding of each concept for the students.
- If a question references a set of options that must be shown to the student, always make sure that those options are actually present in the `content` field for that question. THIS IS SUPER IMPORTANT. As mentioned before, if the reference material does not have the options or data required for the question, generate it based on your understanding of the question and its purpose.
- Use appropriate formatting for the `blocks` in each question. Make use of all the block types available to you to make the content of each question as engaging and readable as possible.
- Do not use the name of the quiz as a heading to mark the start of a question in the `blocks` field for each question. The name of the quiz will already be visible to the student."""

    learning_material_prompt = """A learning material is used for learning about a specific concept. 
    
Make the \"content\" field in learning material contain as much detail as present in the reference material relevant to it. Do not try to summarise it or skip any point.

Use appropriate formatting for the `blocks` in the learning material. Make use of all the block types available to you to make the content as engaging and readable as possible.

Do not use the name of the learning material as a heading to mark the start of the learning material in the `blocks`.  The name of the learning material will already be visible to the student."""

    task_type_prompt = quiz_prompt if task_type == "quiz" else learning_material_prompt

    system_prompt = f"""You are an expert course creator. The user will give you an outline for a concept in a course they are creating along with the reference material to be used as the source for the course content and the name of one of the tasks from the outline.

You need to generate the content for the single task whose name is provided to you out of all the tasks in the outline. The outline contains the name of a concept in the course, its description and a list of tasks in that concept. Each task can be either a learning material, quiz or exam. You are given this outline so that you can clearly identify what part of the reference material should be used for generating the specific task you need to generate and for you to also understand what should not be included in your generated task. For each task, you have been given a description about what should be included in that task. 

{task_type_prompt}

The final output should be a JSON in the following format:

{schema}"""

    return system_prompt


async def generate_course_task(
    client,
    task: Dict,
    concept: Dict,
    file_id: str,
    task_job_uuid: str,
    course_job_uuid: str,
    course_id: int,
):

    system_prompt = get_system_prompt_for_task_generation(task["type"])

    model = openai_plan_to_model_name["text"]

    generation_prompt = f"""Concept details:

{concept}

Task to generate:

{task['name']}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": [
                {
                    "type": "file",
                    "file": {
                        "file_id": file_id,
                    },
                },
            ],
        },
        # separate into 2 user messages for prompt caching to work
        {"role": "user", "content": generation_prompt},
    ]

    LearningMaterial, Quiz = task_generation_schemas()
    response_model = (
        LearningMaterial if task["type"] == TaskType.LEARNING_MATERIAL else Quiz
    )

    output = await client.chat.completions.create(
        model=model,
        messages=messages,
        response_model=response_model,
        max_completion_tokens=16000,
        store=True,
    )

    task["details"] = output.model_dump(exclude_none=True)

    if task["type"] == TaskType.LEARNING_MATERIAL:
        await add_generated_learning_material(task["id"], task)
    else:
        await add_generated_quiz(task["id"], task)

    await update_task_generation_job_status(
        task_job_uuid, GenerateTaskJobStatus.COMPLETED
    )

    course_jobs_status = await get_course_task_generation_jobs_status(course_id)

    websocket_manager = get_manager()

    await websocket_manager.send_item_update(
        course_id,
        {
            "event": "task_completed",
            "task": {
                "id": task["id"],
            },
            "total_completed": course_jobs_status[str(GenerateTaskJobStatus.COMPLETED)],
        },
    )

    if not course_jobs_status[str(GenerateTaskJobStatus.STARTED)]:
        await update_course_generation_job_status(
            course_job_uuid, GenerateCourseJobStatus.COMPLETED
        )


@router.post("/generate/course/{course_id}/tasks")
async def generate_course_tasks(
    course_id: int,
    job_uuid: str = Body(..., embed=True),
):
    job_details = await get_course_generation_job_details(job_uuid)

    client = instructor.from_openai(
        openai.AsyncOpenAI(
            api_key=settings.openai_api_key,
        )
    )

    # Create a list to hold all task coroutines
    tasks = []

    for module in job_details["course_structure"]["modules"]:
        for concept in module["concepts"]:
            for task in concept["tasks"]:
                task_job_uuid = await store_task_generation_request(
                    task["id"],
                    course_id,
                    {
                        "task": task,
                        "concept": concept,
                        "openai_file_id": job_details["openai_file_id"],
                        "course_job_uuid": job_uuid,
                        "course_id": course_id,
                    },
                )
                # Add task to the list instead of adding to background_tasks
                tasks.append(
                    generate_course_task(
                        client,
                        task,
                        concept,
                        job_details["openai_file_id"],
                        task_job_uuid,
                        job_uuid,
                        course_id,
                    )
                )

    # Create a function to run all tasks in parallel
    async def run_tasks_in_parallel():
        try:
            # Run all tasks concurrently using asyncio.gather
            await async_batch_gather(tasks, description="Generating tasks")
        except Exception as e:
            logger.error(f"Error in parallel task execution: {e}")

    # Start the parallel execution in the background without awaiting it
    asyncio.create_task(run_tasks_in_parallel())

    return {
        "success": True,
    }


async def resume_pending_course_structure_generation_jobs():
    incomplete_course_structure_jobs = (
        await get_all_pending_course_structure_generation_jobs()
    )

    if not incomplete_course_structure_jobs:
        return

    tasks = []

    for job in incomplete_course_structure_jobs:
        tasks.append(
            _generate_course_structure(
                job["job_details"]["course_description"],
                job["job_details"]["intended_audience"],
                job["job_details"]["instructions"],
                job["job_details"]["openai_file_id"],
                job["course_id"],
                job["uuid"],
                job["job_details"],
            )
        )

    await async_batch_gather(
        tasks, description="Resuming course structure generation jobs"
    )


async def resume_pending_task_generation_jobs():
    incomplete_course_jobs = await get_all_pending_task_generation_jobs()

    if not incomplete_course_jobs:
        return

    tasks = []

    client = instructor.from_openai(
        openai.AsyncOpenAI(
            api_key=settings.openai_api_key,
        )
    )

    for job in incomplete_course_jobs:
        tasks.append(
            generate_course_task(
                client,
                job["job_details"]["task"],
                job["job_details"]["concept"],
                job["job_details"]["openai_file_id"],
                job["uuid"],
                job["job_details"]["course_job_uuid"],
                job["job_details"]["course_id"],
            )
        )

    await async_batch_gather(tasks, description="Resuming task generation jobs")




================================================================
End of Codebase
================================================================
